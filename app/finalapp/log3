Started by timer
[lockable-resources] acquired lock on [testbed7]
Building remotely on testserver3 (BUILD_SANITY) in workspace /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily
[GA-2.3.0-NYNJ-Daily] $ /bin/sh -xe /tmp/jenkins5266549009990051848.sh
+ set +e
+ project_dir=GA-2.3.0-NYNJ-Daily
+ mkdir GA-2.3.0-NYNJ-Daily
mkdir: cannot create directory â€˜GA-2.3.0-NYNJ-Dailyâ€™: File exists
+ cd GA-2.3.0-NYNJ-Daily
+ DIAMANTI_RPM=/dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/dcx.rpm
+ TEST_RPM=diamanti-test-pkg.tar.gz
+ rm -f e2e /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/dcx.rpm auto_tb7_inventory.json dctl e2e_param.json diamanti-test-pkg.tar.gz
+ export DCTL_CONFIG=/dwshome/naveen/sanity/auto_tb7
+ DCTL_CONFIG=/dwshome/naveen/sanity/auto_tb7
+ mkdir -p /dwshome/naveen/sanity/auto_tb7
+ export KUBECONFIG=/dwshome/naveen/sanity/auto_tb7/.dctl.d/kubeconfig
+ KUBECONFIG=/dwshome/naveen/sanity/auto_tb7/.dctl.d/kubeconfig
+ '[' false = false ']'
++ wget -q -O - http://bldserv1:8080/job/Project17-GA-2.3.0/lastSuccessfulBuild/artifact/artifacts/rpm/
++ head -n 1
++ grep -Po 'diamanti-cx.*?rpm'
+ rpm=diamanti-cx-2.3.0-55.x86_64.rpm
+ wget --quiet http://bldserv1:8080/job/Project17-GA-2.3.0/lastSuccessfulBuild/artifact/artifacts/rpm/diamanti-cx-2.3.0-55.x86_64.rpm -O /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/dcx.rpm
+ wget --quiet http://bldserv1:8080/job/Project17-GA-2.3.0/lastSuccessfulBuild/artifact/artifacts/test/diamanti-test-pkg.tar.gz
+ tar xfz diamanti-test-pkg.tar.gz
+ cd diamanti-test-pkg/bin
+ export PATH=/dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/bin:/dwshome/naveen/scripts:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin
+ PATH=/dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/bin:/dwshome/naveen/scripts:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin
+ export INVENTORY=../qa_tb/auto_tb7_inventory.json
+ INVENTORY=../qa_tb/auto_tb7_inventory.json
++ rpm -qp --queryformat '%{VERSION}' /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/dcx.rpm
+ ver=2.3.0
++ rpm -qp --queryformat '%{RELEASE}' /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/dcx.rpm
+ rel=55
+ iter=0
++ cat /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/bin/version.txt
+ old_run=Daily-52-0
+ [[ Daily-52-0 == *55* ]]
+ dailyskiplist='Network.PingWithDnsName|StorageKubeSnapshot.PatternVerificationOnLCV|Rbac.Multiple|PerfTier.CreateDeleteValidateQosWithCustomMaxBwLimit|PerfTier.NetworkThroughputValidationCustomPerfTierWithMaxBWLimit|DirectionalOnePort|StorageKubeSnapshot.CreateSnapshotAndLCV|Rbac.SecureAuthLinuxLDAP|Benchmarking.WriteThroughputRemoteVolumes'
+ weeklyskiplist='Nvme|ReplicationController.NetworkPodEvacuation|RemoteStorage.IPMIBasedTargetReboot|MaxSize|ResyncTwoTargetReboot|RebootDestinationNodeDuringResync|RebootSourceNodeDuringResync|Pod.UnhealthyNode|Storage.StressWithFioPodsWithReboot|Qos|Rbac|Rbac.MultipleSecureAuthsWindowsLinux|Storage.StressWithFioPods|Snapshot.LimitTestMaxSnapshotsPerLocalVolumeOnANode|StorageKubeSnapshot.PatternVerificationOnLCV|Rbac.FunctionTestsRemoteLAuth|Rbac.FunctionTestsRemoteWAuth|Network.ValidateTrafficAfterIperfClientRcPodEvacuation|Nfs.NfsSrvrAndClientOnSameCluster|Quorum|Cluster.DataValidationOnQHMVolumesAfterNodeRemoveAndAdd|StorageKubeSnapshot.CreateSnapshotAndLCV
'
++ date +%a
+ DAYOFWEEK=Tue
+ '[' Tue == Fri ']'
+ '[' Tue == Sat ']'
+ focus='Sanity|Daily'
+ skiplist='Network.PingWithDnsName|StorageKubeSnapshot.PatternVerificationOnLCV|Rbac.Multiple|PerfTier.CreateDeleteValidateQosWithCustomMaxBwLimit|PerfTier.NetworkThroughputValidationCustomPerfTierWithMaxBWLimit|DirectionalOnePort|StorageKubeSnapshot.CreateSnapshotAndLCV|Rbac.SecureAuthLinuxLDAP|Benchmarking.WriteThroughputRemoteVolumes'
+ failfast=true
+ focus=Daily
+ skiplist='Network.PingWithDnsName|StorageKubeSnapshot.PatternVerificationOnLCV|Rbac.Multiple|PerfTier.CreateDeleteValidateQosWithCustomMaxBwLimit|PerfTier.NetworkThroughputValidationCustomPerfTierWithMaxBWLimit|DirectionalOnePort|StorageKubeSnapshot.CreateSnapshotAndLCV|Rbac.SecureAuthLinuxLDAP|Benchmarking.WriteThroughputRemoteVolumes'
+ echo Daily-55-0
+ rm -rf /dwshome/naveen/.kube
+ tee console_ouput.txt
+ e2e --tb=../qa_tb/auto_tb7_inventory.json --ginkgo.failFast=true --ginkgo.focus=Daily '--ginkgo.skip=Network.PingWithDnsName|StorageKubeSnapshot.PatternVerificationOnLCV|Rbac.Multiple|PerfTier.CreateDeleteValidateQosWithCustomMaxBwLimit|PerfTier.NetworkThroughputValidationCustomPerfTierWithMaxBWLimit|DirectionalOnePort|StorageKubeSnapshot.CreateSnapshotAndLCV|Rbac.SecureAuthLinuxLDAP|Benchmarking.WriteThroughputRemoteVolumes' --rpm=/dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/dcx.rpm --install-method=install --check-stale-resources --check-services-timestamp
Environment variable DIAMANTI_TEST_DIR is not set. Setting it to /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg
Default log file location : /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/logs/e2e_log2019-11-26_21-07-18.log
Default command log file location : /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/logs/e2e_command_log2019-11-26_21-07-18.log
    DEBUG: 2019/11/26 21:07:18 Removing leading and trailing spaces from Ipmi IP of all nodes.
    DEBUG: 2019/11/26 21:07:18 Check all nodes are ready to run test(s):
    DEBUG: 2019/11/26 21:07:18 172.16.6.153 is ready to run test
    DEBUG: 2019/11/26 21:07:18 172.16.6.154 is ready to run test
    DEBUG: 2019/11/26 21:07:18 172.16.6.155 is ready to run test
    DEBUG: 2019/11/26 21:07:35 Testbed type used is to get list of Tcs is: D10
Running Suite: DWS e2e Suite run 1 of 1
=======================================
Random Seed: [1m1574831238[0m - Will randomize all specs
Will run [1m123[0m of [1m418[0m specs

    DEBUG: 2019/11/26 21:07:35 Planning to run tests on following nodes :
    DEBUG: 2019/11/26 21:07:35 172.16.6.153
    DEBUG: 2019/11/26 21:07:35 172.16.6.154
    DEBUG: 2019/11/26 21:07:35 172.16.6.155
    DEBUG: 2019/11/26 21:07:35 Checking existance of loopback device(s) on all cluster nodes
    DEBUG: 2019/11/26 21:07:47 Saving cluster info of primary and secondary clusters: 
    DEBUG: 2019/11/26 21:07:47 Checking if cluster already exists 
    DEBUG: 2019/11/26 21:07:49 OS is centos. Package diamanti-cx is installed on 172.16.6.153
    DEBUG: 2019/11/26 21:07:50 Version of the old dctl : 2.3.99
    DEBUG: 2019/11/26 21:07:50 Version of the rpm which we are going to install: 2.3.0
    DEBUG: 2019/11/26 21:07:50 Method used :install. RPM path: /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/dcx.rpm
    DEBUG: 2019/11/26 21:07:50 Output from 172.16.6.153
Warning: Permanently added '172.16.6.153' (ECDSA) to the list of known hosts.

CentOS Linux 7 (Core)
Kernel 3.10.0-957.el7.x86_64 on an x86_64

    DEBUG: 2019/11/26 21:08:04 [33m Uninstalling diamanti-cx-2.3.99-40.x86_64 from 172.16.6.154 [0m
    DEBUG: 2019/11/26 21:08:05 [33m Uninstalling diamanti-cx-2.3.99-40.x86_64 from 172.16.6.155 [0m
    DEBUG: 2019/11/26 21:08:05 [33m Uninstalling diamanti-cx-2.3.99-40.x86_64 from 172.16.6.153 [0m
diamanti: Stopping services on uninstall...
diamanti: Stopping services on uninstall...
diamanti: Stopping services on uninstall...
warning: /etc/kubernetes/controller-manager saved as /etc/kubernetes/controller-manager.rpmsave
warning: /etc/diamanti/prometheus/prometheus.yml saved as /etc/diamanti/prometheus/prometheus.yml.rpmsave
warning: /etc/diamanti/convoy.conf saved as /etc/diamanti/convoy.conf.rpmsave
diamanti: Cleaning up on uninstall...
warning: /etc/kubernetes/controller-manager saved as /etc/kubernetes/controller-manager.rpmsave
warning: /etc/diamanti/prometheus/prometheus.yml saved as /etc/diamanti/prometheus/prometheus.yml.rpmsave
warning: /etc/diamanti/convoy.conf saved as /etc/diamanti/convoy.conf.rpmsave
diamanti: Cleaning up on uninstall...
warning: /etc/kubernetes/controller-manager saved as /etc/kubernetes/controller-manager.rpmsave
warning: /etc/diamanti/prometheus/prometheus.yml saved as /etc/diamanti/prometheus/prometheus.yml.rpmsave
warning: /etc/diamanti/convoy.conf saved as /etc/diamanti/convoy.conf.rpmsave
diamanti: Cleaning up on uninstall...
Preparing...                                                            (100%)#                                 (100%)##                                (100%)###                               (100%)####                              (100%)#####                             (100%)######                            (100%)#######                           (100%)########                          (100%)#########                         (100%)##########                        (100%)###########                       (100%)############                      (100%)#############                     (100%)##############                    (100%)###############                   (100%)################                  (100%)#################                 (100%)##################                (100%)###################               (100%)####################              (100%)#####################             (100%)######################            (100%)#######################           (100%)########################          (100%)#########################         (100%)##########################        (100%)###########################       (100%)############################      (100%)#############################     (100%)##############################    (100%)###############################   (100%)################################  (100%)################################# (100%)################################# [100%]
Updating / installing...
   1:diamanti-cx-2.3.0-55                                               (  1%)#                                 (  4%)##                                (  7%)###                               ( 10%)####                              ( 13%)#####                             ( 16%)######                            ( 19%)#######                           ( 22%)########                          ( 25%)#########                         ( 28%)##########                        ( 31%)###########                       ( 34%)############                      ( 37%)#############                     ( 40%)##############                    ( 43%)###############                   ( 46%)################                  ( 49%)#################                 ( 51%)##################                ( 54%)###################               ( 57%)####################              ( 60%)#####################             ( 63%)######################            ( 66%)#######################           ( 69%)########################          ( 72%)#########################         ( 75%)##########################        ( 78%)###########################       ( 81%)############################      ( 84%)#############################     ( 87%)##############################    ( 90%)###############################   ( 93%)################################  ( 96%)################################# ( 99%)################################# [100%]
Setting up configuration for firmware installation
Installing Firmware component 1 .....................
Firmware component 1 installation succeeded
    DEBUG: 2019/11/26 21:16:21 Doing sync on 172.16.6.154
Installing Firmware component 2 
Firmware component 2 already up-to-date
Please powercycle for changes to take effect
    DEBUG: 2019/11/26 21:16:44 Doing sync on 172.16.6.153

Output from  172.16.6.154
Preparing...                                                            (100%)#                                 (100%)##                                (100%)###                               (100%)####                              (100%)#####                             (100%)######                            (100%)#######                           (100%)########                          (100%)#########                         (100%)##########                        (100%)###########                       (100%)############                      (100%)#############                     (100%)##############                    (100%)###############                   (100%)################                  (100%)#################                 (100%)##################                (100%)###################               (100%)####################              (100%)#####################             (100%)######################            (100%)#######################           (100%)########################          (100%)#########################         (100%)##########################        (100%)###########################       (100%)############################      (100%)#############################     (100%)##############################    (100%)###############################   (100%)################################  (100%)################################# (100%)################################# [100%]
Updating / installing...
   1:diamanti-cx-2.3.0-55                                               (  1%)#                                 (  4%)##                                (  7%)###                               ( 10%)####                              ( 13%)#####                             ( 16%)######                            ( 19%)#######                           ( 22%)########                          ( 25%)#########                         ( 28%)##########                        ( 31%)###########                       ( 34%)############                      ( 37%)#############                     ( 40%)##############                    ( 43%)###############                   ( 46%)################                  ( 49%)#################                 ( 51%)##################                ( 54%)###################               ( 57%)####################              ( 60%)#####################             ( 63%)######################            ( 66%)#######################           ( 69%)########################          ( 72%)#########################         ( 75%)##########################        ( 78%)###########################       ( 81%)############################      ( 84%)#############################     ( 87%)##############################    ( 90%)###############################   ( 93%)################################  ( 96%)################################# ( 99%)################################# [100%]
Setting up configuration for firmware installation
Installing Firmware component 1 ...................
Firmware component 1 installation succeeded
Installing Firmware component 2 
Firmware component 2 already up-to-date
Please powercycle for changes to take effect
Warning: Permanently added '172.16.6.154' (ECDSA) to the list of known hosts.

CentOS Linux 7 (Core)
Kernel 3.10.0-957.el7.x86_64 on an x86_64

    DEBUG: 2019/11/26 21:16:53 Doing sync on 172.16.6.155

Output from  172.16.6.155
Preparing...                                                            (100%)#                                 (100%)##                                (100%)###                               (100%)####                              (100%)#####                             (100%)######                            (100%)#######                           (100%)########                          (100%)#########                         (100%)##########                        (100%)###########                       (100%)############                      (100%)#############                     (100%)##############                    (100%)###############                   (100%)################                  (100%)#################                 (100%)##################                (100%)###################               (100%)####################              (100%)#####################             (100%)######################            (100%)#######################           (100%)########################          (100%)#########################         (100%)##########################        (100%)###########################       (100%)############################      (100%)#############################     (100%)##############################    (100%)###############################   (100%)################################  (100%)################################# (100%)################################# [100%]
Updating / installing...
   1:diamanti-cx-2.3.0-55                                               (  1%)#                                 (  4%)##                                (  7%)###                               ( 10%)####                              ( 13%)#####                             ( 16%)######                            ( 19%)#######                           ( 22%)########                          ( 25%)#########                         ( 28%)##########                        ( 31%)###########                       ( 34%)############                      ( 37%)#############                     ( 40%)##############                    ( 43%)###############                   ( 46%)################                  ( 49%)#################                 ( 51%)##################                ( 54%)###################               ( 57%)####################              ( 60%)#####################             ( 63%)######################            ( 66%)#######################           ( 69%)########################          ( 72%)#########################         ( 75%)##########################        ( 78%)###########################       ( 81%)############################      ( 84%)#############################     ( 87%)##############################    ( 90%)###############################   ( 93%)################################  ( 96%)################################# ( 99%)################################# [100%]
Setting up configuration for firmware installation
Installing Firmware component 1 .....................
Firmware component 1 installation succeeded
Installing Firmware component 2 
Firmware component 2 already up-to-date
Please powercycle for changes to take effect
Warning: Permanently added '172.16.6.155' (ECDSA) to the list of known hosts.

CentOS Linux 7 (Core)
Kernel 3.10.0-957.el7.x86_64 on an x86_64

    DEBUG: 2019/11/26 21:16:54 Waiting for nodes to come up, will wait upto 800 seconds
.............    DEBUG: 2019/11/26 21:19:17 Nodes are up, waiting for armada to start
.....
    DEBUG: 2019/11/26 21:20:08 Old rpm verison was = 2.3.99
    DEBUG: 2019/11/26 21:20:08 Current rpm verison is = 2.3.0
    DEBUG: 2019/11/26 21:20:08 As current dctl version is lower than old dctl, starting sputil -ta on embedded side
    DEBUG: 2019/11/26 21:20:11 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:20:19 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:20:27 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:20:35 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:20:43 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:20:51 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:20:59 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:21:07 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:21:15 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:21:23 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:21:31 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:21:39 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:21:47 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:21:55 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:22:04 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:22:12 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:22:20 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:22:28 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:22:36 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:22:44 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:22:52 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:23:00 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:23:08 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:23:13 Seems cluster is not exist hence formatting drives using command "sputil -ta" on embedded
    DEBUG: 2019/11/26 21:24:25 Did sputil -a -t on 172.16.6.153
    DEBUG: 2019/11/26 21:24:25 Did sputil -a -t on 172.16.6.154
    DEBUG: 2019/11/26 21:24:25 Did sputil -a -t on 172.16.6.155
    DEBUG: 2019/11/26 21:24:25 Waiting for nodes to come up, will wait upto 800 seconds
.............    DEBUG: 2019/11/26 21:26:47 Nodes are up, waiting for armada to start
......
    DEBUG: 2019/11/26 21:27:47 Starting tests
    DEBUG: 2019/11/26 21:27:48 Cluster Spec Node list is [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/26 21:27:48 Getting dns domain name
    DEBUG: 2019/11/26 21:27:48 FQDN : autotb7.eng.diamanti.com
    DEBUG: 2019/11/26 21:27:48 Generating certificates for the cluster: (Name: autotb7, VIP: 172.16.19.55, FQDN: autotb7.eng.diamanti.com)
    DEBUG: 2019/11/26 21:27:48 Clean up existing certs if any:
    DEBUG: 2019/11/26 21:27:48 Generate unique CA name with current date
    DEBUG: 2019/11/26 21:27:48 Integrate CA name in file
    DEBUG: 2019/11/26 21:27:48 Generate CA certs
    DEBUG: 2019/11/26 21:27:48 Create a CSR to generate a certificate using FQDN, VIP, Cluster Name for a server certs
    DEBUG: 2019/11/26 21:27:48 Generate server certificate:
    DEBUG: 2019/11/26 21:27:48 Getting CertificateAuthority from /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/server_certs/ca.pem file
    DEBUG: 2019/11/26 21:27:48 Getting ServerCertificate from /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/server_certs/server.pem file
    DEBUG: 2019/11/26 21:27:48 Getting ServerPrivateKey from /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/server_certs/server-key.pem file
    DEBUG: 2019/11/26 21:27:48 Creating the cluster
    DEBUG: 2019/11/26 21:28:04 Please import "/dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/server_certs/ca.pem" certificate to your client machine
    DEBUG: 2019/11/26 21:28:04 Sleeping for 60 sec
    DEBUG: 2019/11/26 21:29:04 Save cluster configuration: 
    DEBUG: 2019/11/26 21:29:05 Login to cluster
    DEBUG: 2019/11/26 21:29:05 Polling for cluster login for 300 seconds.
    DEBUG: 2019/11/26 21:29:06 Checking in a loop for cluster status
    DEBUG: 2019/11/26 21:29:06 Found '3' nodes
    DEBUG: 2019/11/26 21:29:06 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/26 21:29:06 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/26 21:29:06 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/26 21:29:06 Creating network default
    DEBUG: 2019/11/26 21:29:06 Creating network blue
    DEBUG: 2019/11/26 21:29:06 Add default tag to default network
    DEBUG: 2019/11/26 21:29:16 Labeled all nodes with node=node$

    DEBUG: 2019/11/26 21:29:16 Getting cluster ID
    DEBUG: 2019/11/26 21:29:17 Created test cluster: a66dcf95-10d6-11ea-8b52-a4bf01194d67
    DEBUG: 2019/11/26 21:29:17 Deleting all LCVs, volumes, snapshots from previous cluster if any.
    DEBUG: 2019/11/26 21:29:17 Recording timestamp of all services on all nodes
    DEBUG: 2019/11/26 21:29:24 Overwritting e2e parameter : ExpectedBasicVnicUsageCount
    DEBUG: 2019/11/26 21:29:26 Checking if given pods are in Running state
    DEBUG: 2019/11/26 21:29:26 Checking if given pods are in Running state
    DEBUG: 2019/11/26 21:29:28 Checking if given pods are in Running state
    DEBUG: 2019/11/26 21:29:28 Checking if given pods are in Running state
    DEBUG: 2019/11/26 21:29:28 Checking if given pods are in Running state
    DEBUG: 2019/11/26 21:29:29 Updating inventory struct
    DEBUG: 2019/11/26 21:29:29 Creating storage classes
    DEBUG: 2019/11/26 21:29:39 rpm=diamanti-cx-2.3.0-55.x86_64
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mEndpoint.Basic Daily N_Endpoint-1.4 N_Endpoint-1.5[0m [90mEndpoint Basic testcases[0m 
  [1mCreate endpoints till limit is reached[0m
  [37m/gocode/main/test/e2e/tests/endpoint.go:86[0m
[BeforeEach] Endpoint Basic testcases
  /gocode/main/test/e2e/tests/endpoint.go:31
    DEBUG: 2019/11/26 21:29:39 Login to cluster
    DEBUG: 2019/11/26 21:29:40 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:29:40 Updating inventory struct
    DEBUG: 2019/11/26 21:29:40 Checking stale resources
    DEBUG: 2019/11/26 21:29:41 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:29:41 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:29:41 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:29:48 Creating storage classes
    DEBUG: 2019/11/26 21:29:57 START_TEST Endpoint.Basic
[It] Create endpoints till limit is reached
  /gocode/main/test/e2e/tests/endpoint.go:86
    DEBUG: 2019/11/26 21:29:57 Create endpoints till we reach a network's limit
    DEBUG: 2019/11/26 21:29:57 Create a valid network with 5 address.
    DEBUG: 2019/11/26 21:29:57 Creating 5 endpoints with newly created network
    DEBUG: 2019/11/26 21:29:57 Create endpoint 1.
    DEBUG: 2019/11/26 21:29:57 Create endpoint 2.
    DEBUG: 2019/11/26 21:29:57 Create endpoint 3.
    DEBUG: 2019/11/26 21:29:57 Create endpoint 4.
    DEBUG: 2019/11/26 21:29:57 Create endpoint 5.
    DEBUG: 2019/11/26 21:29:57 Try to create 6th endpoint.
    DEBUG: 2019/11/26 21:29:58 Endpoint create command failed: failed to run commmand 'dctl  -o json endpoint create ep6 -ns default -n networkname1 -l custom-endpoint=true', status:&{{ } {  0} Failure Failed to allocate IP address from pool networkname1, out of free addresses with err: No free bit  <nil> 500}, error:{
 "metadata": {},
 "status": "Failure",
 "message": "Failed to allocate IP address from pool networkname1, out of free addresses with err: No free bit",
 "code": 500
}



    DEBUG: 2019/11/26 21:29:58 Delete all the endpoints.
    DEBUG: 2019/11/26 21:29:58 Delete endpoint 1.
    DEBUG: 2019/11/26 21:29:58 Delete endpoint 2.
    DEBUG: 2019/11/26 21:29:58 Delete endpoint 3.
    DEBUG: 2019/11/26 21:29:58 Delete endpoint 4.
    DEBUG: 2019/11/26 21:29:58 Delete endpoint 5.
    DEBUG: 2019/11/26 21:29:58 Delete the newly added network.
[AfterEach] Endpoint Basic testcases
  /gocode/main/test/e2e/tests/endpoint.go:41
    DEBUG: 2019/11/26 21:29:58 END_TEST Endpoint.Basic Time-taken : 0.942280466
    DEBUG: 2019/11/26 21:29:58 Checking stale resources
    DEBUG: 2019/11/26 21:29:58 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:29:58 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:29:58 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:18.791 seconds][0m
Endpoint.Basic Daily N_Endpoint-1.4 N_Endpoint-1.5
[90m/gocode/main/test/e2e/tests/endpoint.go:23[0m
  Endpoint Basic testcases
  [90m/gocode/main/test/e2e/tests/endpoint.go:30[0m
    Create endpoints till limit is reached
    [90m/gocode/main/test/e2e/tests/endpoint.go:86[0m
[90m------------------------------[0m
[0mNetwork.PodCreateWithoutNetworkInPodSpec Daily N_Basic-1.17 N_Basic-1.18 Multizone[0m [90mCreating pod without specifying network in pod spec[0m 
  [1mCreate a pod without specifying network in podspec. And check its status if default network is available or it is not available.[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:2322[0m
[BeforeEach] Creating pod without specifying network in pod spec
  /gocode/main/test/e2e/tests/network-pod.go:2311
    DEBUG: 2019/11/26 21:29:58 START_TEST Network.PodCreateWithoutNetworkInPodSpec
    DEBUG: 2019/11/26 21:29:58 Login to cluster
    DEBUG: 2019/11/26 21:29:59 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:29:59 Updating inventory struct
    DEBUG: 2019/11/26 21:29:59 Checking stale resources
    DEBUG: 2019/11/26 21:29:59 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:29:59 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:29:59 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:30:07 Creating storage classes
[It] Create a pod without specifying network in podspec. And check its status if default network is available or it is not available.
  /gocode/main/test/e2e/tests/network-pod.go:2322
    DEBUG: 2019/11/26 21:30:16 Get list of networks tagged as default
    DEBUG: 2019/11/26 21:30:16 Remove default tag of default network
    DEBUG: 2019/11/26 21:30:16 Creating 1 pods of docker.io/redis:3.0.5 image without specifying any network in pod spec
    DEBUG: 2019/11/26 21:30:46 Verifying pod's state. Pod should be in Pending state.
    DEBUG: 2019/11/26 21:30:46 Add default tag to default network
    DEBUG: 2019/11/26 21:30:46 Checking if pod is in running state
    DEBUG: 2019/11/26 21:31:42 Verifying network usage count and Checking if IP of e2etest-pod-1 pod is in range of default tagged networks
    DEBUG: 2019/11/26 21:31:42 IP address ( 172.16.179.6 ) of e2etest-pod-1 is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/26 21:31:42 Deleting the pod
[AfterEach] Creating pod without specifying network in pod spec
  /gocode/main/test/e2e/tests/network-pod.go:2317
    DEBUG: 2019/11/26 21:31:56 END_TEST Network.PodCreateWithoutNetworkInPodSpec Time-taken : 118.275759035
    DEBUG: 2019/11/26 21:31:56 Checking stale resources
    DEBUG: 2019/11/26 21:31:56 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:31:56 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:31:56 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:118.417 seconds][0m
Network.PodCreateWithoutNetworkInPodSpec Daily N_Basic-1.17 N_Basic-1.18 Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:2305[0m
  Creating pod without specifying network in pod spec
  [90m/gocode/main/test/e2e/tests/network-pod.go:2306[0m
    Create a pod without specifying network in podspec. And check its status if default network is available or it is not available.
    [90m/gocode/main/test/e2e/tests/network-pod.go:2322[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries to create perf tier with invalid IOPs.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:182[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/26 21:31:57 Login to cluster
    DEBUG: 2019/11/26 21:31:57 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:31:57 Updating inventory struct
    DEBUG: 2019/11/26 21:31:58 Checking stale resources
    DEBUG: 2019/11/26 21:31:58 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:31:58 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:31:58 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:32:06 Creating storage classes
    DEBUG: 2019/11/26 21:32:14 START_TEST PerfTier.NegativeTests
[It] tries to create perf tier with invalid IOPs.
  /gocode/main/test/e2e/tests/perf-tier.go:182
    DEBUG: 2019/11/26 21:32:14 Try to create perf-tier with invalid IOPs.
    ERROR: 2019/11/26 21:32:14  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b 1G -i -1k', output:, error:Error: Invalid --storage-iops/-i specification.



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/26 21:32:14 END_TEST PerfTier.NegativeTests Time-taken: 0.025274568
    DEBUG: 2019/11/26 21:32:14 Checking stale resources
    DEBUG: 2019/11/26 21:32:14 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:32:14 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:32:14 Checking stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:17.883 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries to create perf tier with invalid IOPs.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:182[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries deleting best-effort QoS class.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:230[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/26 21:32:14 Login to cluster
    DEBUG: 2019/11/26 21:32:15 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:32:15 Updating inventory struct
    DEBUG: 2019/11/26 21:32:16 Checking stale resources
    DEBUG: 2019/11/26 21:32:16 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:32:16 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:32:16 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:32:23 Creating storage classes
    DEBUG: 2019/11/26 21:32:32 START_TEST PerfTier.NegativeTests
[It] tries deleting best-effort QoS class.
  /gocode/main/test/e2e/tests/perf-tier.go:230
[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/26 21:32:32 END_TEST PerfTier.NegativeTests Time-taken: 0.042477975
    DEBUG: 2019/11/26 21:32:32 Checking stale resources
    DEBUG: 2019/11/26 21:32:32 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:32:32 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:32:32 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:17.846 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries deleting best-effort QoS class.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:230[0m
[90m------------------------------[0m
[0mNetwork.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4[0m [90mPing pod's IP from outside world[0m 
  [1mPing pod's IP (created without using any network) from outside world and from its host[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:772[0m
[BeforeEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:700
    DEBUG: 2019/11/26 21:32:32 START_TEST Network.PingPodFromOutside
    DEBUG: 2019/11/26 21:32:32 Login to cluster
    DEBUG: 2019/11/26 21:32:33 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:32:33 Updating inventory struct
    DEBUG: 2019/11/26 21:32:33 Checking stale resources
    DEBUG: 2019/11/26 21:32:33 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:32:33 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:32:33 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:32:41 Creating storage classes
[It] Ping pod's IP (created without using any network) from outside world and from its host
  /gocode/main/test/e2e/tests/network-pod.go:772
    DEBUG: 2019/11/26 21:32:50 Creating 1 pods of docker.io/redis:3.0.5 image with network : none and qos : 
    DEBUG: 2019/11/26 21:32:52 Checking if e2etest-pod-1's IP is not reachable from local machine 
    DEBUG: 2019/11/26 21:32:52 Executing ping command: ping  -c 5 -W 5 172.20.0.6
    DEBUG: 2019/11/26 21:32:56 Checking if e2etest-pod-1's IP is reachable from its host appserv55 
    DEBUG: 2019/11/26 21:32:56 Executing ping command ping -c 5 -W 5 172.20.0.6 from appserv55
    DEBUG: 2019/11/26 21:33:01 172.20.0.6 is pingable from remote machine appserv55
    DEBUG: 2019/11/26 21:33:01 Deleting the pod: e2etest-pod-1
[AfterEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:709
    DEBUG: 2019/11/26 21:33:02 END_TEST Network.PingPodFromOutside Time-taken : 30.159256724
    DEBUG: 2019/11/26 21:33:02 Checking stale resources
    DEBUG: 2019/11/26 21:33:03 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:33:03 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:33:03 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:30.295 seconds][0m
Network.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4
[90m/gocode/main/test/e2e/tests/network-pod.go:694[0m
  Ping pod's IP from outside world
  [90m/gocode/main/test/e2e/tests/network-pod.go:695[0m
    Ping pod's IP (created without using any network) from outside world and from its host
    [90m/gocode/main/test/e2e/tests/network-pod.go:772[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9[0m [90mPing an external IP from from a pod[0m 
  [1mPing external IP from a pod created using private network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:517[0m
[BeforeEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:487
    DEBUG: 2019/11/26 21:33:03 START_TEST Network.PingExternalIP
    DEBUG: 2019/11/26 21:33:03 Login to cluster
    DEBUG: 2019/11/26 21:33:04 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:33:04 Updating inventory struct
    DEBUG: 2019/11/26 21:33:04 Checking stale resources
    DEBUG: 2019/11/26 21:33:04 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:33:04 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:33:04 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:33:12 Creating storage classes
[It] Ping external IP from a pod created using private network
  /gocode/main/test/e2e/tests/network-pod.go:517
    DEBUG: 2019/11/26 21:33:21 Creating private network : blue
    DEBUG: 2019/11/26 21:33:21 Creating 1 pods of docker.io/redis:3.0.5 image with network : blue and qos : high
    DEBUG: 2019/11/26 21:33:24 IP address ( 172.16.180.4 ) of e2etest-pod-1 is between 172.16.180.4 and 172.16.180.253

    DEBUG: 2019/11/26 21:33:24 Trying to ping google-public-dns-a.google.com from pod e2etest-pod-1
    DEBUG: 2019/11/26 21:33:24 google-public-dns-a.google.com is pingable from pod e2etest-pod-1 (172.16.180.4)
    DEBUG: 2019/11/26 21:33:24 Getting managment inteface from pod host ( appserv54 ) 
    DEBUG: 2019/11/26 21:33:25 Management interface of appserv54 is 172.20.0.1

    DEBUG: 2019/11/26 21:33:25 Matching default gateway of pod e2etest-pod-1 with 172.20.0.1 
    DEBUG: 2019/11/26 21:33:25 Default gateway of e2etest-pod-1 is 172.20.0.1
    DEBUG: 2019/11/26 21:33:25 Deleting the pod: e2etest-pod-1
    DEBUG: 2019/11/26 21:33:36 Deleting private network : blue
[AfterEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:496
    DEBUG: 2019/11/26 21:33:37 END_TEST Network.PingExternalIP Time-taken : 34.018590477
    DEBUG: 2019/11/26 21:33:37 Checking stale resources
    DEBUG: 2019/11/26 21:33:37 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:33:37 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:33:37 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:34.154 seconds][0m
Network.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9
[90m/gocode/main/test/e2e/tests/network-pod.go:480[0m
  Ping an external IP from from a pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:481[0m
    Ping external IP from a pod created using private network
    [90m/gocode/main/test/e2e/tests/network-pod.go:517[0m
[90m------------------------------[0m
[0mNetworkRemoteStorage.NicVFsSchedulingTrafficFlowOppositeDirection Daily AT_Scheduling-1.1 Multizone[0m [90mNetwork plus remote storage nic & VFs scheduling, Traffic flows in opposite direction[0m 
  [1mNetwork plus remote storage nic & VFs scheduling, Traffic flows in opposite direction[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:1954[0m
[BeforeEach] Network plus remote storage nic & VFs scheduling, Traffic flows in opposite direction
  /gocode/main/test/e2e/tests/network-pod.go:1938
    DEBUG: 2019/11/26 21:33:37 START_TEST NetworkRemoteStorage.NicVFsSchedulingTrafficFlowOppositeDirection
    DEBUG: 2019/11/26 21:33:37 Login to cluster
    DEBUG: 2019/11/26 21:33:37 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:33:37 Updating inventory struct
    DEBUG: 2019/11/26 21:33:38 Checking stale resources
    DEBUG: 2019/11/26 21:33:38 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:33:38 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:33:38 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:33:46 Creating storage classes
[It] Network plus remote storage nic & VFs scheduling, Traffic flows in opposite direction
  /gocode/main/test/e2e/tests/network-pod.go:1954
    DEBUG: 2019/11/26 21:33:55 Creating 1 pair of iperf client-server pod
    DEBUG: 2019/11/26 21:33:55 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/26 21:33:57 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/26 21:34:28 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/26 21:34:30 Getting pods scheduled on network nicIds
    DEBUG: 2019/11/26 21:34:30 Checking distribution of network pods across nicId(s)
    DEBUG: 2019/11/26 21:34:30 Pod scheduled as expected
    DEBUG: 2019/11/26 21:34:30 Creating fio pod and remote volume with high qos: 
    DEBUG: 2019/11/26 21:34:30 Create 1 fio pod(s):
    DEBUG: 2019/11/26 21:34:30 Creating dynamic pvc : fio-pod-hightest-vol1
    DEBUG: 2019/11/26 21:34:31 Created PVC successfully.
    DEBUG: 2019/11/26 21:34:31 Creating fio pod: fio-pod-high-1
    DEBUG: 2019/11/26 21:34:31 Checking if given pods are in Running state
    DEBUG: 2019/11/26 21:34:42 Getting pods and volumes scheduled on storage nicIds
    DEBUG: 2019/11/26 21:34:42 Checking distribution of storage pods across nicId(s)
    DEBUG: 2019/11/26 21:34:42 Pod scheduled as expected
    DEBUG: 2019/11/26 21:34:42 Deleting all the pods: 
    DEBUG: 2019/11/26 21:35:20 Waitting for volume to move to "Available" state
    DEBUG: 2019/11/26 21:35:20 Delete PVCs: 
    DEBUG: 2019/11/26 21:35:21 Waiting for volumes to get deleted: 
[AfterEach] Network plus remote storage nic & VFs scheduling, Traffic flows in opposite direction
  /gocode/main/test/e2e/tests/network-pod.go:1949
    DEBUG: 2019/11/26 21:36:10 END_TEST NetworkRemoteStorage.NicVFsSchedulingTrafficFlowOppositeDirection Time-taken : 153.569568037
    DEBUG: 2019/11/26 21:36:10 Checking stale resources
    DEBUG: 2019/11/26 21:36:10 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:36:10 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:36:10 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:153.720 seconds][0m
NetworkRemoteStorage.NicVFsSchedulingTrafficFlowOppositeDirection Daily AT_Scheduling-1.1 Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:1932[0m
  Network plus remote storage nic & VFs scheduling, Traffic flows in opposite direction
  [90m/gocode/main/test/e2e/tests/network-pod.go:1933[0m
    Network plus remote storage nic & VFs scheduling, Traffic flows in opposite direction
    [90m/gocode/main/test/e2e/tests/network-pod.go:1954[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mRbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 [0m [90mValidate all Edit and View roles[0m 
  [1mValidate user-view role[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:489[0m
[BeforeEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:447
    DEBUG: 2019/11/26 21:36:10 START_TEST Rbac.RoleValidation
    DEBUG: 2019/11/26 21:36:10 Login to cluster
    DEBUG: 2019/11/26 21:36:11 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:36:11 Updating inventory struct
    DEBUG: 2019/11/26 21:36:12 Checking stale resources
    DEBUG: 2019/11/26 21:36:12 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:36:12 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:36:12 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:36:20 Creating storage classes
[It] Validate user-view role
  /gocode/main/test/e2e/tests/rbac.go:489
    DEBUG: 2019/11/26 21:36:28 Validate user-view role
    DEBUG: 2019/11/26 21:36:29 Creating group grp1 with user-view role(s)
    DEBUG: 2019/11/26 21:36:29 Creating user1 user in grp1 group
    DEBUG: 2019/11/26 21:36:29 Login as user1 user
    DEBUG: 2019/11/26 21:36:30 Operation validated : perftier-list
    ERROR: 2019/11/26 21:36:30  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create temp-pf -b 2G -i 50K', status:&{{Status } {  0} Failure POST on perftiers for "user1" is forbidden: User user1 cannot perform POST on perftiers Forbidden 0xc0005ae640 403}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "POST on perftiers for \"user1\" is forbidden: User user1 cannot perform POST on perftiers",
 "reason": "Forbidden",
 "details": {
  "name": "user1",
  "kind": "perftiers"
 },
 "code": 403
}



    DEBUG: 2019/11/26 21:36:30 Operation validated : perftier-create
    DEBUG: 2019/11/26 21:36:32 Operation validated : perftier-delete
    DEBUG: 2019/11/26 21:36:41 Operation validated : volume-list
    DEBUG: 2019/11/26 21:36:41 Operation validated : volume-create
    DEBUG: 2019/11/26 21:37:11 Operation validated : volume-delete
    DEBUG: 2019/11/26 21:37:13 Operation validated : user-list
    DEBUG: 2019/11/26 21:37:13 Operation validated : user-create
    DEBUG: 2019/11/26 21:37:17 Operation validated : user-delete
[AfterEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:456
    DEBUG: 2019/11/26 21:37:18 END_TEST Rbac.RoleValidation Time-taken : 67.157000621
    DEBUG: 2019/11/26 21:37:18 Checking stale resources
    DEBUG: 2019/11/26 21:37:18 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:37:18 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:37:18 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:67.297 seconds][0m
Rbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 
[90m/gocode/main/test/e2e/tests/rbac.go:437[0m
  Validate all Edit and View roles
  [90m/gocode/main/test/e2e/tests/rbac.go:438[0m
    Validate user-view role
    [90m/gocode/main/test/e2e/tests/rbac.go:489[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mLocalStorage.MultiplePodsMultipleVolumes Daily SP_Basic-1.3 Qos[0m [90mMultiple pods, Multiple volumes.[0m 
  [1mMultiple Pods, Multiple simple volumes.[0m
  [37m/gocode/main/test/e2e/tests/volume.go:3070[0m
[BeforeEach] Multiple pods, Multiple volumes.
  /gocode/main/test/e2e/tests/volume.go:3058
    DEBUG: 2019/11/26 21:37:18 START_TEST LocalStorage.MultiplePodsMultipleVolumes
    DEBUG: 2019/11/26 21:37:18 Login to cluster
    DEBUG: 2019/11/26 21:37:18 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:37:18 Updating inventory struct
    DEBUG: 2019/11/26 21:37:19 Checking stale resources
    DEBUG: 2019/11/26 21:37:19 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:37:19 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:37:19 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:37:27 Creating storage classes
[It] Multiple Pods, Multiple simple volumes.
  /gocode/main/test/e2e/tests/volume.go:3070
    DEBUG: 2019/11/26 21:37:36 Creating 16 Dynamic Persistent Volume Claims (PVCs). Mirror count: 1. Selector: node=node2
    DEBUG: 2019/11/26 21:37:36 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:36 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:37 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:37 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:37 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:37 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:38 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:38 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:38 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:39 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:39 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:39 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:40 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:40 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:40 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:41 Created PVC successfully.
    DEBUG: 2019/11/26 21:37:44 Creating 4 fio pods: 
    DEBUG: 2019/11/26 21:37:45 Checking if given pods are in Running state
    DEBUG: 2019/11/26 21:38:27 Wait for volumes to move into attached state: 
    DEBUG: 2019/11/26 21:38:27 Waitting for volume to move to "Attached" state
    DEBUG: 2019/11/26 21:38:28 Sleeping for 180 seconds, so that prometheus will have some stats
    DEBUG: 2019/11/26 21:41:28 Validating qos associated with each volume: 
    DEBUG: 2019/11/26 21:41:30 Deleting pods : 
    DEBUG: 2019/11/26 21:42:36 Wait for volumes to come in Available state: 
    DEBUG: 2019/11/26 21:42:37 Delete PVCs: 
    DEBUG: 2019/11/26 21:42:40 Waiting for volumes to get deleted: 
[AfterEach] Multiple pods, Multiple volumes.
  /gocode/main/test/e2e/tests/volume.go:3065
    DEBUG: 2019/11/26 21:45:12 END_TEST LocalStorage.MultiplePodsMultipleVolumes Time-taken : 474.33505683
    DEBUG: 2019/11/26 21:45:12 Checking stale resources
    DEBUG: 2019/11/26 21:45:12 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:45:12 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:45:12 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:474.481 seconds][0m
LocalStorage.MultiplePodsMultipleVolumes Daily SP_Basic-1.3 Qos
[90m/gocode/main/test/e2e/tests/volume.go:3051[0m
  Multiple pods, Multiple volumes.
  [90m/gocode/main/test/e2e/tests/volume.go:3053[0m
    Multiple Pods, Multiple simple volumes.
    [90m/gocode/main/test/e2e/tests/volume.go:3070[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRemoteStorage.RebootTarget Daily RS_Reboot-1.3[0m [90mreboot target node after running IO and verify data before and after reboot[0m 
  [1mreboot target node after running IO and verify data before and after reboot[0m
  [37m/gocode/main/test/e2e/tests/volume.go:2575[0m
[BeforeEach] reboot target node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2560
    DEBUG: 2019/11/26 21:45:12 START_TEST RemoteStorage.RebootTarget
    DEBUG: 2019/11/26 21:45:12 Login to cluster
    DEBUG: 2019/11/26 21:45:13 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:45:13 Updating inventory struct
    DEBUG: 2019/11/26 21:45:13 Checking stale resources
    DEBUG: 2019/11/26 21:45:13 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:45:13 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:45:13 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:45:21 Creating storage classes
[It] reboot target node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2575
    DEBUG: 2019/11/26 21:45:30 Verifying whether FBM and L1 usage is zero across all nodes
    DEBUG: 2019/11/26 21:45:36 FBM and L1 usage is Zero across all nodes

    DEBUG: 2019/11/26 21:45:36 Creating 4 volumes of random sizes
    DEBUG: 2019/11/26 21:45:36 Mirror Count: 1
    DEBUG: 2019/11/26 21:45:37 Attaching all 4 volumes
    DEBUG: 2019/11/26 21:45:54 Initiator node : appserv54
    DEBUG: 2019/11/26 21:45:54 Nodes to reboot :[appserv53]
    DEBUG: 2019/11/26 21:45:56 Running WRITE fio job on node : appserv54
    DEBUG: 2019/11/26 21:45:56 FIO Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"akpz\"-12 --verify_interval=4096 --runtime=120 --blocksize=64K --iodepth=16  --time_based  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1

    DEBUG: 2019/11/26 21:47:58 Running VERIFY IOs on all plexes
    DEBUG: 2019/11/26 21:47:58 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"akpz\"-12 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=64K --iodepth=16  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1
    DEBUG: 2019/11/26 21:48:06 Getting cluster quorum nodes
    DEBUG: 2019/11/26 21:48:06 Powering OFF the node appserv53
    DEBUG: 2019/11/26 21:48:06 Node 172.16.6.153 took 0 seconds to power off
    DEBUG: 2019/11/26 21:48:06 Ensuring that appserv53 node is unreachable: 
    DEBUG: 2019/11/26 21:48:06 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/26 21:48:15 Polling to check until node: appserv53 goes down
    DEBUG: 2019/11/26 21:48:45 Error: . Retrying once again...
    DEBUG: 2019/11/26 21:49:48 Powering ON the node appserv53
    DEBUG: 2019/11/26 21:49:48 Node 172.16.6.153 took 0 seconds to power on
    DEBUG: 2019/11/26 21:49:48 Checking if node appserv53 is reachable or not: 
    DEBUG: 2019/11/26 21:49:48 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/26 21:50:07 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/26 21:50:24 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/26 21:50:41 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/26 21:50:58 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/26 21:51:15 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/26 21:51:32 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/26 21:51:49 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/26 21:51:53 appserv53 is pingable from local machine
    DEBUG: 2019/11/26 21:51:53 Checking ssh port is up or not on node: appserv53
    DEBUG: 2019/11/26 21:52:33 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/26 21:52:33 Found '3' nodes
    DEBUG: 2019/11/26 21:52:33 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/26 21:53:18 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/26 21:53:18 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/26 21:53:28 After power cycle/reboot, updating timestamp of node : appserv53
    DEBUG: 2019/11/26 21:53:31 Getting cluster quorum nodes
    DEBUG: 2019/11/26 21:54:31 Updating inventory struct
    DEBUG: 2019/11/26 21:54:32 Waiting for nodes to come up, will wait upto 800 seconds
    DEBUG: 2019/11/26 21:54:44 Nodes are up, waiting for armada to start
.
    DEBUG: 2019/11/26 21:55:54 Waiting for the nodes to go into Ready state
    DEBUG: 2019/11/26 21:55:54 Found '3' nodes
    DEBUG: 2019/11/26 21:55:54 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/26 21:55:54 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/26 21:55:54 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/26 21:55:54 Waiting for volumes to come into Attached state after rebooting cluster nodes.
    DEBUG: 2019/11/26 21:55:54 Detaching all 4 volumes
    DEBUG: 2019/11/26 21:55:56 Re-attaching all 4 volumes
    DEBUG: 2019/11/26 21:56:15 Comparing Volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/26 21:56:17 Comparing the device path & uuid on initiator before and after reboot
    DEBUG: 2019/11/26 21:56:21 Running VERIFY IOs on all plexes
    DEBUG: 2019/11/26 21:56:21 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"akpz\"-12 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=64K --iodepth=16  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1
    DEBUG: 2019/11/26 21:56:27 Successfully completed Verification on all the volumes
    DEBUG: 2019/11/26 21:56:27 Detach & Delete all volumes
[AfterEach] reboot target node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2570
    DEBUG: 2019/11/26 21:57:15 END_TEST RemoteStorage.RebootTarget Time-taken : 722.707898424
    DEBUG: 2019/11/26 21:57:15 Checking stale resources
    DEBUG: 2019/11/26 21:57:15 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:57:15 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:57:15 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:722.860 seconds][0m
RemoteStorage.RebootTarget Daily RS_Reboot-1.3
[90m/gocode/main/test/e2e/tests/volume.go:2553[0m
  reboot target node after running IO and verify data before and after reboot
  [90m/gocode/main/test/e2e/tests/volume.go:2555[0m
    reboot target node after running IO and verify data before and after reboot
    [90m/gocode/main/test/e2e/tests/volume.go:2575[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.EditView Daily Rbac_Local_Basic-2.0[0m [90mUser can edit/view in it's namespace[0m 
  [1mUser can edit/view perf-tier(s)[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:659[0m
[BeforeEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:528
    DEBUG: 2019/11/26 21:57:15 START_TEST Rbac.EditView
    DEBUG: 2019/11/26 21:57:15 Login to cluster
    DEBUG: 2019/11/26 21:57:16 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:57:16 Updating inventory struct
    DEBUG: 2019/11/26 21:57:16 Checking stale resources
    DEBUG: 2019/11/26 21:57:16 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:57:16 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:57:16 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:57:24 Creating storage classes
    DEBUG: 2019/11/26 21:57:34 User Logout
[It] User can edit/view perf-tier(s)
  /gocode/main/test/e2e/tests/rbac.go:659
    DEBUG: 2019/11/26 21:57:35 Creating group, user with role(s)
    DEBUG: 2019/11/26 21:57:35 Creating group jacksgroup with perftier-edit role(s)
    DEBUG: 2019/11/26 21:57:36 Creating jack user in jacksgroup group
    DEBUG: 2019/11/26 21:57:36 Login as jack user
    DEBUG: 2019/11/26 21:57:36 Try to create perf-tier test-perftier with perftier-edit role
    DEBUG: 2019/11/26 21:57:36 Try to list perf-tier.
    DEBUG: 2019/11/26 21:57:36 Editing group role(s)
    DEBUG: 2019/11/26 21:57:37 Editing jacksgroup group with perftier-view role(s)
    DEBUG: 2019/11/26 21:57:37 Login as jack user
    DEBUG: 2019/11/26 21:57:37 Try to list perf-tier.
    DEBUG: 2019/11/26 21:57:38 Try to create perf-tier with perftier-view role.
    ERROR: 2019/11/26 21:57:38  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create test-perftier-1 -b 1G -i 50k', status:&{{Status } {  0} Failure POST on perftiers for "jack" is forbidden: User jack cannot perform POST on perftiers Forbidden 0xc00019a6e0 403}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "POST on perftiers for \"jack\" is forbidden: User jack cannot perform POST on perftiers",
 "reason": "Forbidden",
 "details": {
  "name": "jack",
  "kind": "perftiers"
 },
 "code": 403
}



    DEBUG: 2019/11/26 21:57:38 Editing group role(s)
    DEBUG: 2019/11/26 21:57:38 Editing jacksgroup group with perftier-edit role(s)
    DEBUG: 2019/11/26 21:57:38 Login as jack user
    DEBUG: 2019/11/26 21:57:39 Try to delete perf-tier test-perftier
[AfterEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:539
    DEBUG: 2019/11/26 21:57:39 User Logout
    DEBUG: 2019/11/26 21:57:40 END_TEST Rbac.EditView Time-taken : 24.628991275
    DEBUG: 2019/11/26 21:57:40 Checking stale resources
    DEBUG: 2019/11/26 21:57:40 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:57:40 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:57:40 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:24.761 seconds][0m
Rbac.EditView Daily Rbac_Local_Basic-2.0
[90m/gocode/main/test/e2e/tests/rbac.go:518[0m
  User can edit/view in it's namespace
  [90m/gocode/main/test/e2e/tests/rbac.go:520[0m
    User can edit/view perf-tier(s)
    [90m/gocode/main/test/e2e/tests/rbac.go:659[0m
[90m------------------------------[0m
[0mNetwork.GatewayPing Daily N_Basic-1.15 N_Basic-1.16 Multizone[0m [90mPing gateway of a network from a pod[0m 
  [1mPing gateway of a network(with valid vlan) from a pod[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:389[0m
[BeforeEach] Ping gateway of a network from a pod
  /gocode/main/test/e2e/tests/network-pod.go:373
    DEBUG: 2019/11/26 21:57:40 START_TEST Network.GatewayPing
    DEBUG: 2019/11/26 21:57:40 Login to cluster
    DEBUG: 2019/11/26 21:57:41 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:57:41 Updating inventory struct
    DEBUG: 2019/11/26 21:57:41 Checking stale resources
    DEBUG: 2019/11/26 21:57:41 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:57:41 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:57:41 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:57:49 Creating storage classes
    DEBUG: 2019/11/26 21:57:58 Creating a test pod with docker.io/redis:3.0.5 image, default network and with valid VLAN
[It] Ping gateway of a network(with valid vlan) from a pod
  /gocode/main/test/e2e/tests/network-pod.go:389
    DEBUG: 2019/11/26 21:58:00 Gateway IP of default network is 172.16.179.1
    DEBUG: 2019/11/26 21:58:00 Trying to ping the gateway of network with valid VLAN from pod e2etest-pod
    DEBUG: 2019/11/26 21:58:01 172.16.179.1 is pingable from pod e2etest-pod (172.16.179.6)
[AfterEach] Ping gateway of a network from a pod
  /gocode/main/test/e2e/tests/network-pod.go:427
    DEBUG: 2019/11/26 21:58:01 Deleting the pod
    DEBUG: 2019/11/26 21:58:16 END_TEST Network.GatewayPing Time-taken : 36.559838537
    DEBUG: 2019/11/26 21:58:16 Checking stale resources
    DEBUG: 2019/11/26 21:58:16 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:58:16 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:58:16 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:36.700 seconds][0m
Network.GatewayPing Daily N_Basic-1.15 N_Basic-1.16 Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:366[0m
  Ping gateway of a network from a pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:367[0m
    Ping gateway of a network(with valid vlan) from a pod
    [90m/gocode/main/test/e2e/tests/network-pod.go:389[0m
[90m------------------------------[0m
[0mCluster.RemoveNodeWithPods Management Daily M_Cluster-1.5[0m [90mwhen a node is decommissioned and removed from a cluster[0m 
  [1mshould be created, decommissioned, and removed[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:366[0m
[BeforeEach] when a node is decommissioned and removed from a cluster
  /gocode/main/test/e2e/tests/cluster.go:347
    DEBUG: 2019/11/26 21:58:17 START_TEST Cluster.RemoveNodeWithPods
[AfterEach] when a node is decommissioned and removed from a cluster
  /gocode/main/test/e2e/tests/cluster.go:361
    DEBUG: 2019/11/26 21:58:17 END_TEST Cluster.RemoveNodeWithPods Time-taken : 0.000442834
    DEBUG: 2019/11/26 21:58:17 Checking stale resources
    DEBUG: 2019/11/26 21:58:17 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:58:17 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:58:17 Checking stale resources on the node: appserv55

[36m[1mS [SKIPPING] in Spec Setup (BeforeEach) [0.135 seconds][0m
Cluster.RemoveNodeWithPods Management Daily M_Cluster-1.5
[90m/gocode/main/test/e2e/tests/cluster.go:342[0m
  when a node is decommissioned and removed from a cluster
  [90m/gocode/main/test/e2e/tests/cluster.go:343[0m
    [36m[1mshould be created, decommissioned, and removed [BeforeEach][0m
    [90m/gocode/main/test/e2e/tests/cluster.go:366[0m

    [36mSkipping remove node with pods for now[0m

    /gocode/main/test/e2e/tests/cluster.go:351
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries to create perf tier with IOPs only.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:168[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/26 21:58:17 Login to cluster
    DEBUG: 2019/11/26 21:58:17 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:58:17 Updating inventory struct
    DEBUG: 2019/11/26 21:58:18 Checking stale resources
    DEBUG: 2019/11/26 21:58:18 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:58:18 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:58:18 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:58:26 Creating storage classes
    DEBUG: 2019/11/26 21:58:34 START_TEST PerfTier.NegativeTests
[It] tries to create perf tier with IOPs only.
  /gocode/main/test/e2e/tests/perf-tier.go:168
    DEBUG: 2019/11/26 21:58:34 Try to create perf-tier with IOPs only.
    ERROR: 2019/11/26 21:58:34  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b  -i 50k', output:, error:Error: example usage -  dctl perf-tier create perf-tier1 -i 1k -b 1G -l type=backend 
   storage IOPS should be in kilobytes 
   Network bandwidth should be in gigabytes



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/26 21:58:34 END_TEST PerfTier.NegativeTests Time-taken: 0.026221592
    DEBUG: 2019/11/26 21:58:34 Checking stale resources
    DEBUG: 2019/11/26 21:58:34 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:58:34 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:58:34 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:17.759 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries to create perf tier with IOPs only.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:168[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries to create perf tier with bandwidth only.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:162[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/26 21:58:34 Login to cluster
    DEBUG: 2019/11/26 21:58:35 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:58:35 Updating inventory struct
    DEBUG: 2019/11/26 21:58:36 Checking stale resources
    DEBUG: 2019/11/26 21:58:36 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:58:36 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:58:36 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:58:43 Creating storage classes
    DEBUG: 2019/11/26 21:58:52 START_TEST PerfTier.NegativeTests
[It] tries to create perf tier with bandwidth only.
  /gocode/main/test/e2e/tests/perf-tier.go:162
    DEBUG: 2019/11/26 21:58:52 Try to create perf-tier with bandwidth only.
    ERROR: 2019/11/26 21:58:52  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b 1G -i ', output:, error:Error: example usage -  dctl perf-tier create perf-tier1 -i 1k -b 1G -l type=backend 
   storage IOPS should be in kilobytes 
   Network bandwidth should be in gigabytes



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/26 21:58:52 END_TEST PerfTier.NegativeTests Time-taken: 0.021842299
    DEBUG: 2019/11/26 21:58:52 Checking stale resources
    DEBUG: 2019/11/26 21:58:52 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:58:52 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:58:52 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:17.619 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries to create perf tier with bandwidth only.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:162[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mVolume.NameLengthCheck Management Daily M_Volume-1.1[0m [90mwhen volume name is[0m 
  [1mgreater than defined character limit[0m
  [37m/gocode/main/test/e2e/tests/volume.go:256[0m
[BeforeEach] when volume name is
  /gocode/main/test/e2e/tests/volume.go:220
    DEBUG: 2019/11/26 21:58:52 START_TEST Volume.NameLengthCheck
    DEBUG: 2019/11/26 21:58:52 Login to cluster
    DEBUG: 2019/11/26 21:58:53 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:58:53 Updating inventory struct
    DEBUG: 2019/11/26 21:58:53 Checking stale resources
    DEBUG: 2019/11/26 21:58:53 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:58:53 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:58:53 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:59:01 Creating storage classes
[It] greater than defined character limit
  /gocode/main/test/e2e/tests/volume.go:256
    DEBUG: 2019/11/26 21:59:10 Creating the volume
[AfterEach] when volume name is
  /gocode/main/test/e2e/tests/volume.go:228
    DEBUG: 2019/11/26 21:59:10 END_TEST Volume.NameLengthCheck Time-taken : 17.609406055
    DEBUG: 2019/11/26 21:59:10 Checking stale resources
    DEBUG: 2019/11/26 21:59:10 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:59:10 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:59:10 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:17.746 seconds][0m
Volume.NameLengthCheck Management Daily M_Volume-1.1
[90m/gocode/main/test/e2e/tests/volume.go:210[0m
  when volume name is
  [90m/gocode/main/test/e2e/tests/volume.go:213[0m
    greater than defined character limit
    [90m/gocode/main/test/e2e/tests/volume.go:256[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mRbac.Role Daily Rbac_Custom-1.0[0m [90mrbac role test[0m 
  [1mcustom role test[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:348[0m
[BeforeEach] rbac role test
  /gocode/main/test/e2e/tests/rbac.go:334
    DEBUG: 2019/11/26 21:59:10 START_TEST Rbac.Role
    DEBUG: 2019/11/26 21:59:10 Login to cluster
    DEBUG: 2019/11/26 21:59:10 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:59:10 Updating inventory struct
    DEBUG: 2019/11/26 21:59:11 Checking stale resources
    DEBUG: 2019/11/26 21:59:11 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:59:11 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:59:11 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:59:19 Creating storage classes
[It] custom role test
  /gocode/main/test/e2e/tests/rbac.go:348
    DEBUG: 2019/11/26 21:59:28 Create group
    DEBUG: 2019/11/26 21:59:28 Create user
    DEBUG: 2019/11/26 21:59:28 Login as user
    DEBUG: 2019/11/26 21:59:29 List users
    DEBUG: 2019/11/26 21:59:29 List groups
    DEBUG: 2019/11/26 21:59:29 Create group
    DEBUG: 2019/11/26 21:59:29 Create user
    DEBUG: 2019/11/26 21:59:29 List roles
    DEBUG: 2019/11/26 21:59:29 Create role
    DEBUG: 2019/11/26 21:59:29 List Networks
    DEBUG: 2019/11/26 21:59:29 Create Network
    DEBUG: 2019/11/26 21:59:29 Login as user
    DEBUG: 2019/11/26 21:59:30 Delete group
[AfterEach] rbac role test
  /gocode/main/test/e2e/tests/rbac.go:342
    DEBUG: 2019/11/26 21:59:31 END_TEST Rbac.Role Time-taken : 20.934256007
    DEBUG: 2019/11/26 21:59:31 Checking stale resources
    DEBUG: 2019/11/26 21:59:31 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:59:31 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:59:31 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:21.083 seconds][0m
Rbac.Role Daily Rbac_Custom-1.0
[90m/gocode/main/test/e2e/tests/rbac.go:326[0m
  rbac role test
  [90m/gocode/main/test/e2e/tests/rbac.go:329[0m
    custom role test
    [90m/gocode/main/test/e2e/tests/rbac.go:348[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mMirroring.OnlinePlexAdd  Daily SM_PlexAdd-1.10[0m [90mCreate several simple volumes and add [online] plexes to these volumes.[0m 
  [1mCreate simple volumes, do IOs, add plexes, validate data on each plex of each volume.[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:1857[0m
[BeforeEach] Create several simple volumes and add [online] plexes to these volumes.
  /gocode/main/test/e2e/tests/mirroring.go:1843
    DEBUG: 2019/11/26 21:59:31 START_TEST Mirroring.OnlinePlexAdd
    DEBUG: 2019/11/26 21:59:31 Login to cluster
    DEBUG: 2019/11/26 21:59:31 Checking basic Vnic usage
    DEBUG: 2019/11/26 21:59:32 Updating inventory struct
    DEBUG: 2019/11/26 21:59:32 Checking stale resources
    DEBUG: 2019/11/26 21:59:32 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 21:59:32 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 21:59:32 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 21:59:40 Creating storage classes
[It] Create simple volumes, do IOs, add plexes, validate data on each plex of each volume.
  /gocode/main/test/e2e/tests/mirroring.go:1857
    DEBUG: 2019/11/26 21:59:48 Creating 8 volumes. Mirror Count: 1:
    DEBUG: 2019/11/26 21:59:48 Mirror Count: 1
    DEBUG: 2019/11/26 21:59:50 Attaching volumes: 
    DEBUG: 2019/11/26 22:00:24 Running write fio job on all volumes: 
    DEBUG: 2019/11/26 22:00:24 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:00:24 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:00:24 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:02:25 Adding new plex to each simple volumes. Expected PlexCount 2: 
    DEBUG: 2019/11/26 22:02:27 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:02:27 Volume name & Plex : test-vol1.p1. Plex State : Resyncing
    DEBUG: 2019/11/26 22:02:29 Volume "test-vol1" has index "0" in embedded.
    DEBUG: 2019/11/26 22:02:29 Volume: test-vol1. Resync offset: 69

    DEBUG: 2019/11/26 22:02:30 Volume: test-vol1. Resync offset: 81

    DEBUG: 2019/11/26 22:02:30 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/26 22:03:00 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:03:00 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:03:00 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/26 22:03:00 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:03:00 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:03:00 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/26 22:03:00 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:03:00 Volume name & Plex : test-vol3.p1. Plex State : Resyncing
    DEBUG: 2019/11/26 22:03:01 Volume "test-vol3" has index "0" in embedded.
    DEBUG: 2019/11/26 22:03:02 Volume: test-vol3. Resync offset: 70

    DEBUG: 2019/11/26 22:03:03 Volume: test-vol3. Resync offset: 71

    DEBUG: 2019/11/26 22:03:03 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/26 22:03:33 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:03:33 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:03:33 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/26 22:03:33 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:03:33 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:03:33 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/26 22:03:33 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:03:33 Volume name & Plex : test-vol5.p1. Plex State : Resyncing
    DEBUG: 2019/11/26 22:03:34 Volume "test-vol5" has index "1" in embedded.
    DEBUG: 2019/11/26 22:03:35 Volume: test-vol5. Resync offset: 84

    DEBUG: 2019/11/26 22:03:36 Volume: test-vol5. Resync offset: 85

    DEBUG: 2019/11/26 22:03:36 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/26 22:04:06 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:04:06 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:04:06 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/26 22:04:06 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:04:06 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:04:06 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/26 22:04:06 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:04:06 Volume name & Plex : test-vol7.p1. Plex State : Resyncing
    DEBUG: 2019/11/26 22:04:07 Volume "test-vol7" has index "2" in embedded.
    DEBUG: 2019/11/26 22:04:08 Volume: test-vol7. Resync offset: 94

    DEBUG: 2019/11/26 22:04:09 Volume: test-vol7. Resync offset: 95

    DEBUG: 2019/11/26 22:04:09 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/26 22:04:39 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:04:39 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:04:39 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/26 22:04:39 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:04:39 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:04:39 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/26 22:04:39 Running read fio job on all volumes: 
    DEBUG: 2019/11/26 22:04:40 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 22:04:40 Changing preferred plex of volume: test-vol3. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 22:04:40 Changing preferred plex of volume: test-vol2. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 22:04:41 Changing preferred plex of volume: test-vol6. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 22:04:42 Changing preferred plex of volume: test-vol4. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 22:04:42 Changing preferred plex of volume: test-vol5. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 22:04:42 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:04:43 Changing preferred plex of volume: test-vol7. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/26 22:04:43 Changing preferred plex of volume: test-vol8. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/26 22:04:44 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:04:44 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:05:29 Changing preferred plex of volume: test-vol3. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 22:05:30 Changing preferred plex of volume: test-vol5. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 22:05:31 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:05:37 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 22:05:38 Changing preferred plex of volume: test-vol6. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 22:05:40 Changing preferred plex of volume: test-vol7. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/26 22:05:41 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:05:41 Changing preferred plex of volume: test-vol2. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 22:05:42 Changing preferred plex of volume: test-vol4. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 22:05:44 Changing preferred plex of volume: test-vol8. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/26 22:05:44 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:07:26 Adding new plex to each simple volumes. Expected PlexCount 3: 
    DEBUG: 2019/11/26 22:07:27 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:07:27 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:07:27 Volume name & Plex : test-vol1.p2. Plex State : Resyncing
    DEBUG: 2019/11/26 22:07:29 Volume "test-vol1" has index "0" in embedded.
    DEBUG: 2019/11/26 22:07:29 Volume: test-vol1. Resync offset: 55

    DEBUG: 2019/11/26 22:07:30 Volume: test-vol1. Resync offset: 64

    DEBUG: 2019/11/26 22:07:30 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/26 22:08:00 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:08:00 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:08:00 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/26 22:08:00 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/26 22:08:00 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:08:00 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:08:00 Volume name & Plex : test-vol2.p2. Plex State : InUse
    DEBUG: 2019/11/26 22:08:00 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/26 22:08:00 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:08:00 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:08:00 Volume name & Plex : test-vol3.p2. Plex State : Resyncing
    DEBUG: 2019/11/26 22:08:02 Volume "test-vol3" has index "0" in embedded.
    DEBUG: 2019/11/26 22:08:02 Volume: test-vol3. Resync offset: 76

    DEBUG: 2019/11/26 22:08:03 Volume: test-vol3. Resync offset: 77

    DEBUG: 2019/11/26 22:08:03 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/26 22:08:33 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:08:33 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:08:33 Volume name & Plex : test-vol3.p2. Plex State : InUse
    DEBUG: 2019/11/26 22:08:33 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/26 22:08:33 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:08:33 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:08:33 Volume name & Plex : test-vol4.p2. Plex State : InUse
    DEBUG: 2019/11/26 22:08:33 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/26 22:08:33 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:08:33 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:08:33 Volume name & Plex : test-vol5.p2. Plex State : Resyncing
    DEBUG: 2019/11/26 22:08:35 Volume "test-vol5" has index "1" in embedded.
    DEBUG: 2019/11/26 22:08:35 Volume: test-vol5. Resync offset: 87

    DEBUG: 2019/11/26 22:08:36 Volume: test-vol5. Resync offset: 88

    DEBUG: 2019/11/26 22:08:36 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/26 22:09:06 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:09:06 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:09:06 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/26 22:09:06 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/26 22:09:06 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:09:06 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:09:06 Volume name & Plex : test-vol6.p2. Plex State : InUse
    DEBUG: 2019/11/26 22:09:06 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/26 22:09:06 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:09:06 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:09:06 Volume name & Plex : test-vol7.p2. Plex State : Resyncing
    DEBUG: 2019/11/26 22:09:07 Volume "test-vol7" has index "2" in embedded.
    DEBUG: 2019/11/26 22:09:08 Volume: test-vol7. Resync offset: 92

    DEBUG: 2019/11/26 22:09:09 Volume: test-vol7. Resync offset: 92

    DEBUG: 2019/11/26 22:09:15 Volume: test-vol7. Resync offset: 98

    DEBUG: 2019/11/26 22:09:15 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/26 22:09:45 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:09:45 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:09:45 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/26 22:09:45 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/26 22:09:45 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:09:45 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:09:45 Volume name & Plex : test-vol8.p2. Plex State : InUse
    DEBUG: 2019/11/26 22:09:45 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/26 22:09:45 Running read fio job on all volumes: 
    DEBUG: 2019/11/26 22:09:46 Changing preferred plex of volume: test-vol3. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 22:09:46 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 22:09:46 Changing preferred plex of volume: test-vol2. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 22:09:48 Changing preferred plex of volume: test-vol4. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 22:09:48 Changing preferred plex of volume: test-vol5. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 22:09:48 Changing preferred plex of volume: test-vol6. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 22:09:48 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:09:49 Changing preferred plex of volume: test-vol8. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/26 22:09:49 Changing preferred plex of volume: test-vol7. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/26 22:09:50 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:09:50 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:10:37 Changing preferred plex of volume: test-vol3. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 22:10:38 Changing preferred plex of volume: test-vol5. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 22:10:39 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:10:42 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 22:10:44 Changing preferred plex of volume: test-vol6. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 22:10:45 Changing preferred plex of volume: test-vol7. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/26 22:10:46 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:10:47 Changing preferred plex of volume: test-vol2. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 22:10:48 Changing preferred plex of volume: test-vol4. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 22:10:50 Changing preferred plex of volume: test-vol8. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/26 22:10:50 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:12:15 Changing preferred plex of volume: test-vol3. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/26 22:12:16 Changing preferred plex of volume: test-vol5. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 22:12:17 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:12:26 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/26 22:12:27 Changing preferred plex of volume: test-vol6. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 22:12:29 Changing preferred plex of volume: test-vol7. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/26 22:12:29 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:12:32 Changing preferred plex of volume: test-vol2. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/26 22:12:33 Changing preferred plex of volume: test-vol4. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 22:12:35 Changing preferred plex of volume: test-vol8. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/26 22:12:35 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:14:16 Detach & delete all volumes: 
[AfterEach] Create several simple volumes and add [online] plexes to these volumes.
  /gocode/main/test/e2e/tests/mirroring.go:1853
    DEBUG: 2019/11/26 22:15:18 END_TEST Mirroring.OnlinePlexAdd Time-taken : 946.666633891
    DEBUG: 2019/11/26 22:15:18 Checking stale resources
    DEBUG: 2019/11/26 22:15:18 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:15:18 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:15:18 Checking stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:946.815 seconds][0m
Mirroring.OnlinePlexAdd  Daily SM_PlexAdd-1.10
[90m/gocode/main/test/e2e/tests/mirroring.go:1836[0m
  Create several simple volumes and add [online] plexes to these volumes.
  [90m/gocode/main/test/e2e/tests/mirroring.go:1837[0m
    Create simple volumes, do IOs, add plexes, validate data on each plex of each volume.
    [90m/gocode/main/test/e2e/tests/mirroring.go:1857[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mEndpoint.Basic Daily N_Endpoint-1.4 N_Endpoint-1.5[0m [90mEndpoint Basic testcases[0m 
  [1mDelete the same endpoint again[0m
  [37m/gocode/main/test/e2e/tests/endpoint.go:71[0m
[BeforeEach] Endpoint Basic testcases
  /gocode/main/test/e2e/tests/endpoint.go:31
    DEBUG: 2019/11/26 22:15:18 Login to cluster
    DEBUG: 2019/11/26 22:15:18 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:15:18 Updating inventory struct
    DEBUG: 2019/11/26 22:15:19 Checking stale resources
    DEBUG: 2019/11/26 22:15:19 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:15:19 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:15:19 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:15:27 Creating storage classes
    DEBUG: 2019/11/26 22:15:37 START_TEST Endpoint.Basic
[It] Delete the same endpoint again
  /gocode/main/test/e2e/tests/endpoint.go:71
    DEBUG: 2019/11/26 22:15:37 Create a endpoint.
    DEBUG: 2019/11/26 22:15:37 Delete the endpoint.
    DEBUG: 2019/11/26 22:15:37 Try to delete the same endpoint again.
[AfterEach] Endpoint Basic testcases
  /gocode/main/test/e2e/tests/endpoint.go:41
    DEBUG: 2019/11/26 22:15:37 END_TEST Endpoint.Basic Time-taken : 0.228341157
    DEBUG: 2019/11/26 22:15:37 Checking stale resources
    DEBUG: 2019/11/26 22:15:37 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:15:37 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:15:37 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.425 seconds][0m
Endpoint.Basic Daily N_Endpoint-1.4 N_Endpoint-1.5
[90m/gocode/main/test/e2e/tests/endpoint.go:23[0m
  Endpoint Basic testcases
  [90m/gocode/main/test/e2e/tests/endpoint.go:30[0m
    Delete the same endpoint again
    [90m/gocode/main/test/e2e/tests/endpoint.go:71[0m
[90m------------------------------[0m
[0mRbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 [0m [90mValidate all Edit and View roles[0m 
  [1mValidate perftier-view role[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:461[0m
[BeforeEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:447
    DEBUG: 2019/11/26 22:15:37 START_TEST Rbac.RoleValidation
    DEBUG: 2019/11/26 22:15:37 Login to cluster
    DEBUG: 2019/11/26 22:15:38 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:15:38 Updating inventory struct
    DEBUG: 2019/11/26 22:15:38 Checking stale resources
    DEBUG: 2019/11/26 22:15:38 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:15:38 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:15:38 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:15:46 Creating storage classes
[It] Validate perftier-view role
  /gocode/main/test/e2e/tests/rbac.go:461
    DEBUG: 2019/11/26 22:15:55 Validate perftier-view role
    DEBUG: 2019/11/26 22:15:55 Creating group grp1 with perftier-view role(s)
    DEBUG: 2019/11/26 22:15:55 Creating user1 user in grp1 group
    DEBUG: 2019/11/26 22:15:55 Login as user1 user
    DEBUG: 2019/11/26 22:15:56 Operation validated : perftier-list
    ERROR: 2019/11/26 22:15:56  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create temp-pf -b 2G -i 50K', status:&{{Status } {  0} Failure POST on perftiers for "user1" is forbidden: User user1 cannot perform POST on perftiers Forbidden 0xc00019a4b0 403}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "POST on perftiers for \"user1\" is forbidden: User user1 cannot perform POST on perftiers",
 "reason": "Forbidden",
 "details": {
  "name": "user1",
  "kind": "perftiers"
 },
 "code": 403
}



    DEBUG: 2019/11/26 22:15:56 Operation validated : perftier-create
    DEBUG: 2019/11/26 22:15:59 Operation validated : perftier-delete
    DEBUG: 2019/11/26 22:16:16 Operation validated : volume-list
    DEBUG: 2019/11/26 22:16:16 Operation validated : volume-create
    DEBUG: 2019/11/26 22:16:46 Operation validated : volume-delete
    DEBUG: 2019/11/26 22:16:48 Operation validated : user-list
    DEBUG: 2019/11/26 22:16:48 Operation validated : user-create
    DEBUG: 2019/11/26 22:16:52 Operation validated : user-delete
[AfterEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:456
    DEBUG: 2019/11/26 22:16:53 END_TEST Rbac.RoleValidation Time-taken : 75.744920433
    DEBUG: 2019/11/26 22:16:53 Checking stale resources
    DEBUG: 2019/11/26 22:16:53 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:16:53 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:16:53 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:75.879 seconds][0m
Rbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 
[90m/gocode/main/test/e2e/tests/rbac.go:437[0m
  Validate all Edit and View roles
  [90m/gocode/main/test/e2e/tests/rbac.go:438[0m
    Validate perftier-view role
    [90m/gocode/main/test/e2e/tests/rbac.go:461[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mInvalid gateway test[0m
  [37m/gocode/main/test/e2e/tests/network.go:67[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:35
    DEBUG: 2019/11/26 22:16:53 START_TEST Network.NegativeTests
    DEBUG: 2019/11/26 22:16:53 Login to cluster
    DEBUG: 2019/11/26 22:16:54 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:16:54 Updating inventory struct
    DEBUG: 2019/11/26 22:16:54 Checking stale resources
    DEBUG: 2019/11/26 22:16:54 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:16:54 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:16:54 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:17:02 Creating storage classes
[It] Invalid gateway test
  /gocode/main/test/e2e/tests/network.go:67
    DEBUG: 2019/11/26 22:17:11 Try to create network with invalid gateway.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:48
    DEBUG: 2019/11/26 22:17:11 END_TEST Network.NegativeTests Time-taken : 17.624217765
    DEBUG: 2019/11/26 22:17:11 Checking stale resources
    DEBUG: 2019/11/26 22:17:11 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:17:11 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:17:11 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:17.764 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:26[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:27[0m
    Invalid gateway test
    [90m/gocode/main/test/e2e/tests/network.go:67[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mLocalStorage.RebootTestFsckCheck Daily S_Reboot-2.3[0m [90mreboot test to check file system errors with fsck command on local storage volumes[0m 
  [1mreboot test with fsck check on local storage[0m
  [37m/gocode/main/test/e2e/tests/volume.go:1479[0m
[BeforeEach] reboot test to check file system errors with fsck command on local storage volumes
  /gocode/main/test/e2e/tests/volume.go:1465
    DEBUG: 2019/11/26 22:17:11 START_TEST LocalStorage.RebootTestFsckCheck
    DEBUG: 2019/11/26 22:17:11 Login to cluster
    DEBUG: 2019/11/26 22:17:11 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:17:11 Updating inventory struct
    DEBUG: 2019/11/26 22:17:12 Checking stale resources
    DEBUG: 2019/11/26 22:17:12 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:17:12 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:17:12 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:17:20 Creating storage classes
[It] reboot test with fsck check on local storage
  /gocode/main/test/e2e/tests/volume.go:1479
    DEBUG: 2019/11/26 22:17:29 Creating 8 volumes of random sizes between 1GiB and 200GiB
    DEBUG: 2019/11/26 22:17:30 Attching volumes locally.
    DEBUG: 2019/11/26 22:18:04 Create XFS on odd no volumes and EXT4 on even no volumes.
    DEBUG: 2019/11/26 22:18:13 Volume2uuid_mapping: 
test-vol1 96b5cd7d-10dd-11ea-b97d-a4bf01147ffc
test-vol2 96cc0f91-10dd-11ea-b97d-a4bf01147ffc
test-vol3 96deec71-10dd-11ea-b97d-a4bf01147ffc
test-vol4 96f476b9-10dd-11ea-b97d-a4bf01147ffc
test-vol5 970ad0d8-10dd-11ea-b97d-a4bf01147ffc
test-vol6 9721d01d-10dd-11ea-b97d-a4bf01147ffc
test-vol7 9736e37a-10dd-11ea-b97d-a4bf01147ffc
test-vol8 974dabe3-10dd-11ea-b97d-a4bf01147ffc

    DEBUG: 2019/11/26 22:18:45 Running fio job on all the volumes
    DEBUG: 2019/11/26 22:18:45 FIO Command : sudo /usr/local/bin/fio --ioengine=sync --direct=1 --runtime=300 --time_based --iodepth=16 --group_reporting --blocksize_range=4K-1024K --rw=randwrite --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --name=job1 --filename=/mnt/test-vol1/file --name=job2 --filename=/mnt/test-vol2/file --name=job3 --filename=/mnt/test-vol3/file --name=job4 --filename=/mnt/test-vol4/file --name=job5 --filename=/mnt/test-vol5/file --name=job6 --filename=/mnt/test-vol6/file --name=job7 --filename=/mnt/test-vol7/file --name=job8 --filename=/mnt/test-vol8/file
    DEBUG: 2019/11/26 22:23:46 Getting cluster quorum nodes
    DEBUG: 2019/11/26 22:23:46 Powering OFF the node appserv54
    DEBUG: 2019/11/26 22:23:46 Node 172.16.6.154 took 0 seconds to power off
    DEBUG: 2019/11/26 22:23:46 Ensuring that appserv54 node is unreachable: 
    DEBUG: 2019/11/26 22:23:46 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:23:56 Polling to check until node: appserv54 goes down
    DEBUG: 2019/11/26 22:24:26 Error: . Retrying once again...
    DEBUG: 2019/11/26 22:25:27 Powering ON the node appserv54
    DEBUG: 2019/11/26 22:25:28 Node 172.16.6.154 took 0 seconds to power on
    DEBUG: 2019/11/26 22:25:28 Checking if node appserv54 is reachable or not: 
    DEBUG: 2019/11/26 22:25:28 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:25:47 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:26:04 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:26:21 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:26:38 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:26:55 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:27:12 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:27:29 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:27:33 appserv54 is pingable from local machine
    DEBUG: 2019/11/26 22:27:33 Checking ssh port is up or not on node: appserv54
    DEBUG: 2019/11/26 22:28:13 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/26 22:28:13 Found '3' nodes
    DEBUG: 2019/11/26 22:28:13 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/26 22:28:13 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/26 22:28:57 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/26 22:29:07 After power cycle/reboot, updating timestamp of node : appserv54
    DEBUG: 2019/11/26 22:29:10 Getting cluster quorum nodes
    DEBUG: 2019/11/26 22:30:10 Updating inventory struct
    DEBUG: 2019/11/26 22:30:11 Doing fsck on all the volumes
    DEBUG: 2019/11/26 22:30:11 output : fsck from util-linux 2.23.2
/dev/nvme6n1: recovering journal
/dev/nvme6n1: clean, 12/67248 files, 39671/268800 blocks


    DEBUG: 2019/11/26 22:30:12 output : fsck from util-linux 2.23.2
/dev/nvme2n1: recovering journal
/dev/nvme2n1: clean, 12/651520 files, 342994/2601600 blocks


    DEBUG: 2019/11/26 22:30:13 output : fsck from util-linux 2.23.2
/dev/nvme8n1: recovering journal
/dev/nvme8n1: clean, 12/1234576 files, 615340/4934400 blocks


    DEBUG: 2019/11/26 22:30:13 output : fsck from util-linux 2.23.2
/dev/nvme7n1: recovering journal
/dev/nvme7n1: clean, 12/1818624 files, 884730/7267200 blocks


    DEBUG: 2019/11/26 22:30:14 output : fsck from util-linux 2.23.2
/dev/nvme4n1: recovering journal
/dev/nvme4n1: clean, 12/2400256 files, 1156648/9600000 blocks


    DEBUG: 2019/11/26 22:30:14 output : fsck from util-linux 2.23.2
/dev/nvme1n1: recovering journal
/dev/nvme1n1: clean, 12/2984240 files, 1426639/11932800 blocks


    DEBUG: 2019/11/26 22:30:15 output : fsck from util-linux 2.23.2
/dev/nvme5n1: recovering journal
/dev/nvme5n1: clean, 12/3571712 files, 1698064/14265600 blocks


    DEBUG: 2019/11/26 22:30:15 output : fsck from util-linux 2.23.2
/dev/nvme3n1: recovering journal
/dev/nvme3n1: clean, 12/4153344 files, 1966450/16598400 blocks


    DEBUG: 2019/11/26 22:30:15 Comparing Volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/26 22:30:21 Volume2uuid_mapping: 
test-vol1 96b5cd7d-10dd-11ea-b97d-a4bf01147ffc
test-vol2 96cc0f91-10dd-11ea-b97d-a4bf01147ffc
test-vol3 96deec71-10dd-11ea-b97d-a4bf01147ffc
test-vol4 96f476b9-10dd-11ea-b97d-a4bf01147ffc
test-vol5 970ad0d8-10dd-11ea-b97d-a4bf01147ffc
test-vol6 9721d01d-10dd-11ea-b97d-a4bf01147ffc
test-vol7 9736e37a-10dd-11ea-b97d-a4bf01147ffc
test-vol8 974dabe3-10dd-11ea-b97d-a4bf01147ffc

    DEBUG: 2019/11/26 22:30:21 Volume to uuid mapping. After reboot: test-vol1 96b5cd7d-10dd-11ea-b97d-a4bf01147ffc
test-vol2 96cc0f91-10dd-11ea-b97d-a4bf01147ffc
test-vol3 96deec71-10dd-11ea-b97d-a4bf01147ffc
test-vol4 96f476b9-10dd-11ea-b97d-a4bf01147ffc
test-vol5 970ad0d8-10dd-11ea-b97d-a4bf01147ffc
test-vol6 9721d01d-10dd-11ea-b97d-a4bf01147ffc
test-vol7 9736e37a-10dd-11ea-b97d-a4bf01147ffc
test-vol8 974dabe3-10dd-11ea-b97d-a4bf01147ffc

    DEBUG: 2019/11/26 22:30:21 Comparing the UUID before and after reboot for all volumes
    DEBUG: 2019/11/26 22:30:21 Mounting all volumes
    DEBUG: 2019/11/26 22:30:25 Unmounting all volumes
    DEBUG: 2019/11/26 22:30:29 Detach & Delete all volumes
[AfterEach] reboot test to check file system errors with fsck command on local storage volumes
  /gocode/main/test/e2e/tests/volume.go:1474
    DEBUG: 2019/11/26 22:31:27 END_TEST LocalStorage.RebootTestFsckCheck Time-taken : 855.753837618
    DEBUG: 2019/11/26 22:31:27 Checking stale resources
    DEBUG: 2019/11/26 22:31:27 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:31:27 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:31:27 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:855.896 seconds][0m
LocalStorage.RebootTestFsckCheck Daily S_Reboot-2.3
[90m/gocode/main/test/e2e/tests/volume.go:1455[0m
  reboot test to check file system errors with fsck command on local storage volumes
  [90m/gocode/main/test/e2e/tests/volume.go:1458[0m
    reboot test with fsck check on local storage
    [90m/gocode/main/test/e2e/tests/volume.go:1479[0m
[90m------------------------------[0m
[0mRbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 [0m [90mValidate all Edit and View roles[0m 
  [1mValidate volume-view role[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:475[0m
[BeforeEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:447
    DEBUG: 2019/11/26 22:31:27 START_TEST Rbac.RoleValidation
    DEBUG: 2019/11/26 22:31:27 Login to cluster
    DEBUG: 2019/11/26 22:31:27 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:31:27 Updating inventory struct
    DEBUG: 2019/11/26 22:31:28 Checking stale resources
    DEBUG: 2019/11/26 22:31:28 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:31:28 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:31:28 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:31:36 Creating storage classes
[It] Validate volume-view role
  /gocode/main/test/e2e/tests/rbac.go:475
    DEBUG: 2019/11/26 22:31:47 Validate volume-view role
    DEBUG: 2019/11/26 22:31:48 Creating group grp1 with volume-view role(s)
    DEBUG: 2019/11/26 22:31:48 Creating user1 user in grp1 group
    DEBUG: 2019/11/26 22:31:49 Login as user1 user
    DEBUG: 2019/11/26 22:31:49 Operation validated : perftier-list
    ERROR: 2019/11/26 22:31:50  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create temp-pf -b 2G -i 50K', status:&{{Status } {  0} Failure POST on perftiers for "user1" is forbidden: User user1 cannot perform POST on perftiers Forbidden 0xc0009f6550 403}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "POST on perftiers for \"user1\" is forbidden: User user1 cannot perform POST on perftiers",
 "reason": "Forbidden",
 "details": {
  "name": "user1",
  "kind": "perftiers"
 },
 "code": 403
}



    DEBUG: 2019/11/26 22:31:50 Operation validated : perftier-create
    DEBUG: 2019/11/26 22:31:55 Operation validated : perftier-delete
    DEBUG: 2019/11/26 22:32:26 Operation validated : volume-list
    DEBUG: 2019/11/26 22:32:26 Operation validated : volume-create
    DEBUG: 2019/11/26 22:32:56 Operation validated : volume-delete
    DEBUG: 2019/11/26 22:32:58 Operation validated : user-list
    DEBUG: 2019/11/26 22:32:58 Operation validated : user-create
    DEBUG: 2019/11/26 22:33:02 Operation validated : user-delete
[AfterEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:456
    DEBUG: 2019/11/26 22:33:03 END_TEST Rbac.RoleValidation Time-taken : 96.254469067
    DEBUG: 2019/11/26 22:33:03 Checking stale resources
    DEBUG: 2019/11/26 22:33:03 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:33:03 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:33:03 Checking stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:96.388 seconds][0m
Rbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 
[90m/gocode/main/test/e2e/tests/rbac.go:437[0m
  Validate all Edit and View roles
  [90m/gocode/main/test/e2e/tests/rbac.go:438[0m
    Validate volume-view role
    [90m/gocode/main/test/e2e/tests/rbac.go:475[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mOut of range IP address test[0m
  [37m/gocode/main/test/e2e/tests/network.go:74[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:35
    DEBUG: 2019/11/26 22:33:03 START_TEST Network.NegativeTests
    DEBUG: 2019/11/26 22:33:03 Login to cluster
    DEBUG: 2019/11/26 22:33:04 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:33:04 Updating inventory struct
    DEBUG: 2019/11/26 22:33:04 Checking stale resources
    DEBUG: 2019/11/26 22:33:04 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:33:04 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:33:04 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:33:12 Creating storage classes
[It] Out of range IP address test
  /gocode/main/test/e2e/tests/network.go:74
    DEBUG: 2019/11/26 22:33:21 Try to create network with out of range IP address.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:48
    DEBUG: 2019/11/26 22:33:21 END_TEST Network.NegativeTests Time-taken : 17.909859783
    DEBUG: 2019/11/26 22:33:21 Checking stale resources
    DEBUG: 2019/11/26 22:33:21 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:33:21 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:33:21 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:18.059 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:26[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:27[0m
    Out of range IP address test
    [90m/gocode/main/test/e2e/tests/network.go:74[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mNetwork with vlan > 4094.[0m
  [37m/gocode/main/test/e2e/tests/network.go:60[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:35
    DEBUG: 2019/11/26 22:33:21 START_TEST Network.NegativeTests
    DEBUG: 2019/11/26 22:33:21 Login to cluster
    DEBUG: 2019/11/26 22:33:22 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:33:22 Updating inventory struct
    DEBUG: 2019/11/26 22:33:22 Checking stale resources
    DEBUG: 2019/11/26 22:33:22 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:33:22 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:33:22 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:33:30 Creating storage classes
[It] Network with vlan > 4094.
  /gocode/main/test/e2e/tests/network.go:60
    DEBUG: 2019/11/26 22:33:39 Try to create network with vlan > 4094.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:48
    DEBUG: 2019/11/26 22:33:39 END_TEST Network.NegativeTests Time-taken : 17.735153501
    DEBUG: 2019/11/26 22:33:39 Checking stale resources
    DEBUG: 2019/11/26 22:33:39 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:33:39 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:33:39 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:17.877 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:26[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:27[0m
    Network with vlan > 4094.
    [90m/gocode/main/test/e2e/tests/network.go:60[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mCreate overlapping network.[0m
  [37m/gocode/main/test/e2e/tests/network.go:103[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:35
    DEBUG: 2019/11/26 22:33:39 START_TEST Network.NegativeTests
    DEBUG: 2019/11/26 22:33:39 Login to cluster
    DEBUG: 2019/11/26 22:33:40 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:33:40 Updating inventory struct
    DEBUG: 2019/11/26 22:33:40 Checking stale resources
    DEBUG: 2019/11/26 22:33:40 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:33:40 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:33:40 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:33:48 Creating storage classes
[It] Create overlapping network.
  /gocode/main/test/e2e/tests/network.go:103
    DEBUG: 2019/11/26 22:33:56 Create a valid network.
    DEBUG: 2019/11/26 22:33:56 Try to create network with overlapping IP address.
    DEBUG: 2019/11/26 22:33:57 Delete the added network.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:48
    DEBUG: 2019/11/26 22:33:57 END_TEST Network.NegativeTests Time-taken : 17.678850366
    DEBUG: 2019/11/26 22:33:57 Checking stale resources
    DEBUG: 2019/11/26 22:33:57 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:33:57 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:33:57 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:17.818 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:26[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:27[0m
    Create overlapping network.
    [90m/gocode/main/test/e2e/tests/network.go:103[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mCluster.MasterMigration Management Daily M_Migration_1.0 M_Migration_1.1[0m [90mMigrate master to other quorum node. Check services running on new master and stopped on old master. Reboot the new master, master should change[0m 
  [1mMigrate master to other quorum node. Check services running on new master and stopped on old master. Reboot the new master, master failover should happen normally[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:813[0m
[BeforeEach] Migrate master to other quorum node. Check services running on new master and stopped on old master. Reboot the new master, master should change
  /gocode/main/test/e2e/tests/cluster.go:800
    DEBUG: 2019/11/26 22:33:57 START_TEST Cluster.MasterMigration
    DEBUG: 2019/11/26 22:33:57 Login to cluster
    DEBUG: 2019/11/26 22:33:58 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:33:58 Updating inventory struct
    DEBUG: 2019/11/26 22:33:58 Checking stale resources
    DEBUG: 2019/11/26 22:33:59 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:33:59 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:33:59 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:34:06 Creating storage classes
[It] Migrate master to other quorum node. Check services running on new master and stopped on old master. Reboot the new master, master failover should happen normally
  /gocode/main/test/e2e/tests/cluster.go:813
    DEBUG: 2019/11/26 22:34:15 Getting master
    DEBUG: 2019/11/26 22:34:15 Master node is appserv53 
    DEBUG: 2019/11/26 22:34:15 Getting cluster quorum nodes
    DEBUG: 2019/11/26 22:34:15 Getting cluster quorum nodes except master
    DEBUG: 2019/11/26 22:34:15 Quorum nodes except master: [appserv54 appserv55]
    DEBUG: 2019/11/26 22:34:15 Quorum node selected for new master : appserv54 
    DEBUG: 2019/11/26 22:34:15 Migrating the master from node appserv53 to quorum node appserv54
    DEBUG: 2019/11/26 22:34:15 Waiting for 60 seconds for master to get migrate
    DEBUG: 2019/11/26 22:35:15 Checking whether the services from old master switched to new master appserv54
    DEBUG: 2019/11/26 22:35:18 Verifying whether the services are stopped on old master appserv53 
    DEBUG: 2019/11/26 22:35:23 Master migrated from appserv53 to the new node appserv54
    DEBUG: 2019/11/26 22:35:23 After successful master migration, to check if master failover works, rebooting new master node: appserv54
    DEBUG: 2019/11/26 22:35:23 Doing sync on 172.16.6.154
    DEBUG: 2019/11/26 22:35:23 Getting cluster quorum nodes
    DEBUG: 2019/11/26 22:35:23 Powering OFF the node appserv54
    DEBUG: 2019/11/26 22:35:23 Doing sync on 172.16.6.154
    DEBUG: 2019/11/26 22:35:24 Ensuring that appserv54 node is unreachable: 
    DEBUG: 2019/11/26 22:35:24 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:35:28 appserv54 is pingable from local machine
    DEBUG: 2019/11/26 22:35:38 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:35:43 appserv54 is pingable from local machine
    DEBUG: 2019/11/26 22:35:53 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:36:02 Polling to check until node: appserv54 goes down
    DEBUG: 2019/11/26 22:36:52 Sleeping for 60 seconds for master node to go down and master failover to happen
    DEBUG: 2019/11/26 22:37:52 Looking for a new master
    DEBUG: 2019/11/26 22:37:52 Getting the new master
    DEBUG: 2019/11/26 22:37:55 New master after powering off the existing master of cluster is : appserv53
    DEBUG: 2019/11/26 22:37:55 Powering on old master node 
    DEBUG: 2019/11/26 22:37:55 Powering ON the node appserv54
    DEBUG: 2019/11/26 22:37:55 Node 172.16.6.154 took 0 seconds to power on
    DEBUG: 2019/11/26 22:37:55 Checking if node appserv54 is reachable or not: 
    DEBUG: 2019/11/26 22:37:55 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:38:12 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:38:29 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:38:46 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:39:03 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:39:20 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:39:37 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:39:54 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:40:11 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/26 22:40:15 appserv54 is pingable from local machine
    DEBUG: 2019/11/26 22:40:15 Checking ssh port is up or not on node: appserv54
    DEBUG: 2019/11/26 22:40:45 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/26 22:40:45 Found '3' nodes
    DEBUG: 2019/11/26 22:40:45 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/26 22:41:21 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/26 22:41:21 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/26 22:41:31 After power cycle/reboot, updating timestamp of node : appserv54
    DEBUG: 2019/11/26 22:41:34 Getting cluster quorum nodes
    DEBUG: 2019/11/26 22:42:34 Updating inventory struct
    DEBUG: 2019/11/26 22:42:34 Checking that mastership did not change from "appserv53", after powering on old master node "appserv54"
    DEBUG: 2019/11/26 22:42:34 Master failover happened successfully.
[AfterEach] Migrate master to other quorum node. Check services running on new master and stopped on old master. Reboot the new master, master should change
  /gocode/main/test/e2e/tests/cluster.go:808
    DEBUG: 2019/11/26 22:42:34 END_TEST Cluster.MasterMigration Time-taken : 5716.37846566
    DEBUG: 2019/11/26 22:42:34 Checking stale resources
    DEBUG: 2019/11/26 22:42:35 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:42:35 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:42:35 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:517.768 seconds][0m
Cluster.MasterMigration Management Daily M_Migration_1.0 M_Migration_1.1
[90m/gocode/main/test/e2e/tests/cluster.go:795[0m
  Migrate master to other quorum node. Check services running on new master and stopped on old master. Reboot the new master, master should change
  [90m/gocode/main/test/e2e/tests/cluster.go:796[0m
    Migrate master to other quorum node. Check services running on new master and stopped on old master. Reboot the new master, master failover should happen normally
    [90m/gocode/main/test/e2e/tests/cluster.go:813[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mCluster.RecommissionNode Management Daily M_Cluster-1.6[0m [90mwhen a node is decommissioned and recommissioned[0m 
  [1mshould be created, decommissioned, and recommissioned[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:461[0m
[BeforeEach] when a node is decommissioned and recommissioned
  /gocode/main/test/e2e/tests/cluster.go:442
    DEBUG: 2019/11/26 22:42:35 START_TEST Cluster.RecommissionNode
[AfterEach] when a node is decommissioned and recommissioned
  /gocode/main/test/e2e/tests/cluster.go:456
    DEBUG: 2019/11/26 22:42:35 END_TEST Cluster.RecommissionNode Time-taken : 0.000606363
    DEBUG: 2019/11/26 22:42:35 Checking stale resources
    DEBUG: 2019/11/26 22:42:35 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:42:35 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:42:35 Checking stale resources on the node: appserv53

[36m[1mS [SKIPPING] in Spec Setup (BeforeEach) [0.140 seconds][0m
Cluster.RecommissionNode Management Daily M_Cluster-1.6
[90m/gocode/main/test/e2e/tests/cluster.go:437[0m
  when a node is decommissioned and recommissioned
  [90m/gocode/main/test/e2e/tests/cluster.go:438[0m
    [36m[1mshould be created, decommissioned, and recommissioned [BeforeEach][0m
    [90m/gocode/main/test/e2e/tests/cluster.go:461[0m

    [36mSkipping de/recommission test for now[0m

    /gocode/main/test/e2e/tests/cluster.go:446
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 [0m [90mValidate all Edit and View roles[0m 
  [1mValidate perftier-edit role[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:468[0m
[BeforeEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:447
    DEBUG: 2019/11/26 22:42:35 START_TEST Rbac.RoleValidation
    DEBUG: 2019/11/26 22:42:35 Login to cluster
    DEBUG: 2019/11/26 22:42:35 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:42:35 Updating inventory struct
    DEBUG: 2019/11/26 22:42:36 Checking stale resources
    DEBUG: 2019/11/26 22:42:36 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:42:36 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:42:36 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:42:44 Creating storage classes
[It] Validate perftier-edit role
  /gocode/main/test/e2e/tests/rbac.go:468
    DEBUG: 2019/11/26 22:42:54 Validate perftier-edit role
    DEBUG: 2019/11/26 22:42:54 Creating group grp1 with perftier-edit role(s)
    DEBUG: 2019/11/26 22:42:54 Creating user1 user in grp1 group
    DEBUG: 2019/11/26 22:42:54 Login as user1 user
    DEBUG: 2019/11/26 22:42:55 Operation validated : perftier-list
    DEBUG: 2019/11/26 22:42:55 Operation validated : perftier-create
    DEBUG: 2019/11/26 22:42:56 Operation validated : perftier-delete
    DEBUG: 2019/11/26 22:43:31 Operation validated : volume-list
    DEBUG: 2019/11/26 22:43:31 Operation validated : volume-create
    DEBUG: 2019/11/26 22:44:03 Operation validated : volume-delete
    DEBUG: 2019/11/26 22:44:05 Operation validated : user-list
    DEBUG: 2019/11/26 22:44:05 Operation validated : user-create
    DEBUG: 2019/11/26 22:44:09 Operation validated : user-delete
[AfterEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:456
    DEBUG: 2019/11/26 22:44:09 END_TEST Rbac.RoleValidation Time-taken : 94.66727165
    DEBUG: 2019/11/26 22:44:09 Checking stale resources
    DEBUG: 2019/11/26 22:44:09 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:44:09 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:44:09 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:94.801 seconds][0m
Rbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 
[90m/gocode/main/test/e2e/tests/rbac.go:437[0m
  Validate all Edit and View roles
  [90m/gocode/main/test/e2e/tests/rbac.go:438[0m
    Validate perftier-edit role
    [90m/gocode/main/test/e2e/tests/rbac.go:468[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9[0m [90mPing an external IP from from a pod[0m 
  [1mPing external IP from a pod created using endpoint of public network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:538[0m
[BeforeEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:487
    DEBUG: 2019/11/26 22:44:10 START_TEST Network.PingExternalIP
    DEBUG: 2019/11/26 22:44:10 Login to cluster
    DEBUG: 2019/11/26 22:44:10 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:44:10 Updating inventory struct
    DEBUG: 2019/11/26 22:44:11 Checking stale resources
    DEBUG: 2019/11/26 22:44:11 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:44:11 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:44:11 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:44:19 Creating storage classes
[It] Ping external IP from a pod created using endpoint of public network
  /gocode/main/test/e2e/tests/network-pod.go:538
    DEBUG: 2019/11/26 22:44:27 Creating endpoint test-ep 
    DEBUG: 2019/11/26 22:44:27 Creating a pod with  docker.io/redis:3.0.5 image & the endpoint 
    DEBUG: 2019/11/26 22:44:30 IP address ( 172.16.179.6 ) of e2e-test-pod is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/26 22:44:30 Trying to ping google-public-dns-a.google.com from pod e2e-test-pod
    DEBUG: 2019/11/26 22:44:31 google-public-dns-a.google.com is pingable from pod e2e-test-pod (172.16.179.6)
    DEBUG: 2019/11/26 22:44:31 Network gateway is 172.16.179.1
    DEBUG: 2019/11/26 22:44:31 Matching default gateway of pod e2e-test-pod with 172.16.179.1 
    DEBUG: 2019/11/26 22:44:31 Default gateway of e2e-test-pod is 172.16.179.1
    DEBUG: 2019/11/26 22:44:31 Deleting the pod: e2e-test-pod
    DEBUG: 2019/11/26 22:44:46 Delete Endpoint: test-ep
[AfterEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:496
    DEBUG: 2019/11/26 22:44:46 END_TEST Network.PingExternalIP Time-taken : 36.300404189
    DEBUG: 2019/11/26 22:44:46 Checking stale resources
    DEBUG: 2019/11/26 22:44:46 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:44:46 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:44:46 Checking stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:36.421 seconds][0m
Network.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9
[90m/gocode/main/test/e2e/tests/network-pod.go:480[0m
  Ping an external IP from from a pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:481[0m
    Ping external IP from a pod created using endpoint of public network
    [90m/gocode/main/test/e2e/tests/network-pod.go:538[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mHostNetwork.Basic Daily N_HostNetwork-1.0 N_HostNetwork-1.2 N_HostNetwork-3.0 N_HostNetwork-1.4 N_HostNetwork-1.6 N_HostNetwork-2.0 N_HostNetwork-2.2[0m [90mCreate single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted[0m 
  [1mShould create multiple host-networks with node selector for 1 node, check if endpoints got created, delete host network, check if endpoints deleted[0m
  [37m/gocode/main/test/e2e/tests/network.go:543[0m
[BeforeEach] Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  /gocode/main/test/e2e/tests/network.go:525
    DEBUG: 2019/11/26 22:44:46 START_TEST HostNetwork.Basic
    DEBUG: 2019/11/26 22:44:46 Login to cluster
    DEBUG: 2019/11/26 22:44:46 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:44:47 Updating inventory struct
    DEBUG: 2019/11/26 22:44:47 Checking stale resources
    DEBUG: 2019/11/26 22:44:47 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:44:47 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:44:47 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:44:55 Creating storage classes
[It] Should create multiple host-networks with node selector for 1 node, check if endpoints got created, delete host network, check if endpoints deleted
  /gocode/main/test/e2e/tests/network.go:543
    DEBUG: 2019/11/26 22:45:04 Creating perf-tier : template
    DEBUG: 2019/11/26 22:45:04 Disabling helm feature
    DEBUG: 2019/11/26 22:45:05 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:45:07 Assigning label to nodes where the host network should get created
    DEBUG: 2019/11/26 22:45:07 Assigned label : label=hostNetwork to node : appserv54
    DEBUG: 2019/11/26 22:45:07 Updating non default network as host network
    DEBUG: 2019/11/26 22:45:08 Vnic allocation for host-network interface is successful
    DEBUG: 2019/11/26 22:45:08 Waiting for allocation of VF to endpoints of network : blue
    DEBUG: 2019/11/26 22:45:09 VF enp129s7f1 allocated to endpoint blue.appserv54 successfully
    DEBUG: 2019/11/26 22:45:09 Updating default network as host network
    DEBUG: 2019/11/26 22:45:09 Vnic allocation for host-network interface is successful
    DEBUG: 2019/11/26 22:45:09 Waiting for allocation of VF to endpoints of network : default
    DEBUG: 2019/11/26 22:45:10 VF enp129s7f7d2 allocated to endpoint default.appserv54 successfully
    DEBUG: 2019/11/26 22:45:10 Checking enpoint are provisioned and attached or not for hostnetwork : blue
    DEBUG: 2019/11/26 22:45:10 Checking enpoint are provisioned and attached or not for hostnetwork : default
    DEBUG: 2019/11/26 22:45:10 Deleting host-network blue
    DEBUG: 2019/11/26 22:45:13 Endpoint blue.appserv54 deleted successfully
    DEBUG: 2019/11/26 22:45:13 Endpoints created by host-network got deleted
    DEBUG: 2019/11/26 22:45:13 Deleting host-network default
    DEBUG: 2019/11/26 22:45:15 Endpoint default.appserv54 deleted successfully
    DEBUG: 2019/11/26 22:45:15 Endpoints created by host-network got deleted
    DEBUG: 2019/11/26 22:45:15 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:45:15 VNICS usage is : 0
    DEBUG: 2019/11/26 22:45:15 After deleting host networks, vnics released by host-network endpoints
    DEBUG: 2019/11/26 22:45:15 Removing hostNetwork label from the nodes : [appserv54]
    DEBUG: 2019/11/26 22:45:15 Removed label : label from node : appserv54
    DEBUG: 2019/11/26 22:45:15 Creating networks deleted by this TC
    DEBUG: 2019/11/26 22:45:15 Deleting the perf-tier : template
    DEBUG: 2019/11/26 22:45:15 Enabling helm feature.
[AfterEach] Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  /gocode/main/test/e2e/tests/network.go:531
    DEBUG: 2019/11/26 22:45:26 END_TEST HostNetwork.Basic Time-taken : 39.6071164
    DEBUG: 2019/11/26 22:45:26 Checking stale resources
    DEBUG: 2019/11/26 22:45:26 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:45:26 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:45:26 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:39.742 seconds][0m
HostNetwork.Basic Daily N_HostNetwork-1.0 N_HostNetwork-1.2 N_HostNetwork-3.0 N_HostNetwork-1.4 N_HostNetwork-1.6 N_HostNetwork-2.0 N_HostNetwork-2.2
[90m/gocode/main/test/e2e/tests/network.go:516[0m
  Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  [90m/gocode/main/test/e2e/tests/network.go:517[0m
    Should create multiple host-networks with node selector for 1 node, check if endpoints got created, delete host network, check if endpoints deleted
    [90m/gocode/main/test/e2e/tests/network.go:543[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9[0m [90mPing an external IP from from a pod[0m 
  [1mPing external IP from a pod created using invalid VLAN[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:568[0m
[BeforeEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:487
    DEBUG: 2019/11/26 22:45:26 START_TEST Network.PingExternalIP
    DEBUG: 2019/11/26 22:45:26 Login to cluster
    DEBUG: 2019/11/26 22:45:26 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:45:26 Updating inventory struct
    DEBUG: 2019/11/26 22:45:27 Checking stale resources
    DEBUG: 2019/11/26 22:45:27 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:45:27 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:45:27 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:45:35 Creating storage classes
[It] Ping external IP from a pod created using invalid VLAN
  /gocode/main/test/e2e/tests/network-pod.go:568
    DEBUG: 2019/11/26 22:45:44 Creating a test network ( testnetwork ) with invalid vlan
    DEBUG: 2019/11/26 22:45:44 Creating 1 pods of docker.io/redis:3.0.5 image with network : testnetwork and qos : high
    DEBUG: 2019/11/26 22:45:46 IP address ( 56.12.100.2 ) of e2etest-pod-1 is between 56.12.100.2 and 56.12.100.254

    DEBUG: 2019/11/26 22:45:46 Checking if google-public-dns-a.google.com is not pingable from pod e2etest-pod-1
    DEBUG: 2019/11/26 22:45:48 Output : PING google-public-dns-a.google.com (8.8.8.8): 48 data bytes
....60 bytes from e2etest-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 c9d1   0 0040  40  01 c9bc 56.12.100.2  8.8.8.8 
60 bytes from e2etest-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 d2d1   0 0040  40  01 c0bc 56.12.100.2  8.8.8.8 
60 bytes from e2etest-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 dad1   0 0040  40  01 b8bc 56.12.100.2  8.8.8.8 
60 bytes from e2etest-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 dcd1   0 0040  40  01 b6bc 56.12.100.2  8.8.8.8 
60 bytes from e2etest-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 ddd1   0 0040  40  01 b5bc 56.12.100.2  8.8.8.8 
--- google-public-dns-a.google.com ping statistics ---
5 packets transmitted, 0 packets received, 100% packet loss
, Error : failed to run commmand 'kubectl exec e2etest-pod-1 -- ping -f -c 5 -W 5 google-public-dns-a.google.com', output:PING google-public-dns-a.google.com (8.8.8.8): 48 data bytes
....60 bytes from e2etest-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 c9d1   0 0040  40  01 c9bc 56.12.100.2  8.8.8.8 
60 bytes from e2etest-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 d2d1   0 0040  40  01 c0bc 56.12.100.2  8.8.8.8 
60 bytes from e2etest-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 dad1   0 0040  40  01 b8bc 56.12.100.2  8.8.8.8 
60 bytes from e2etest-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 dcd1   0 0040  40  01 b6bc 56.12.100.2  8.8.8.8 
60 bytes from e2etest-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 ddd1   0 0040  40  01 b5bc 56.12.100.2  8.8.8.8 
--- google-public-dns-a.google.com ping statistics ---
5 packets transmitted, 0 packets received, 100% packet loss
, error:command terminated with exit code 1

, command terminated with exit code 1

    DEBUG: 2019/11/26 22:45:48 Deleting the pod: e2etest-pod-1
    DEBUG: 2019/11/26 22:45:56 Deleting test network : testnetwork
[AfterEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:496
    DEBUG: 2019/11/26 22:45:56 END_TEST Network.PingExternalIP Time-taken : 30.172282362
    DEBUG: 2019/11/26 22:45:56 Checking stale resources
    DEBUG: 2019/11/26 22:45:56 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:45:56 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:45:56 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:30.303 seconds][0m
Network.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9
[90m/gocode/main/test/e2e/tests/network-pod.go:480[0m
  Ping an external IP from from a pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:481[0m
    Ping external IP from a pod created using invalid VLAN
    [90m/gocode/main/test/e2e/tests/network-pod.go:568[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mMirroring.OnlinePlexDelete Daily SM_PlexDelete-1.0[0m [90mCreate mirrored volumes and delete [online] plexes from these volumes.[0m 
  [1mCreate mirrored volumes and delete [online] plexes from these volumes.[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:2106[0m
[BeforeEach] Create mirrored volumes and delete [online] plexes from these volumes.
  /gocode/main/test/e2e/tests/mirroring.go:2091
    DEBUG: 2019/11/26 22:45:56 START_TEST Mirroring.OnlinePlexDelete
    DEBUG: 2019/11/26 22:45:56 Login to cluster
    DEBUG: 2019/11/26 22:45:57 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:45:57 Updating inventory struct
    DEBUG: 2019/11/26 22:45:57 Checking stale resources
    DEBUG: 2019/11/26 22:45:57 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:45:57 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:45:57 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:46:05 Creating storage classes
[It] Create mirrored volumes and delete [online] plexes from these volumes.
  /gocode/main/test/e2e/tests/mirroring.go:2106
    DEBUG: 2019/11/26 22:46:14 Creating 8 volumes. Mirror Count: 3:
    DEBUG: 2019/11/26 22:46:14 Mirror Count: 3
    DEBUG: 2019/11/26 22:46:17 Attaching volumes: 
    DEBUG: 2019/11/26 22:46:52 Running write fio job on all volumes: 
    DEBUG: 2019/11/26 22:46:53 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:46:53 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:46:53 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:48:54 Running read fio job on all volumes: 
    DEBUG: 2019/11/26 22:48:55 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 22:48:55 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/26 22:48:55 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 22:48:57 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/26 22:48:57 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 22:48:57 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/26 22:48:57 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:48:58 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/26 22:48:58 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/26 22:48:59 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:48:59 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:50:09 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 22:50:11 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/26 22:50:11 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/26 22:50:11 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:50:12 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 22:50:12 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 22:50:13 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/26 22:50:13 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/26 22:50:14 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:50:15 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/26 22:50:15 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:50:57 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/26 22:50:59 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/26 22:50:59 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:51:26 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/26 22:51:27 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 22:51:27 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 22:51:29 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/26 22:51:29 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/26 22:51:30 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:51:30 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/26 22:51:31 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:52:44 Removing a plex from each volume. Expected PlexCount: 2
    DEBUG: 2019/11/26 22:53:32 Removing a plex from each volume. Expected PlexCount: 1
    DEBUG: 2019/11/26 22:54:32 Wait till resync completion on all volumes
    DEBUG: 2019/11/26 22:54:32 Number of volumes : 8
    DEBUG: 2019/11/26 22:54:32 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/26 22:54:32 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:54:32 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/26 22:54:32 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/26 22:54:32 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:54:32 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/26 22:54:32 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/26 22:54:32 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:54:32 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/26 22:54:32 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/26 22:54:32 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:54:32 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/26 22:54:32 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/26 22:54:32 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:54:32 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/26 22:54:32 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/26 22:54:33 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/26 22:54:33 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/26 22:54:33 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/26 22:54:33 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/26 22:54:33 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/26 22:54:33 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/26 22:54:33 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/26 22:54:33 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/26 22:54:33 Running verify fio on all volumes across all plexes: 
    DEBUG: 2019/11/26 22:54:33 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:54:33 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/26 22:54:33 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/26 22:55:18 Detach & delete all volumes: 
[AfterEach] Create mirrored volumes and delete [online] plexes from these volumes.
  /gocode/main/test/e2e/tests/mirroring.go:2101
    DEBUG: 2019/11/26 22:56:02 END_TEST Mirroring.OnlinePlexDelete Time-taken : 605.553777959
    DEBUG: 2019/11/26 22:56:02 Checking stale resources
    DEBUG: 2019/11/26 22:56:02 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:56:02 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:56:02 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:605.678 seconds][0m
Mirroring.OnlinePlexDelete Daily SM_PlexDelete-1.0
[90m/gocode/main/test/e2e/tests/mirroring.go:2085[0m
  Create mirrored volumes and delete [online] plexes from these volumes.
  [90m/gocode/main/test/e2e/tests/mirroring.go:2086[0m
    Create mirrored volumes and delete [online] plexes from these volumes.
    [90m/gocode/main/test/e2e/tests/mirroring.go:2106[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.User Daily Rbac_User-1.0 Rbac_User-1.1[0m [90mrbac user test[0m 
  [1muser-view test[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:96[0m
[BeforeEach] rbac user test
  /gocode/main/test/e2e/tests/rbac.go:82
    DEBUG: 2019/11/26 22:56:02 START_TEST Rbac.User
    DEBUG: 2019/11/26 22:56:02 Login to cluster
    DEBUG: 2019/11/26 22:56:02 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:56:02 Updating inventory struct
    DEBUG: 2019/11/26 22:56:03 Checking stale resources
    DEBUG: 2019/11/26 22:56:03 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:56:03 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:56:03 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:56:11 Creating storage classes
[It] user-view test
  /gocode/main/test/e2e/tests/rbac.go:96
    DEBUG: 2019/11/26 22:56:21 Create group
    DEBUG: 2019/11/26 22:56:21 Create user
    DEBUG: 2019/11/26 22:56:21 Login as user
    DEBUG: 2019/11/26 22:56:22 List users
    DEBUG: 2019/11/26 22:56:22 List groups
    DEBUG: 2019/11/26 22:56:22 List roles
    DEBUG: 2019/11/26 22:56:22 List auth-server
    DEBUG: 2019/11/26 22:56:22 Create user
    DEBUG: 2019/11/26 22:56:22 Create group
    DEBUG: 2019/11/26 22:56:22 Create role
    DEBUG: 2019/11/26 22:56:22 Create auth-server
    DEBUG: 2019/11/26 22:56:22 Create Network
[AfterEach] rbac user test
  /gocode/main/test/e2e/tests/rbac.go:90
    DEBUG: 2019/11/26 22:56:23 END_TEST Rbac.User Time-taken : 21.630178014
    DEBUG: 2019/11/26 22:56:23 Checking stale resources
    DEBUG: 2019/11/26 22:56:23 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:56:23 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:56:23 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:21.760 seconds][0m
Rbac.User Daily Rbac_User-1.0 Rbac_User-1.1
[90m/gocode/main/test/e2e/tests/rbac.go:74[0m
  rbac user test
  [90m/gocode/main/test/e2e/tests/rbac.go:77[0m
    user-view test
    [90m/gocode/main/test/e2e/tests/rbac.go:96[0m
[90m------------------------------[0m
[0mHostNetwork.Basic Daily N_HostNetwork-1.0 N_HostNetwork-1.2 N_HostNetwork-3.0 N_HostNetwork-1.4 N_HostNetwork-1.6 N_HostNetwork-2.0 N_HostNetwork-2.2[0m [90mCreate single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted[0m 
  [1mShould create multiple host-networks with node selector for more than 1 node, check if endpoints got created, delete host network, check if endpoints deleted[0m
  [37m/gocode/main/test/e2e/tests/network.go:550[0m
[BeforeEach] Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  /gocode/main/test/e2e/tests/network.go:525
    DEBUG: 2019/11/26 22:56:23 START_TEST HostNetwork.Basic
    DEBUG: 2019/11/26 22:56:23 Login to cluster
    DEBUG: 2019/11/26 22:56:24 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:56:24 Updating inventory struct
    DEBUG: 2019/11/26 22:56:25 Checking stale resources
    DEBUG: 2019/11/26 22:56:25 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:56:25 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:56:25 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:56:32 Creating storage classes
[It] Should create multiple host-networks with node selector for more than 1 node, check if endpoints got created, delete host network, check if endpoints deleted
  /gocode/main/test/e2e/tests/network.go:550
    DEBUG: 2019/11/26 22:56:41 Creating perf-tier : template
    DEBUG: 2019/11/26 22:56:41 Disabling helm feature
    DEBUG: 2019/11/26 22:56:42 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:56:45 Assigning label to nodes where the host network should get created
    DEBUG: 2019/11/26 22:56:45 Assigned label : label=hostNetwork to node : appserv53
    DEBUG: 2019/11/26 22:56:45 Assigned label : label=hostNetwork to node : appserv54
    DEBUG: 2019/11/26 22:56:45 Updating non default network as host network
    DEBUG: 2019/11/26 22:56:46 Vnic allocation for host-network interface is successful
    DEBUG: 2019/11/26 22:56:46 Waiting for allocation of VF to endpoints of network : blue
    DEBUG: 2019/11/26 22:56:47 VF enp129s8f1 allocated to endpoint blue.appserv53 successfully
    DEBUG: 2019/11/26 22:56:47 VF enp129s5f5 allocated to endpoint blue.appserv54 successfully
    DEBUG: 2019/11/26 22:56:47 Updating default network as host network
    DEBUG: 2019/11/26 22:56:47 Vnic allocation for host-network interface is successful
    DEBUG: 2019/11/26 22:56:47 Waiting for allocation of VF to endpoints of network : default
    DEBUG: 2019/11/26 22:56:48 VF enp129s3f2d2 allocated to endpoint default.appserv53 successfully
    DEBUG: 2019/11/26 22:56:48 VF enp129s4f7d2 allocated to endpoint default.appserv54 successfully
    DEBUG: 2019/11/26 22:56:48 Checking enpoint are provisioned and attached or not for hostnetwork : blue
    DEBUG: 2019/11/26 22:56:49 Creating 1 pairs of iperf client-server pods with high QoS
    DEBUG: 2019/11/26 22:56:49 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/26 22:56:51 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/26 22:57:21 Creating iperf Client pod: iperf-client-1
    DEBUG: 2019/11/26 22:57:23 Getting host-network interface of a cluster node where client pods scheduled
    DEBUG: 2019/11/26 22:57:24 tx_bytes on host-network interface enp129s5f5 when data transfer starts is : 552035292 bytes
    DEBUG: 2019/11/26 22:57:54 tx_bytes on host-network interface enp129s5f5 after waiting time is : 31377063004 bytes
    DEBUG: 2019/11/26 22:57:54 Data is going through host-network interface. Byte direction :  tx_bytes
    DEBUG: 2019/11/26 22:57:54 Deleting pods:
    DEBUG: 2019/11/26 22:57:54 Deleting pods : 
    DEBUG: 2019/11/26 22:58:06 Deleting services: 
    DEBUG: 2019/11/26 22:58:06 Deleting service(s)
    DEBUG: 2019/11/26 22:58:06 Checking enpoint are provisioned and attached or not for hostnetwork : default
    DEBUG: 2019/11/26 22:58:06 Creating 1 pairs of iperf client-server pods with high QoS
    DEBUG: 2019/11/26 22:58:06 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/26 22:58:09 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/26 22:58:39 Creating iperf Client pod: iperf-client-1
    DEBUG: 2019/11/26 22:58:42 Getting host-network interface of a cluster node where client pods scheduled
    DEBUG: 2019/11/26 22:58:42 tx_bytes on host-network interface enp129s4f7d2 when data transfer starts is : 1822428540 bytes
    DEBUG: 2019/11/26 22:59:13 tx_bytes on host-network interface enp129s4f7d2 after waiting time is : 36740495280 bytes
    DEBUG: 2019/11/26 22:59:13 Data is going through host-network interface. Byte direction :  tx_bytes
    DEBUG: 2019/11/26 22:59:13 Deleting pods:
    DEBUG: 2019/11/26 22:59:13 Deleting pods : 
    DEBUG: 2019/11/26 22:59:28 Deleting services: 
    DEBUG: 2019/11/26 22:59:28 Deleting service(s)
    DEBUG: 2019/11/26 22:59:28 Deleting host-network blue
    DEBUG: 2019/11/26 22:59:31 Endpoint blue.appserv53 deleted successfully
    DEBUG: 2019/11/26 22:59:31 Endpoint blue.appserv54 deleted successfully
    DEBUG: 2019/11/26 22:59:31 Endpoints created by host-network got deleted
    DEBUG: 2019/11/26 22:59:31 Deleting host-network default
    DEBUG: 2019/11/26 22:59:35 Endpoint default.appserv53 deleted successfully
    DEBUG: 2019/11/26 22:59:35 Endpoint default.appserv54 deleted successfully
    DEBUG: 2019/11/26 22:59:35 Endpoints created by host-network got deleted
    DEBUG: 2019/11/26 22:59:35 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:59:35 VNICS usage is : 0
    DEBUG: 2019/11/26 22:59:35 After deleting host networks, vnics released by host-network endpoints
    DEBUG: 2019/11/26 22:59:35 Removing hostNetwork label from the nodes : [appserv53 appserv54]
    DEBUG: 2019/11/26 22:59:35 Removed label : label from node : appserv53
    DEBUG: 2019/11/26 22:59:35 Removed label : label from node : appserv54
    DEBUG: 2019/11/26 22:59:35 Creating networks deleted by this TC
    DEBUG: 2019/11/26 22:59:35 Deleting the perf-tier : template
    DEBUG: 2019/11/26 22:59:35 Enabling helm feature.
[AfterEach] Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  /gocode/main/test/e2e/tests/network.go:531
    DEBUG: 2019/11/26 22:59:46 END_TEST HostNetwork.Basic Time-taken : 202.169636156
    DEBUG: 2019/11/26 22:59:46 Checking stale resources
    DEBUG: 2019/11/26 22:59:46 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:59:46 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:59:46 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:202.307 seconds][0m
HostNetwork.Basic Daily N_HostNetwork-1.0 N_HostNetwork-1.2 N_HostNetwork-3.0 N_HostNetwork-1.4 N_HostNetwork-1.6 N_HostNetwork-2.0 N_HostNetwork-2.2
[90m/gocode/main/test/e2e/tests/network.go:516[0m
  Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  [90m/gocode/main/test/e2e/tests/network.go:517[0m
    Should create multiple host-networks with node selector for more than 1 node, check if endpoints got created, delete host network, check if endpoints deleted
    [90m/gocode/main/test/e2e/tests/network.go:550[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14[0m [90mCheck if a pod's IP is pingable from other pod[0m 
  [1mCheck if a pod's IP is pingable from other pod using private network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:869[0m
[BeforeEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:848
    DEBUG: 2019/11/26 22:59:46 START_TEST Network.PingBetweenTwoPods
    DEBUG: 2019/11/26 22:59:46 Login to cluster
    DEBUG: 2019/11/26 22:59:46 Checking basic Vnic usage
    DEBUG: 2019/11/26 22:59:46 Updating inventory struct
    DEBUG: 2019/11/26 22:59:47 Checking stale resources
    DEBUG: 2019/11/26 22:59:47 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 22:59:47 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 22:59:47 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 22:59:55 Creating storage classes
[It] Check if a pod's IP is pingable from other pod using private network
  /gocode/main/test/e2e/tests/network-pod.go:869
    DEBUG: 2019/11/26 23:00:04 Creating private network : blue
    DEBUG: 2019/11/26 23:00:04 Creating 2 pods of docker.io/redis:3.0.5 image with network : blue and qos : high
    DEBUG: 2019/11/26 23:00:09 IP address ( 172.16.180.4 ) of e2etest-pod-1 is between 172.16.180.4 and 172.16.180.253

    DEBUG: 2019/11/26 23:00:10 IP address ( 172.16.180.5 ) of e2etest-pod-2 is between 172.16.180.4 and 172.16.180.253

    DEBUG: 2019/11/26 23:00:10 Trying to ping the e2etest-pod-2's IP from pod e2etest-pod-1
    DEBUG: 2019/11/26 23:00:10 172.16.180.5 is pingable from pod e2etest-pod-1 (172.16.180.4)
    DEBUG: 2019/11/26 23:00:10 Deleting pods : 
    DEBUG: 2019/11/26 23:00:26 Deleting private network : blue
[AfterEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:857
    DEBUG: 2019/11/26 23:00:26 END_TEST Network.PingBetweenTwoPods Time-taken : 40.721915503
    DEBUG: 2019/11/26 23:00:26 Checking stale resources
    DEBUG: 2019/11/26 23:00:27 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:00:27 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:00:27 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:40.864 seconds][0m
Network.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14
[90m/gocode/main/test/e2e/tests/network-pod.go:842[0m
  Check if a pod's IP is pingable from other pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:843[0m
    Check if a pod's IP is pingable from other pod using private network
    [90m/gocode/main/test/e2e/tests/network-pod.go:869[0m
[90m------------------------------[0m
[0mLocalStorage.RebootInitiator Daily S_Basic-2.0 S_Basic-2.1 S_Reboot-2.0 S_Reboot-2.2 S_Reboot-2.6[0m [90mreboot initiator node after running IO and verify data before and after reboot[0m 
  [1mreboot initiator node after running IO and verify data before and after reboot[0m
  [37m/gocode/main/test/e2e/tests/volume.go:2451[0m
[BeforeEach] reboot initiator node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2439
    DEBUG: 2019/11/26 23:00:27 START_TEST LocalStorage.RebootInitiator
    DEBUG: 2019/11/26 23:00:27 Login to cluster
    DEBUG: 2019/11/26 23:00:27 Checking basic Vnic usage
    DEBUG: 2019/11/26 23:00:28 Updating inventory struct
    DEBUG: 2019/11/26 23:00:28 Checking stale resources
    DEBUG: 2019/11/26 23:00:28 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:00:28 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:00:28 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 23:00:36 Creating storage classes
[It] reboot initiator node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2451
    DEBUG: 2019/11/26 23:00:45 Verifying whether FBM and L1 usage is zero across all nodes
    DEBUG: 2019/11/26 23:00:51 FBM and L1 usage is Zero across all nodes

    DEBUG: 2019/11/26 23:00:51 Creating 4 volumes of random sizes
    DEBUG: 2019/11/26 23:00:51 Mirror Count: 1
    DEBUG: 2019/11/26 23:00:52 Attaching all 4 volumes
    DEBUG: 2019/11/26 23:01:09 Initiator node : appserv55
    DEBUG: 2019/11/26 23:01:09 Nodes to reboot :[appserv55]
    DEBUG: 2019/11/26 23:01:12 Running WRITE fio job on node : appserv55
    DEBUG: 2019/11/26 23:01:12 FIO Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"efgh\"-12 --verify_interval=4096 --runtime=120 --blocksize=512K --iodepth=8  --time_based  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1

    DEBUG: 2019/11/26 23:03:13 Running VERIFY IOs on all plexes
    DEBUG: 2019/11/26 23:03:13 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"efgh\"-12 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=512K --iodepth=8  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1
    DEBUG: 2019/11/26 23:03:23 Getting cluster quorum nodes
    DEBUG: 2019/11/26 23:03:23 Powering OFF the node appserv55
    DEBUG: 2019/11/26 23:03:23 Node 172.16.6.155 took 0 seconds to power off
    DEBUG: 2019/11/26 23:03:23 Ensuring that appserv55 node is unreachable: 
    DEBUG: 2019/11/26 23:03:23 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/26 23:03:32 Polling to check until node: appserv55 goes down
    DEBUG: 2019/11/26 23:05:14 Powering ON the node appserv55
    DEBUG: 2019/11/26 23:05:15 Node 172.16.6.155 took 1 seconds to power on
    DEBUG: 2019/11/26 23:05:15 Checking if node appserv55 is reachable or not: 
    DEBUG: 2019/11/26 23:05:15 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/26 23:05:34 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/26 23:05:51 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/26 23:06:08 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/26 23:06:25 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/26 23:06:42 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/26 23:06:59 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/26 23:07:16 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/26 23:07:20 appserv55 is pingable from local machine
    DEBUG: 2019/11/26 23:07:20 Checking ssh port is up or not on node: appserv55
    DEBUG: 2019/11/26 23:08:00 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/26 23:08:00 Found '3' nodes
    DEBUG: 2019/11/26 23:08:00 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/26 23:08:00 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/26 23:08:00 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/26 23:09:03 After power cycle/reboot, updating timestamp of node : appserv55
    DEBUG: 2019/11/26 23:09:05 Getting cluster quorum nodes
    DEBUG: 2019/11/26 23:10:05 Updating inventory struct
    DEBUG: 2019/11/26 23:10:05 Waiting for nodes to come up, will wait upto 800 seconds
    DEBUG: 2019/11/26 23:10:17 Nodes are up, waiting for armada to start
.
    DEBUG: 2019/11/26 23:11:27 Waiting for the nodes to go into Ready state
    DEBUG: 2019/11/26 23:11:27 Found '3' nodes
    DEBUG: 2019/11/26 23:11:27 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/26 23:11:27 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/26 23:11:27 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/26 23:11:28 Waiting for volumes to come into Attached state after rebooting cluster nodes.
    DEBUG: 2019/11/26 23:11:28 Detaching all 4 volumes
    DEBUG: 2019/11/26 23:11:29 Re-attaching all 4 volumes
    DEBUG: 2019/11/26 23:11:48 Comparing Volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/26 23:11:51 Comparing the device path & uuid on initiator before and after reboot
    DEBUG: 2019/11/26 23:11:54 Running VERIFY IOs on all plexes
    DEBUG: 2019/11/26 23:11:54 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"efgh\"-12 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=512K --iodepth=8  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1
    DEBUG: 2019/11/26 23:12:02 Successfully completed Verification on all the volumes
    DEBUG: 2019/11/26 23:12:02 Detach & Delete all volumes
[AfterEach] reboot initiator node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2446
    DEBUG: 2019/11/26 23:13:02 END_TEST LocalStorage.RebootInitiator Time-taken : 755.545151665
    DEBUG: 2019/11/26 23:13:02 Checking stale resources
    DEBUG: 2019/11/26 23:13:02 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:13:02 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 23:13:02 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:755.683 seconds][0m
LocalStorage.RebootInitiator Daily S_Basic-2.0 S_Basic-2.1 S_Reboot-2.0 S_Reboot-2.2 S_Reboot-2.6
[90m/gocode/main/test/e2e/tests/volume.go:2432[0m
  reboot initiator node after running IO and verify data before and after reboot
  [90m/gocode/main/test/e2e/tests/volume.go:2434[0m
    reboot initiator node after running IO and verify data before and after reboot
    [90m/gocode/main/test/e2e/tests/volume.go:2451[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mCluster.RemoteStorageRemoveInitiatorAndTargetNode Management Daily QM_Pod-1.1 QM_Pod-1.2[0m [90mRemoving initiator and target node one at a time should fail for remote volumes[0m 
  [1mTry removing initiator and target node one at a time for remote volumes. It should fail[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:2111[0m
[BeforeEach] Removing initiator and target node one at a time should fail for remote volumes
  /gocode/main/test/e2e/tests/cluster.go:2098
    DEBUG: 2019/11/26 23:13:02 START_TEST Cluster.RemoteStorageRemoveInitiatorAndTargetNode
[It] Try removing initiator and target node one at a time for remote volumes. It should fail
  /gocode/main/test/e2e/tests/cluster.go:2111
    DEBUG: 2019/11/26 23:13:02 Getting quorum nodes for creating fio pod and remotely attached volume
    DEBUG: 2019/11/26 23:13:02 Target quorum nodes : [appserv53 appserv54 appserv55] 
    DEBUG: 2019/11/26 23:13:02 Creating fio pod with remote volume.
    DEBUG: 2019/11/26 23:13:02 Creating 1 Dynamic Persistent Volume Claims (PVCs). Mirror count: 1. Selector: node=node0
    DEBUG: 2019/11/26 23:13:04 Created PVC successfully.
    DEBUG: 2019/11/26 23:13:04 Creating 1 fio pods: 
    DEBUG: 2019/11/26 23:13:05 Checking if given pods are in Running state
    DEBUG: 2019/11/26 23:13:10 Wait for volumes to move into attached state: 
    DEBUG: 2019/11/26 23:13:10 Waitting for volume to move to "Attached" state
    DEBUG: 2019/11/26 23:13:10 Removing quorum node [target node] having volume : appserv53
    DEBUG: 2019/11/26 23:13:10 Removing quorum node [initiator node] where fio pod is running : appserv54
    DEBUG: 2019/11/26 23:13:10 Deleting pods : 
    DEBUG: 2019/11/26 23:13:14 Delete PVCs: 
    DEBUG: 2019/11/26 23:13:14 Waiting for volumes to get deleted: 
[AfterEach] Removing initiator and target node one at a time should fail for remote volumes
  /gocode/main/test/e2e/tests/cluster.go:2106
    DEBUG: 2019/11/26 23:14:02 END_TEST Cluster.RemoteStorageRemoveInitiatorAndTargetNode Time-taken : 59.541058375
    DEBUG: 2019/11/26 23:14:02 Checking stale resources
    DEBUG: 2019/11/26 23:14:02 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:14:02 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:14:02 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:59.731 seconds][0m
Cluster.RemoteStorageRemoveInitiatorAndTargetNode Management Daily QM_Pod-1.1 QM_Pod-1.2
[90m/gocode/main/test/e2e/tests/cluster.go:2085[0m
  Removing initiator and target node one at a time should fail for remote volumes
  [90m/gocode/main/test/e2e/tests/cluster.go:2086[0m
    Try removing initiator and target node one at a time for remote volumes. It should fail
    [90m/gocode/main/test/e2e/tests/cluster.go:2111[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.VnicsLimit Daily N_Stress-1.0 N_Negative-2.1 Multizone[0m [90mNetwork stress test by exausting VNICs[0m 
  [1mNetwork stress test by exausting VNICs[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:199[0m
[BeforeEach] Network stress test by exausting VNICs
  /gocode/main/test/e2e/tests/network-pod.go:188
    DEBUG: 2019/11/26 23:14:02 START_TEST Network.VnicsLimit
    DEBUG: 2019/11/26 23:14:02 Login to cluster
    DEBUG: 2019/11/26 23:14:03 Checking basic Vnic usage
    DEBUG: 2019/11/26 23:14:03 Updating inventory struct
    DEBUG: 2019/11/26 23:14:03 Checking stale resources
    DEBUG: 2019/11/26 23:14:03 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:14:03 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:14:03 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 23:14:11 Creating storage classes
[It] Network stress test by exausting VNICs
  /gocode/main/test/e2e/tests/network-pod.go:199
    DEBUG: 2019/11/26 23:14:20 There are total 187 VNICs available in the cluster
    DEBUG: 2019/11/26 23:14:20 Creating 187 pods of docker.io/redis:3.0.5 image with network : default and qos : best-effort
    DEBUG: 2019/11/26 23:15:21 Checking if given pods are in Running state
    DEBUG: 2019/11/26 23:15:41 Validating resource reservation for a cluster having 189 vnics
    DEBUG: 2019/11/26 23:15:41 Validating Vnics reservation in cluster
    DEBUG: 2019/11/26 23:15:41 Validating bandwidth reservation in cluster
    DEBUG: 2019/11/26 23:15:45 Cluster Node: appserv53. Used Bandwidth: 0
    DEBUG: 2019/11/26 23:15:45 Node : appserv53, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/26 23:15:48 Cluster Node: appserv54. Used Bandwidth: 0
    DEBUG: 2019/11/26 23:15:48 Node : appserv54, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/26 23:15:52 Cluster Node: appserv55. Used Bandwidth: 0
    DEBUG: 2019/11/26 23:15:52 Node : appserv55, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/26 23:15:52 Validation network reservation in cluster
    DEBUG: 2019/11/26 23:15:52 Trying to ping IP address (172.16.179.6) of e2e-test-pod-1 pod from all pod.
    DEBUG: 2019/11/26 23:17:13 Creating a pod exceeding VNICs count
    DEBUG: 2019/11/26 23:17:13 Creating 1 pods of docker.io/redis:3.0.5 image with network : default and qos : best-effort
    DEBUG: 2019/11/26 23:17:13 Waiting for 30 seconds before checking status of pod test-pod1
    DEBUG: 2019/11/26 23:17:43 Ensuring test-pod1 pod is in Pending state: 
    DEBUG: 2019/11/26 23:17:43 Deleting a pod e2e-test-pod-1 which is in Running state
    DEBUG: 2019/11/26 23:17:58 Checking if all pods are in Running state:
    DEBUG: 2019/11/26 23:17:58 Checking if given pods are in Running state
    DEBUG: 2019/11/26 23:18:18 Deleting pods : 
[AfterEach] Network stress test by exausting VNICs
  /gocode/main/test/e2e/tests/network-pod.go:194
    DEBUG: 2019/11/26 23:21:51 END_TEST Network.VnicsLimit Time-taken : 468.845225117
    DEBUG: 2019/11/26 23:21:51 Checking stale resources
    DEBUG: 2019/11/26 23:21:51 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:21:51 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:21:51 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:469.010 seconds][0m
Network.VnicsLimit Daily N_Stress-1.0 N_Negative-2.1 Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:173[0m
  Network stress test by exausting VNICs
  [90m/gocode/main/test/e2e/tests/network-pod.go:174[0m
    Network stress test by exausting VNICs
    [90m/gocode/main/test/e2e/tests/network-pod.go:199[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mZero VLAN test[0m
  [37m/gocode/main/test/e2e/tests/network.go:53[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:35
    DEBUG: 2019/11/26 23:21:51 START_TEST Network.NegativeTests
    DEBUG: 2019/11/26 23:21:51 Login to cluster
    DEBUG: 2019/11/26 23:21:52 Checking basic Vnic usage
    DEBUG: 2019/11/26 23:21:52 Updating inventory struct
    DEBUG: 2019/11/26 23:21:52 Checking stale resources
    DEBUG: 2019/11/26 23:21:52 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:21:52 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:21:52 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 23:22:00 Creating storage classes
[It] Zero VLAN test
  /gocode/main/test/e2e/tests/network.go:53
    DEBUG: 2019/11/26 23:22:11 Try to create network with Zero VLAN.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:48
    DEBUG: 2019/11/26 23:22:11 END_TEST Network.NegativeTests Time-taken : 19.76766896
    DEBUG: 2019/11/26 23:22:11 Checking stale resources
    DEBUG: 2019/11/26 23:22:11 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:22:11 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:22:11 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.945 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:26[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:27[0m
    Zero VLAN test
    [90m/gocode/main/test/e2e/tests/network.go:53[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRemoteStorage.MultiplePodsOneRemoteVolumeEach Daily RSP_Basic-1.0 Qos[0m [90mMultiple Pods, One remote volume each.[0m 
  [1mMultiple Pods, One remote volume each.[0m
  [37m/gocode/main/test/e2e/tests/volume.go:3132[0m
[BeforeEach] Multiple Pods, One remote volume each.
  /gocode/main/test/e2e/tests/volume.go:3118
    DEBUG: 2019/11/26 23:22:11 START_TEST RemoteStorage.MultiplePodsOneRemoteVolumeEach
    DEBUG: 2019/11/26 23:22:11 Login to cluster
    DEBUG: 2019/11/26 23:22:12 Checking basic Vnic usage
    DEBUG: 2019/11/26 23:22:12 Updating inventory struct
    DEBUG: 2019/11/26 23:22:12 Checking stale resources
    DEBUG: 2019/11/26 23:22:12 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:22:12 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:22:12 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 23:22:20 Creating storage classes
[It] Multiple Pods, One remote volume each.
  /gocode/main/test/e2e/tests/volume.go:3132
    DEBUG: 2019/11/26 23:22:30 Creating 10 Dynamic Persistent Volume Claims (PVCs). Mirror count: 1. Selector: node=node1
    DEBUG: 2019/11/26 23:22:30 Created PVC successfully.
    DEBUG: 2019/11/26 23:22:31 Created PVC successfully.
    DEBUG: 2019/11/26 23:22:31 Created PVC successfully.
    DEBUG: 2019/11/26 23:22:31 Created PVC successfully.
    DEBUG: 2019/11/26 23:22:32 Created PVC successfully.
    DEBUG: 2019/11/26 23:22:32 Created PVC successfully.
    DEBUG: 2019/11/26 23:22:32 Created PVC successfully.
    DEBUG: 2019/11/26 23:22:33 Created PVC successfully.
    DEBUG: 2019/11/26 23:22:33 Created PVC successfully.
    DEBUG: 2019/11/26 23:22:33 Created PVC successfully.
    DEBUG: 2019/11/26 23:22:36 Creating 10 fio pods: 
    DEBUG: 2019/11/26 23:22:40 Checking if given pods are in Running state
    DEBUG: 2019/11/26 23:23:11 Wait for volumes to move into attached state: 
    DEBUG: 2019/11/26 23:23:11 Waitting for volume to move to "Attached" state
    DEBUG: 2019/11/26 23:23:12 Sleeping for 180 seconds, so that prometheus will have some stats
    DEBUG: 2019/11/26 23:26:12 Validating qos associated with each volume: 
    DEBUG: 2019/11/26 23:26:13 Deleting pods : 
    DEBUG: 2019/11/26 23:26:27 Wait for volumes to come in Available state: 
    DEBUG: 2019/11/26 23:26:27 Delete PVCs: 
    DEBUG: 2019/11/26 23:26:30 Waiting for volumes to get deleted: 
[AfterEach] Multiple Pods, One remote volume each.
  /gocode/main/test/e2e/tests/volume.go:3127
    DEBUG: 2019/11/26 23:28:04 END_TEST RemoteStorage.MultiplePodsOneRemoteVolumeEach Time-taken : 352.72855093
    DEBUG: 2019/11/26 23:28:04 Checking stale resources
    DEBUG: 2019/11/26 23:28:04 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:28:04 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:28:04 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:352.872 seconds][0m
RemoteStorage.MultiplePodsOneRemoteVolumeEach Daily RSP_Basic-1.0 Qos
[90m/gocode/main/test/e2e/tests/volume.go:3111[0m
  Multiple Pods, One remote volume each.
  [90m/gocode/main/test/e2e/tests/volume.go:3113[0m
    Multiple Pods, One remote volume each.
    [90m/gocode/main/test/e2e/tests/volume.go:3132[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRemoteStorage.NicVFsScheduling Daily AT_Scheduling-2.0[0m [90mOccupy 2 VFs on nicID 1 and start traffic for 2 more VFs, they should be scheduled on nicId 3[0m 
  [1mCreate pods and check scheduling on nicID(s)[0m
  [37m/gocode/main/test/e2e/tests/volume.go:3251[0m
[BeforeEach] Occupy 2 VFs on nicID 1 and start traffic for 2 more VFs, they should be scheduled on nicId 3
  /gocode/main/test/e2e/tests/volume.go:3236
    DEBUG: 2019/11/26 23:28:04 START_TEST RemoteStorage.NicVFsScheduling
    DEBUG: 2019/11/26 23:28:04 Login to cluster
    DEBUG: 2019/11/26 23:28:04 Checking basic Vnic usage
    DEBUG: 2019/11/26 23:28:04 Updating inventory struct
    DEBUG: 2019/11/26 23:28:05 Checking stale resources
    DEBUG: 2019/11/26 23:28:05 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:28:05 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:28:05 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 23:28:13 Creating storage classes
[It] Create pods and check scheduling on nicID(s)
  /gocode/main/test/e2e/tests/volume.go:3251
    DEBUG: 2019/11/26 23:28:22 Creating 4 pods and 4 remote volumes with high qos: 
    DEBUG: 2019/11/26 23:28:22 Create 4 fio pod(s):
    DEBUG: 2019/11/26 23:28:22 Creating dynamic pvc : fio-pod-hightest-vol1
    DEBUG: 2019/11/26 23:28:22 Created PVC successfully.
    DEBUG: 2019/11/26 23:28:22 Creating fio pod: fio-pod-high-1
    DEBUG: 2019/11/26 23:28:22 Creating dynamic pvc : fio-pod-hightest-vol2
    DEBUG: 2019/11/26 23:28:23 Created PVC successfully.
    DEBUG: 2019/11/26 23:28:23 Creating fio pod: fio-pod-high-2
    DEBUG: 2019/11/26 23:28:23 Creating dynamic pvc : fio-pod-hightest-vol3
    DEBUG: 2019/11/26 23:28:23 Created PVC successfully.
    DEBUG: 2019/11/26 23:28:23 Creating fio pod: fio-pod-high-3
    DEBUG: 2019/11/26 23:28:24 Creating dynamic pvc : fio-pod-hightest-vol4
    DEBUG: 2019/11/26 23:28:24 Created PVC successfully.
    DEBUG: 2019/11/26 23:28:24 Creating fio pod: fio-pod-high-4
    DEBUG: 2019/11/26 23:28:24 Checking if given pods are in Running state
    DEBUG: 2019/11/26 23:28:35 Delete all pods which are scheduled on nicId 3
    DEBUG: 2019/11/26 23:28:35 Deleting pods : 
    DEBUG: 2019/11/26 23:28:46 Waitting for volume to move to "Available" state
    DEBUG: 2019/11/26 23:28:46 Deleting PVCs which are used by pods which was deleted
    DEBUG: 2019/11/26 23:28:46 Waiting for volumes to get deleted: 
    DEBUG: 2019/11/26 23:29:32 Creating 2 pods and 2 remote volumes with high qos: 
    DEBUG: 2019/11/26 23:29:32 Create 2 fio pod(s):
    DEBUG: 2019/11/26 23:29:32 Creating dynamic pvc : fio-pod-high-newtest-vol1
    DEBUG: 2019/11/26 23:29:32 Created PVC successfully.
    DEBUG: 2019/11/26 23:29:32 Creating fio pod: fio-pod-high-new-1
    DEBUG: 2019/11/26 23:29:33 Creating dynamic pvc : fio-pod-high-newtest-vol2
    DEBUG: 2019/11/26 23:29:33 Created PVC successfully.
    DEBUG: 2019/11/26 23:29:33 Creating fio pod: fio-pod-high-new-2
    DEBUG: 2019/11/26 23:29:33 Checking if given pods are in Running state
    DEBUG: 2019/11/26 23:29:44 Deleting all the pods: 
    DEBUG: 2019/11/26 23:30:26 Waitting for volume to move to "Available" state
    DEBUG: 2019/11/26 23:30:26 Delete PVCs: 
    DEBUG: 2019/11/26 23:30:27 Waiting for volumes to get deleted: 
[AfterEach] Occupy 2 VFs on nicID 1 and start traffic for 2 more VFs, they should be scheduled on nicId 3
  /gocode/main/test/e2e/tests/volume.go:3246
    DEBUG: 2019/11/26 23:31:32 END_TEST RemoteStorage.NicVFsScheduling Time-taken : 208.360603976
    DEBUG: 2019/11/26 23:31:32 Checking stale resources
    DEBUG: 2019/11/26 23:31:32 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:31:32 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:31:32 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:208.518 seconds][0m
RemoteStorage.NicVFsScheduling Daily AT_Scheduling-2.0
[90m/gocode/main/test/e2e/tests/volume.go:3227[0m
  Occupy 2 VFs on nicID 1 and start traffic for 2 more VFs, they should be scheduled on nicId 3
  [90m/gocode/main/test/e2e/tests/volume.go:3229[0m
    Create pods and check scheduling on nicID(s)
    [90m/gocode/main/test/e2e/tests/volume.go:3251[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4[0m [90mPing pod's IP from outside world[0m 
  [1mPing pod's IP from outside world using public network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:714[0m
[BeforeEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:700
    DEBUG: 2019/11/26 23:31:32 START_TEST Network.PingPodFromOutside
    DEBUG: 2019/11/26 23:31:32 Login to cluster
    DEBUG: 2019/11/26 23:31:33 Checking basic Vnic usage
    DEBUG: 2019/11/26 23:31:33 Updating inventory struct
    DEBUG: 2019/11/26 23:31:34 Checking stale resources
    DEBUG: 2019/11/26 23:31:34 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:31:34 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:31:34 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 23:31:42 Creating storage classes
[It] Ping pod's IP from outside world using public network
  /gocode/main/test/e2e/tests/network-pod.go:714
    DEBUG: 2019/11/26 23:31:51 Creating 1 pods of docker.io/redis:3.0.5 image with network : default and qos : high
    DEBUG: 2019/11/26 23:31:54 IP address ( 172.16.179.6 ) of e2etest-pod-1 is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/26 23:31:54 Trying to ping the e2etest-pod-1
    DEBUG: 2019/11/26 23:31:54 Executing ping command: ping  -c 5 -W 5 172.16.179.6
    DEBUG: 2019/11/26 23:31:58 172.16.179.6 is pingable from local machine
    DEBUG: 2019/11/26 23:31:58 Deleting pods : 
[AfterEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:709
    DEBUG: 2019/11/26 23:32:08 END_TEST Network.PingPodFromOutside Time-taken : 35.792230389
    DEBUG: 2019/11/26 23:32:08 Checking stale resources
    DEBUG: 2019/11/26 23:32:08 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:32:08 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:32:08 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:35.961 seconds][0m
Network.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4
[90m/gocode/main/test/e2e/tests/network-pod.go:694[0m
  Ping pod's IP from outside world
  [90m/gocode/main/test/e2e/tests/network-pod.go:695[0m
    Ping pod's IP from outside world using public network
    [90m/gocode/main/test/e2e/tests/network-pod.go:714[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mNetwork.IpAddressPoolLimitTests Daily N_Requirements-1.5 N_Requirements-1.6[0m [90mExhaust ip alloaction pool of  a network X, Create a pod with network X, Pod must go into pending state.[0m 
  [1mExhaust ip alloaction pool of network X, Create a pod with network X, Pod must go into pending state.[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:3044[0m
[BeforeEach] Exhaust ip alloaction pool of  a network X, Create a pod with network X, Pod must go into pending state.
  /gocode/main/test/e2e/tests/network-pod.go:3033
    DEBUG: 2019/11/26 23:32:08 START_TEST Network.IpAddressPoolLimitTests
    DEBUG: 2019/11/26 23:32:08 Login to cluster
    DEBUG: 2019/11/26 23:32:09 Checking basic Vnic usage
    DEBUG: 2019/11/26 23:32:09 Updating inventory struct
    DEBUG: 2019/11/26 23:32:10 Checking stale resources
    DEBUG: 2019/11/26 23:32:10 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:32:10 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:32:10 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 23:32:18 Creating storage classes
[It] Exhaust ip alloaction pool of network X, Create a pod with network X, Pod must go into pending state.
  /gocode/main/test/e2e/tests/network-pod.go:3044
    DEBUG: 2019/11/26 23:32:28 Creating test a network with ip pool of 10 addresss: 
    DEBUG: 2019/11/26 23:32:29 Creating pods to exhaust ipaddress pool: 
    DEBUG: 2019/11/26 23:32:29 Creating 10 pods of docker.io/redis:3.0.5 image with network : e2e-test-network and qos : medium
    DEBUG: 2019/11/26 23:32:55 Ensure that network ip pool is exhausted: 
    DEBUG: 2019/11/26 23:32:56 Create one more pod in private network: 
    DEBUG: 2019/11/26 23:32:56 Creating 1 pods of docker.io/redis:3.0.5 image with network : e2e-test-network and qos : medium
    DEBUG: 2019/11/26 23:33:01 Ensure pod is pending state: 
    DEBUG: 2019/11/26 23:33:01 Delete pods: 
    DEBUG: 2019/11/26 23:33:01 Deleting pods : 
    DEBUG: 2019/11/26 23:33:23 Delete test network: 
[AfterEach] Exhaust ip alloaction pool of  a network X, Create a pod with network X, Pod must go into pending state.
  /gocode/main/test/e2e/tests/network-pod.go:3039
    DEBUG: 2019/11/26 23:33:23 END_TEST Network.IpAddressPoolLimitTests : 74.411613346
    DEBUG: 2019/11/26 23:33:23 Checking stale resources
    DEBUG: 2019/11/26 23:33:23 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:33:23 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:33:23 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:74.550 seconds][0m
Network.IpAddressPoolLimitTests Daily N_Requirements-1.5 N_Requirements-1.6
[90m/gocode/main/test/e2e/tests/network-pod.go:3025[0m
  Exhaust ip alloaction pool of  a network X, Create a pod with network X, Pod must go into pending state.
  [90m/gocode/main/test/e2e/tests/network-pod.go:3026[0m
    Exhaust ip alloaction pool of network X, Create a pod with network X, Pod must go into pending state.
    [90m/gocode/main/test/e2e/tests/network-pod.go:3044[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mLocalStorage.IOVerification Daily S_Verify-1.4[0m [90mVolume basic io verification test for daily[0m 
  [1mVolume basic io verification[0m
  [37m/gocode/main/test/e2e/tests/volume.go:2172[0m
[BeforeEach] Volume basic io verification test for daily
  /gocode/main/test/e2e/tests/volume.go:2155
    DEBUG: 2019/11/26 23:33:23 START_TEST LocalStorage.IOVerification
    DEBUG: 2019/11/26 23:33:23 Login to cluster
    DEBUG: 2019/11/26 23:33:23 Checking basic Vnic usage
    DEBUG: 2019/11/26 23:33:24 Updating inventory struct
    DEBUG: 2019/11/26 23:33:24 Checking stale resources
    DEBUG: 2019/11/26 23:33:24 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:33:24 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 23:33:24 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:33:32 Creating storage classes
[It] Volume basic io verification
  /gocode/main/test/e2e/tests/volume.go:2172
    DEBUG: 2019/11/26 23:33:41 Pick up appserv53 as targetNode and appserv54 as initiatorNode
    DEBUG: 2019/11/26 23:33:41 Creating a volumes of 4294967296 size
    DEBUG: 2019/11/26 23:33:41 Attaching the volume
    DEBUG: 2019/11/26 23:33:46 Verifying whether FBM and L1 usage is zero
    DEBUG: 2019/11/26 23:33:52 FBM and L1 usage is Zero across all nodes

    DEBUG: 2019/11/26 23:33:53 Running fio job on a volume
    DEBUG: 2019/11/26 23:33:53 FIO Command : sudo /usr/local/bin/fio --ioengine=sync --rw=write --iodepth=16 --thread=1  --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --direct=1 --bs=4K --do_verify=0 --verify_state_save=1 --fill_device=1 --name=job1 --filename=/dev/nvme1n1

    DEBUG: 2019/11/26 23:38:54 Getting ios count from fio command output
    DEBUG: 2019/11/26 23:38:54 Calculating data written on drives
    DEBUG: 2019/11/26 23:38:55 Matching both counts
    DEBUG: 2019/11/26 23:38:55 IOs by fio job : 4125.000000 MB and Data written on drives 4125.000000 MB. Both are same
    DEBUG: 2019/11/26 23:38:55 Detach volume
    DEBUG: 2019/11/26 23:38:57 Delete volume
[AfterEach] Volume basic io verification test for daily
  /gocode/main/test/e2e/tests/volume.go:2167
    DEBUG: 2019/11/26 23:39:32 END_TEST LocalStorage.IOVerification Time-taken : 369.139030275
    DEBUG: 2019/11/26 23:39:32 Checking stale resources
    DEBUG: 2019/11/26 23:39:32 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:39:32 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:39:32 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:369.312 seconds][0m
LocalStorage.IOVerification Daily S_Verify-1.4
[90m/gocode/main/test/e2e/tests/volume.go:2146[0m
  Volume basic io verification test for daily
  [90m/gocode/main/test/e2e/tests/volume.go:2148[0m
    Volume basic io verification
    [90m/gocode/main/test/e2e/tests/volume.go:2172[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mMirroring.Basic Daily SM_Basic-1.0 SM_Basic-1.1 SM_Verify-1.1 SM_Stress-1.0[0m [90mMirrored volume basic test[0m 
  [1mmirrored volume basic operations[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:52[0m
[BeforeEach] Mirrored volume basic test
  /gocode/main/test/e2e/tests/mirroring.go:38
    DEBUG: 2019/11/26 23:39:32 START_TEST Mirroring.Basic
    DEBUG: 2019/11/26 23:39:32 Login to cluster
    DEBUG: 2019/11/26 23:39:33 Checking basic Vnic usage
    DEBUG: 2019/11/26 23:39:33 Updating inventory struct
    DEBUG: 2019/11/26 23:39:33 Checking stale resources
    DEBUG: 2019/11/26 23:39:34 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/26 23:39:34 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/26 23:39:34 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/26 23:39:42 Creating storage classes
[It] mirrored volume basic operations
  /gocode/main/test/e2e/tests/mirroring.go:52
    DEBUG: 2019/11/26 23:39:52 Assigning mirror label to nodes : [appserv53 appserv54 appserv55] to schedule mirrored volume plexes
    DEBUG: 2019/11/26 23:39:52 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/26 23:39:52 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/26 23:39:52 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/26 23:39:52 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/26 23:39:55 Removing mirror label from the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/26 23:39:55 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/26 23:39:55 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/26 23:39:55 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/26 23:39:55 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/26 23:39:55 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/26 23:39:55 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/26 23:40:33 Total number of volumes: 24

    DEBUG: 2019/11/26 23:40:33 Running WRITE IOs with SHA512 checksum with IO SIZE (4K)
    DEBUG: 2019/11/26 23:40:34 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:40:34 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:40:34 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:41:05 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/26 23:41:05 Running VERIFY IOs with SHA512 checksum with IO SIZE (4K)
    DEBUG: 2019/11/26 23:41:07 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:07 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:07 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:08 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:09 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:09 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:10 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:10 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:10 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:11 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:11 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:11 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:13 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:13 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:13 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:14 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:14 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:14 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:15 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:15 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:16 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:17 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:17 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:17 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 0
    DEBUG: 2019/11/26 23:41:17 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:41:17 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:41:18 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:41:35 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:37 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:38 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:39 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:39 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:40 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:40 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:41 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:41 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:41 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:42 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:43 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:43 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:43 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:44 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:44 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:45 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:45 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:45 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:45 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:41:47 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:47 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:48 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:48 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:49 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:41:49 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 1
    DEBUG: 2019/11/26 23:41:50 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:42:03 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:04 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:05 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:05 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:06 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:06 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:07 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:07 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:08 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:08 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:08 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:09 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:09 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:10 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:10 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:11 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:11 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:12 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:12 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:13 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:13 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:42:13 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:14 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:14 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:15 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:42:15 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 2
    DEBUG: 2019/11/26 23:42:16 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:42:40 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/26 23:42:40 Running WRITE IOs with SHA512 checksum with IO SIZE (64K)
    DEBUG: 2019/11/26 23:42:41 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:42:41 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:42:41 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:43:12 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/26 23:43:12 Running VERIFY IOs with SHA512 checksum with IO SIZE (64K)
    DEBUG: 2019/11/26 23:43:14 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:14 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:14 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:15 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:15 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:15 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:17 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:17 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:17 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:18 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:18 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:18 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:19 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:19 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:19 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:21 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:21 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:21 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:22 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:22 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:22 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:24 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:24 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:24 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 0
    DEBUG: 2019/11/26 23:43:24 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:43:24 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:43:24 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:43:39 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:39 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:40 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:41 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:42 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:42 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:43 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:43 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:43 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:44 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:45 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:45 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:45 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:46 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:46 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:47 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:47 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:47 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:48 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:49 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:49 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:49 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:43:49 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:43:49 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:51 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:52 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 1
    DEBUG: 2019/11/26 23:43:53 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:44:03 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:05 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:06 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:06 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:07 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:07 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:07 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:08 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:09 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:09 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:09 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:10 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:10 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:11 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:11 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:11 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:12 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:13 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:13 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:14 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:44:14 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:14 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:15 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:15 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:16 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:44:16 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 2
    DEBUG: 2019/11/26 23:44:17 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:44:36 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/26 23:44:36 Running WRITE IOs with SHA512 checksum with BS Range (4K to 1M)
    DEBUG: 2019/11/26 23:44:37 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:44:37 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:44:37 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:45:08 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/26 23:45:08 Running VERIFY IOs with SHA512 checksum with BS Range (4K to 1M)
    DEBUG: 2019/11/26 23:45:10 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:10 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:10 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:12 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:12 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:12 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:13 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:13 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:13 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:14 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:14 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:15 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:16 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:16 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:16 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:17 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:17 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:17 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:18 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:19 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:19 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:20 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:20 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:20 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 0
    DEBUG: 2019/11/26 23:45:21 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:45:21 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:45:21 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:45:37 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:37 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:38 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:39 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:39 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:40 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:40 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:40 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:41 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:41 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:42 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:42 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:43 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:43 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:44 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:44 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:44 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:45 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:45 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:46 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:46 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:47 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:47 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:47 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:45:48 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:45:48 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 1
    DEBUG: 2019/11/26 23:45:49 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:46:01 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:03 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:04 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:04 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:04 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:05 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:05 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:06 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:07 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:07 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:07 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:08 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:08 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:09 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:09 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:09 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:10 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:11 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:11 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:11 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:12 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:46:12 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:13 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:13 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:14 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 2
    DEBUG: 2019/11/26 23:46:14 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:46:15 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:46:34 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/26 23:46:34 Running WRITE IOs with SHA512 checksum with BS Range (32K to 1024K)
    DEBUG: 2019/11/26 23:46:35 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:46:35 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:46:35 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:47:06 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/26 23:47:06 Running VERIFY IOs with SHA512 checksum with BS Range (32K to 1024K)
    DEBUG: 2019/11/26 23:47:08 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:08 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:08 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:09 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:09 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:10 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:11 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:11 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:11 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:12 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:12 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:12 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:14 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:14 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:14 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:15 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:15 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:15 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:16 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:16 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:16 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:18 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:18 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:18 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 0
    DEBUG: 2019/11/26 23:47:18 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:47:18 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:47:19 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:47:34 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:34 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:35 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:35 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:37 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:37 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:37 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:38 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:38 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:39 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:39 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:39 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:40 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:41 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:41 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:41 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:42 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:42 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:43 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:43 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:43 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:44 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:44 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:47:44 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:47:45 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:47 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 1
    DEBUG: 2019/11/26 23:47:48 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:47:57 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/26 23:47:58 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:00 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:01 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:02 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:02 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:02 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:03 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:04 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:04 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:04 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:05 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:05 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:06 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:06 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:07 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:07 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:07 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:48:08 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:08 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:09 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:10 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:11 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:11 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:12 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:48:12 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 2
    DEBUG: 2019/11/26 23:48:13 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:48:29 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/26 23:48:29 Running WRITE IOs with SHA512 checksum with IO SIZE(4K), bs_unaligned
    DEBUG: 2019/11/26 23:48:30 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:48:30 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:48:30 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:49:30 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/26 23:49:30 Running VERIFY IOs with SHA512 checksum with IO SIZE(4K), bs_unaligned
    DEBUG: 2019/11/26 23:49:32 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:32 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:32 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:34 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:34 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:34 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:35 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:35 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:35 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:36 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:36 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:36 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:38 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:38 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:38 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:39 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:39 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:39 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:40 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:40 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:40 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:42 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:42 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:42 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 0
    DEBUG: 2019/11/26 23:49:42 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:49:43 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:49:43 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:54:03 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:04 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:05 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:07 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:08 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:09 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:11 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:12 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:13 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:54:21 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:23 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:24 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:24 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:26 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:26 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:27 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:27 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:28 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:28 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:30 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:30 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:31 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:31 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:32 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:54:32 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:34 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 1
    DEBUG: 2019/11/26 23:54:34 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:58:43 Changing preferred plex of volume: appserv54-test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/26 23:58:44 Changing preferred plex of volume: appserv54-test-vol2. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/26 23:58:45 Changing preferred plex of volume: appserv54-test-vol3. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/26 23:58:47 Changing preferred plex of volume: appserv54-test-vol4. Volume load index: 9.New preferred plex: 2
    DEBUG: 2019/11/26 23:58:48 Changing preferred plex of volume: appserv54-test-vol5. Volume load index: 12.New preferred plex: 2
    DEBUG: 2019/11/26 23:58:49 Changing preferred plex of volume: appserv54-test-vol6. Volume load index: 15.New preferred plex: 2
    DEBUG: 2019/11/26 23:58:51 Changing preferred plex of volume: appserv54-test-vol7. Volume load index: 18.New preferred plex: 2
    DEBUG: 2019/11/26 23:58:52 Changing preferred plex of volume: appserv54-test-vol8. Volume load index: 21.New preferred plex: 2
    DEBUG: 2019/11/26 23:58:53 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:59:11 Changing preferred plex of volume: appserv55-test-vol1. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:12 Changing preferred plex of volume: appserv55-test-vol2. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:14 Changing preferred plex of volume: appserv55-test-vol3. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:15 Changing preferred plex of volume: appserv55-test-vol4. Volume load index: 11.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:16 Changing preferred plex of volume: appserv55-test-vol5. Volume load index: 14.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:18 Changing preferred plex of volume: appserv55-test-vol6. Volume load index: 16.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:19 Changing preferred plex of volume: appserv55-test-vol7. Volume load index: 20.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:19 Changing preferred plex of volume: appserv53-test-vol1. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:20 Changing preferred plex of volume: appserv55-test-vol8. Volume load index: 22.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:20 Changing preferred plex of volume: appserv53-test-vol2. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:21 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/26 23:59:22 Changing preferred plex of volume: appserv53-test-vol3. Volume load index: 8.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:23 Changing preferred plex of volume: appserv53-test-vol4. Volume load index: 10.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:24 Changing preferred plex of volume: appserv53-test-vol5. Volume load index: 13.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:26 Changing preferred plex of volume: appserv53-test-vol6. Volume load index: 16.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:27 Changing preferred plex of volume: appserv53-test-vol7. Volume load index: 19.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:28 Changing preferred plex of volume: appserv53-test-vol8. Volume load index: 23.New preferred plex: 2
    DEBUG: 2019/11/26 23:59:29 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/27 00:03:58 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 00:03:58 Creating file system & mounting volumes on the node : appserv55
    DEBUG: 2019/11/27 00:03:58 Creating file system & mounting volumes on the node : appserv53
    DEBUG: 2019/11/27 00:03:58 Creating file system & mounting volumes on the node : appserv54
    DEBUG: 2019/11/27 00:04:06 Umount volumes and delete mount directory on node : appserv54
    DEBUG: 2019/11/27 00:04:07 Umount volumes and delete mount directory on node : appserv55
    DEBUG: 2019/11/27 00:04:07 Umount volumes and delete mount directory on node : appserv53
    DEBUG: 2019/11/27 00:04:08 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:04:08 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:04:08 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:04:13 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 00:04:13 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 00:04:13 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 00:05:07 Rebooting all the nodes simultaneously and checking for FBM and L1 consistency
Powering OFF the node appserv55
Powering OFF the node appserv53
Powering OFF the node appserv54
    DEBUG: 2019/11/27 00:05:07 Node 172.16.6.155 took 0 seconds to power off
    DEBUG: 2019/11/27 00:05:07 Node 172.16.6.153 took 0 seconds to power off
    DEBUG: 2019/11/27 00:05:07 Node 172.16.6.154 took 0 seconds to power off
Powering ON the node appserv55
Powering ON the node appserv53
Powering ON the node appserv54
    DEBUG: 2019/11/27 00:05:37 Node 172.16.6.155 took 0 seconds to power on
    DEBUG: 2019/11/27 00:05:37 Node 172.16.6.153 took 0 seconds to power on
    DEBUG: 2019/11/27 00:05:38 Node 172.16.6.154 took 1 seconds to power on
    DEBUG: 2019/11/27 00:05:38 Waiting for nodes to come up, will wait upto 800 seconds
.............    DEBUG: 2019/11/27 00:07:59 Nodes are up, waiting for armada to start
......
    DEBUG: 2019/11/27 00:09:59 Found '3' nodes
    DEBUG: 2019/11/27 00:09:59 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 00:10:00 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 00:10:00 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 00:10:00 Recording timestamp of all services on all nodes
[AfterEach] Mirrored volume basic test
  /gocode/main/test/e2e/tests/mirroring.go:47
    DEBUG: 2019/11/27 00:10:07 END_TEST Mirroring.Basic Time-taken : 1835.218372897
    DEBUG: 2019/11/27 00:10:07 Checking stale resources
    DEBUG: 2019/11/27 00:10:07 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:10:07 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:10:07 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:1835.372 seconds][0m
Mirroring.Basic Daily SM_Basic-1.0 SM_Basic-1.1 SM_Verify-1.1 SM_Stress-1.0
[90m/gocode/main/test/e2e/tests/mirroring.go:30[0m
  Mirrored volume basic test
  [90m/gocode/main/test/e2e/tests/mirroring.go:33[0m
    mirrored volume basic operations
    [90m/gocode/main/test/e2e/tests/mirroring.go:52[0m
[90m------------------------------[0m
[0mLocalStorage.RebootDuringIoFsckCheck Daily S_Reboot-2.4[0m [90mReboot (during IOs) test to check file system errors with fsck command on local storage volumes[0m 
  [1mreboot test with fsck check on local storage[0m
  [37m/gocode/main/test/e2e/tests/volume.go:1510[0m
[BeforeEach] Reboot (during IOs) test to check file system errors with fsck command on local storage volumes
  /gocode/main/test/e2e/tests/volume.go:1495
    DEBUG: 2019/11/27 00:10:08 START_TEST LocalStorage.RebootDuringIoFsckCheck
    DEBUG: 2019/11/27 00:10:08 Login to cluster
    DEBUG: 2019/11/27 00:10:08 Checking basic Vnic usage
    DEBUG: 2019/11/27 00:10:08 Updating inventory struct
    DEBUG: 2019/11/27 00:10:09 Checking stale resources
    DEBUG: 2019/11/27 00:10:09 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:10:09 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:10:09 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:10:17 Creating storage classes
[It] reboot test with fsck check on local storage
  /gocode/main/test/e2e/tests/volume.go:1510
    DEBUG: 2019/11/27 00:10:27 Creating 8 volumes of random sizes between 1GiB and 200GiB
    DEBUG: 2019/11/27 00:10:28 Attching volumes locally.
    DEBUG: 2019/11/27 00:11:05 Create XFS on odd no volumes and EXT4 on even no volumes.
    DEBUG: 2019/11/27 00:11:12 Volume2uuid_mapping: 
test-vol1 5ec630a6-10ed-11ea-b2ea-a4bf01194d67
test-vol2 5edee107-10ed-11ea-b2ea-a4bf01194d67
test-vol3 5ef59ecb-10ed-11ea-b2ea-a4bf01194d67
test-vol4 5f0c29a6-10ed-11ea-b2ea-a4bf01194d67
test-vol5 5f226e85-10ed-11ea-b2ea-a4bf01194d67
test-vol6 5f37e8b3-10ed-11ea-b2ea-a4bf01194d67
test-vol7 5f4bf9a5-10ed-11ea-b2ea-a4bf01194d67
test-vol8 5f621c12-10ed-11ea-b2ea-a4bf01194d67

    DEBUG: 2019/11/27 00:11:23 Running fio job on all the volumes
    DEBUG: 2019/11/27 00:11:26 Number of running fio process: 11

    DEBUG: 2019/11/27 00:13:06 Getting cluster quorum nodes
    DEBUG: 2019/11/27 00:13:06 Powering OFF the node appserv54
    DEBUG: 2019/11/27 00:13:06 Node 172.16.6.154 took 0 seconds to power off
    DEBUG: 2019/11/27 00:13:06 Ensuring that appserv54 node is unreachable: 
    DEBUG: 2019/11/27 00:13:06 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 00:13:15 Polling to check until node: appserv54 goes down
    DEBUG: 2019/11/27 00:14:37 Powering ON the node appserv54
    DEBUG: 2019/11/27 00:14:37 Node 172.16.6.154 took 0 seconds to power on
    DEBUG: 2019/11/27 00:14:37 Checking if node appserv54 is reachable or not: 
    DEBUG: 2019/11/27 00:14:37 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 00:14:54 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 00:15:11 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 00:15:28 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 00:15:45 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 00:16:02 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 00:16:19 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 00:16:36 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 00:16:53 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 00:16:57 appserv54 is pingable from local machine
    DEBUG: 2019/11/27 00:16:57 Checking ssh port is up or not on node: appserv54
    DEBUG: 2019/11/27 00:17:27 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/27 00:17:27 Found '3' nodes
    DEBUG: 2019/11/27 00:17:27 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 00:18:09 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 00:18:09 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 00:18:19 After power cycle/reboot, updating timestamp of node : appserv54
    DEBUG: 2019/11/27 00:18:22 Getting cluster quorum nodes
    DEBUG: 2019/11/27 00:19:22 Updating inventory struct
    DEBUG: 2019/11/27 00:19:22 Doing fsck on all the volumes
    DEBUG: 2019/11/27 00:19:23 output : fsck from util-linux 2.23.2
/dev/nvme5n1: recovering journal
/dev/nvme5n1: clean, 12/67248 files, 39671/268800 blocks


    DEBUG: 2019/11/27 00:19:24 output : fsck from util-linux 2.23.2
/dev/nvme7n1: recovering journal
/dev/nvme7n1: clean, 12/182528 files, 102952/729600 blocks


    DEBUG: 2019/11/27 00:19:24 output : fsck from util-linux 2.23.2
/dev/nvme4n1: recovering journal
/dev/nvme4n1: clean, 12/295408 files, 171797/1180800 blocks


    DEBUG: 2019/11/27 00:19:25 output : fsck from util-linux 2.23.2
/dev/nvme6n1: recovering journal
/dev/nvme6n1: clean, 12/410448 files, 229853/1641600 blocks


    DEBUG: 2019/11/27 00:19:25 output : fsck from util-linux 2.23.2
/dev/nvme1n1: recovering journal
/dev/nvme1n1: clean, 12/526240 files, 280845/2102400 blocks


    DEBUG: 2019/11/27 00:19:26 output : fsck from util-linux 2.23.2
/dev/nvme2n1: recovering journal
/dev/nvme2n1: clean, 12/638976 files, 337342/2553600 blocks


    DEBUG: 2019/11/27 00:19:26 output : fsck from util-linux 2.23.2
/dev/nvme8n1: recovering journal
/dev/nvme8n1: clean, 12/753664 files, 389073/3014400 blocks


    DEBUG: 2019/11/27 00:19:27 output : fsck from util-linux 2.23.2
/dev/nvme3n1: recovering journal
/dev/nvme3n1: clean, 12/869696 files, 445007/3475200 blocks


    DEBUG: 2019/11/27 00:19:27 Comparing Volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/27 00:19:32 Volume2uuid_mapping: 
test-vol1 5ec630a6-10ed-11ea-b2ea-a4bf01194d67
test-vol2 5edee107-10ed-11ea-b2ea-a4bf01194d67
test-vol3 5ef59ecb-10ed-11ea-b2ea-a4bf01194d67
test-vol4 5f0c29a6-10ed-11ea-b2ea-a4bf01194d67
test-vol5 5f226e85-10ed-11ea-b2ea-a4bf01194d67
test-vol6 5f37e8b3-10ed-11ea-b2ea-a4bf01194d67
test-vol7 5f4bf9a5-10ed-11ea-b2ea-a4bf01194d67
test-vol8 5f621c12-10ed-11ea-b2ea-a4bf01194d67

    DEBUG: 2019/11/27 00:19:32 Volume to uuid mapping. After reboot: test-vol1 5ec630a6-10ed-11ea-b2ea-a4bf01194d67
test-vol2 5edee107-10ed-11ea-b2ea-a4bf01194d67
test-vol3 5ef59ecb-10ed-11ea-b2ea-a4bf01194d67
test-vol4 5f0c29a6-10ed-11ea-b2ea-a4bf01194d67
test-vol5 5f226e85-10ed-11ea-b2ea-a4bf01194d67
test-vol6 5f37e8b3-10ed-11ea-b2ea-a4bf01194d67
test-vol7 5f4bf9a5-10ed-11ea-b2ea-a4bf01194d67
test-vol8 5f621c12-10ed-11ea-b2ea-a4bf01194d67

    DEBUG: 2019/11/27 00:19:32 Comparing the UUID before and after reboot for all volumes
    DEBUG: 2019/11/27 00:19:32 Mounting all volumes
    DEBUG: 2019/11/27 00:19:37 Unmounting all volumes
    DEBUG: 2019/11/27 00:19:41 Detach & Delete all volumes
[AfterEach] Reboot (during IOs) test to check file system errors with fsck command on local storage volumes
  /gocode/main/test/e2e/tests/volume.go:1505
    DEBUG: 2019/11/27 00:20:17 END_TEST LocalStorage.RebootDuringIoFsckCheck Time-taken : 609.319475451
    DEBUG: 2019/11/27 00:20:17 Checking stale resources
    DEBUG: 2019/11/27 00:20:17 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:20:17 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:20:17 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:609.463 seconds][0m
LocalStorage.RebootDuringIoFsckCheck Daily S_Reboot-2.4
[90m/gocode/main/test/e2e/tests/volume.go:1485[0m
  Reboot (during IOs) test to check file system errors with fsck command on local storage volumes
  [90m/gocode/main/test/e2e/tests/volume.go:1488[0m
    reboot test with fsck check on local storage
    [90m/gocode/main/test/e2e/tests/volume.go:1510[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mNetwork.IperfPodsWithRestartPolicyNever Daily N_Basic-2.3 MutliZone[0m [90mCreating iperf pods with restart policy Never[0m 
  [1mCreate iperf client-server pairs with restart policy Never.[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:2414[0m
[BeforeEach] Creating iperf pods with restart policy Never
  /gocode/main/test/e2e/tests/network-pod.go:2402
    DEBUG: 2019/11/27 00:20:17 START_TEST Network.IperfPodsWithRestartPolicyNever
    DEBUG: 2019/11/27 00:20:17 Login to cluster
    DEBUG: 2019/11/27 00:20:18 Checking basic Vnic usage
    DEBUG: 2019/11/27 00:20:18 Updating inventory struct
    DEBUG: 2019/11/27 00:20:18 Checking stale resources
    DEBUG: 2019/11/27 00:20:18 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:20:18 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:20:18 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:20:26 Creating storage classes
[It] Create iperf client-server pairs with restart policy Never.
  /gocode/main/test/e2e/tests/network-pod.go:2414
    DEBUG: 2019/11/27 00:20:36 Creating 8 pairs of iperf client-server pod.
    DEBUG: 2019/11/27 00:20:36 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/27 00:20:39 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/27 00:20:39 Creating iperf server pod: iperf-serverhigh2
    DEBUG: 2019/11/27 00:20:42 Creating service with name: iperf-serverhigh2
    DEBUG: 2019/11/27 00:20:42 Creating iperf server pod: iperf-serverhigh3
    DEBUG: 2019/11/27 00:20:45 Creating service with name: iperf-serverhigh3
    DEBUG: 2019/11/27 00:20:45 Creating iperf server pod: iperf-serverhigh4
    DEBUG: 2019/11/27 00:20:47 Creating service with name: iperf-serverhigh4
    DEBUG: 2019/11/27 00:20:47 Creating iperf server pod: iperf-serverhigh5
    DEBUG: 2019/11/27 00:20:50 Creating service with name: iperf-serverhigh5
    DEBUG: 2019/11/27 00:20:50 Creating iperf server pod: iperf-serverhigh6
    DEBUG: 2019/11/27 00:20:53 Creating service with name: iperf-serverhigh6
    DEBUG: 2019/11/27 00:20:53 Creating iperf server pod: iperf-serverhigh7
    DEBUG: 2019/11/27 00:20:56 Creating service with name: iperf-serverhigh7
    DEBUG: 2019/11/27 00:20:56 Creating iperf server pod: iperf-serverhigh8
    DEBUG: 2019/11/27 00:20:58 Creating service with name: iperf-serverhigh8
    DEBUG: 2019/11/27 00:21:29 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/27 00:21:31 Creating iperf Client pod: iperf-clienthigh2
    DEBUG: 2019/11/27 00:21:41 Creating iperf Client pod: iperf-clienthigh3
    DEBUG: 2019/11/27 00:21:48 Creating iperf Client pod: iperf-clienthigh4
    DEBUG: 2019/11/27 00:21:57 Creating iperf Client pod: iperf-clienthigh5
    DEBUG: 2019/11/27 00:22:06 Creating iperf Client pod: iperf-clienthigh6
    DEBUG: 2019/11/27 00:22:17 Creating iperf Client pod: iperf-clienthigh7
    DEBUG: 2019/11/27 00:22:20 Creating iperf Client pod: iperf-clienthigh8
    DEBUG: 2019/11/27 00:22:28 Validating resource reservation for a cluster having 18 vnics
    DEBUG: 2019/11/27 00:22:28 Validating Vnics reservation in cluster
    DEBUG: 2019/11/27 00:22:28 Validating bandwidth reservation in cluster
    DEBUG: 2019/11/27 00:22:28 Cluster Node: appserv53. Used Bandwidth: 3000000000
    DEBUG: 2019/11/27 00:22:28 Node : appserv53, Actual bandwith : 3000000000, Expected bandwidth :3000000000
    DEBUG: 2019/11/27 00:22:29 Cluster Node: appserv54. Used Bandwidth: 2500000000
    DEBUG: 2019/11/27 00:22:29 Node : appserv54, Actual bandwith : 2500000000, Expected bandwidth :2500000000
    DEBUG: 2019/11/27 00:22:29 Cluster Node: appserv55. Used Bandwidth: 2500000000
    DEBUG: 2019/11/27 00:22:29 Node : appserv55, Actual bandwith : 2500000000, Expected bandwidth :2500000000
    DEBUG: 2019/11/27 00:22:29 Validation network reservation in cluster
    DEBUG: 2019/11/27 00:22:29 Sleeping for 720 sec
    DEBUG: 2019/11/27 00:34:29 Checking if all client pods are in Complete state/Succeeded phase
    DEBUG: 2019/11/27 00:34:31 Deleting pods : 
    DEBUG: 2019/11/27 00:34:51 Deleting service(s)
    DEBUG: 2019/11/27 00:34:51 Waiting 60 sec for resource release
    DEBUG: 2019/11/27 00:35:51 Validating resource reservation for a cluster having 2 vnics
    DEBUG: 2019/11/27 00:35:51 Validating Vnics reservation in cluster
    DEBUG: 2019/11/27 00:35:52 Validating bandwidth reservation in cluster
    DEBUG: 2019/11/27 00:35:52 Cluster Node: appserv53. Used Bandwidth: 0
    DEBUG: 2019/11/27 00:35:52 Node : appserv53, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/27 00:35:52 Cluster Node: appserv54. Used Bandwidth: 0
    DEBUG: 2019/11/27 00:35:52 Node : appserv54, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/27 00:35:52 Cluster Node: appserv55. Used Bandwidth: 0
    DEBUG: 2019/11/27 00:35:52 Node : appserv55, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/27 00:35:52 Validation network reservation in cluster
[AfterEach] Creating iperf pods with restart policy Never
  /gocode/main/test/e2e/tests/network-pod.go:2409
    DEBUG: 2019/11/27 00:35:52 END_TEST Network.IperfPodsWithRestartPolicyNever Time-taken : 935.13983829
    DEBUG: 2019/11/27 00:35:52 Checking stale resources
    DEBUG: 2019/11/27 00:35:52 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:35:52 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:35:52 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:935.295 seconds][0m
Network.IperfPodsWithRestartPolicyNever Daily N_Basic-2.3 MutliZone
[90m/gocode/main/test/e2e/tests/network-pod.go:2396[0m
  Creating iperf pods with restart policy Never
  [90m/gocode/main/test/e2e/tests/network-pod.go:2397[0m
    Create iperf client-server pairs with restart policy Never.
    [90m/gocode/main/test/e2e/tests/network-pod.go:2414[0m
[90m------------------------------[0m
[0mReplicator.Basic Management Daily M_Replicator-1.0 M_Replicator-1.1[0m [90mReplicator basic tests[0m 
  [1mReplicator create, update[0m
  [37m/gocode/main/test/e2e/tests/pod.go:238[0m
[BeforeEach] Replicator basic tests
  /gocode/main/test/e2e/tests/pod.go:227
    DEBUG: 2019/11/27 00:35:52 START_TEST Replicator.Basic
    DEBUG: 2019/11/27 00:35:52 Login to cluster
    DEBUG: 2019/11/27 00:35:53 Checking basic Vnic usage
    DEBUG: 2019/11/27 00:35:53 Updating inventory struct
    DEBUG: 2019/11/27 00:35:54 Checking stale resources
    DEBUG: 2019/11/27 00:35:54 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:35:54 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:35:54 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:36:02 Creating storage classes
[It] Replicator create, update
  /gocode/main/test/e2e/tests/pod.go:238
    DEBUG: 2019/11/27 00:36:11 Create Replicator
    DEBUG: 2019/11/27 00:36:16 List the pods.
    DEBUG: 2019/11/27 00:36:27 Update Replicator
    DEBUG: 2019/11/27 00:36:27 List the pods.
    DEBUG: 2019/11/27 00:36:30 List the pods.
    DEBUG: 2019/11/27 00:36:33 List the pods.
    DEBUG: 2019/11/27 00:36:36 List the pods.
    DEBUG: 2019/11/27 00:36:39 List the pods.
    DEBUG: 2019/11/27 00:36:43 List the pods.
    DEBUG: 2019/11/27 00:36:46 List the pods.
    DEBUG: 2019/11/27 00:36:46 Update replicas to 0 and Delete Replicator
    DEBUG: 2019/11/27 00:36:46 List the pods.
    DEBUG: 2019/11/27 00:36:49 List the pods.
    DEBUG: 2019/11/27 00:36:52 List the pods.
    DEBUG: 2019/11/27 00:36:55 List the pods.
    DEBUG: 2019/11/27 00:36:58 List the pods.
    DEBUG: 2019/11/27 00:37:02 List the pods.
    DEBUG: 2019/11/27 00:37:02 List the pods.
    DEBUG: 2019/11/27 00:37:02 Waiting for network resource release after deletion of replicator
    DEBUG: 2019/11/27 00:37:02 Make sure that blue network usage is zero
[AfterEach] Replicator basic tests
  /gocode/main/test/e2e/tests/pod.go:233
    DEBUG: 2019/11/27 00:37:02 END_TEST Replicator.Basic Time-taken : 70.081248304
    DEBUG: 2019/11/27 00:37:02 Checking stale resources
    DEBUG: 2019/11/27 00:37:02 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:37:02 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:37:02 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:70.240 seconds][0m
Replicator.Basic Management Daily M_Replicator-1.0 M_Replicator-1.1
[90m/gocode/main/test/e2e/tests/pod.go:219[0m
  Replicator basic tests
  [90m/gocode/main/test/e2e/tests/pod.go:221[0m
    Replicator create, update
    [90m/gocode/main/test/e2e/tests/pod.go:238[0m
[90m------------------------------[0m
[0mNetwork.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9[0m [90mPing an external IP from from a pod[0m 
  [1mPing external IP from a pod created using public network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:501[0m
[BeforeEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:487
    DEBUG: 2019/11/27 00:37:03 START_TEST Network.PingExternalIP
    DEBUG: 2019/11/27 00:37:03 Login to cluster
    DEBUG: 2019/11/27 00:37:03 Checking basic Vnic usage
    DEBUG: 2019/11/27 00:37:03 Updating inventory struct
    DEBUG: 2019/11/27 00:37:04 Checking stale resources
    DEBUG: 2019/11/27 00:37:04 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:37:04 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:37:04 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:37:12 Creating storage classes
[It] Ping external IP from a pod created using public network
  /gocode/main/test/e2e/tests/network-pod.go:501
    DEBUG: 2019/11/27 00:37:21 Creating 1 pods of docker.io/redis:3.0.5 image with network : default and qos : high
    DEBUG: 2019/11/27 00:37:24 IP address ( 172.16.179.6 ) of e2etest-pod-1 is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/27 00:37:24 Trying to ping google-public-dns-a.google.com from pod e2etest-pod-1
    DEBUG: 2019/11/27 00:37:24 google-public-dns-a.google.com is pingable from pod e2etest-pod-1 (172.16.179.6)
    DEBUG: 2019/11/27 00:37:24 Network gateway is 172.16.179.1
    DEBUG: 2019/11/27 00:37:24 Matching default gateway of pod e2etest-pod-1 with 172.16.179.1 
    DEBUG: 2019/11/27 00:37:24 Default gateway of e2etest-pod-1 is 172.16.179.1
    DEBUG: 2019/11/27 00:37:24 Deleting the pod: e2etest-pod-1
[AfterEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:496
    DEBUG: 2019/11/27 00:37:36 END_TEST Network.PingExternalIP Time-taken : 33.78207599
    DEBUG: 2019/11/27 00:37:36 Checking stale resources
    DEBUG: 2019/11/27 00:37:36 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:37:36 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:37:36 Checking stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:33.951 seconds][0m
Network.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9
[90m/gocode/main/test/e2e/tests/network-pod.go:480[0m
  Ping an external IP from from a pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:481[0m
    Ping external IP from a pod created using public network
    [90m/gocode/main/test/e2e/tests/network-pod.go:501[0m
[90m------------------------------[0m
[0mMirroring.ConfigStress Daily SM_Stress-1.1 SM_Stress-1.2 SM_Stress-1.3 SM_Stress-1.4[0m [90mcreate maximum mirrored volumes and run attach detach delete in a loop[0m 
  [1mcreates maximum mirrored volumes and runs attach detach delete in a loop[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:356[0m
[BeforeEach] create maximum mirrored volumes and run attach detach delete in a loop
  /gocode/main/test/e2e/tests/mirroring.go:342
    DEBUG: 2019/11/27 00:37:37 START_TEST Mirroring.ConfigStress
    DEBUG: 2019/11/27 00:37:37 Login to cluster
    DEBUG: 2019/11/27 00:37:37 Checking basic Vnic usage
    DEBUG: 2019/11/27 00:37:37 Updating inventory struct
    DEBUG: 2019/11/27 00:37:38 Checking stale resources
    DEBUG: 2019/11/27 00:37:38 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:37:38 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:37:38 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:37:46 Creating storage classes
[It] creates maximum mirrored volumes and runs attach detach delete in a loop
  /gocode/main/test/e2e/tests/mirroring.go:356
    DEBUG: 2019/11/27 00:37:56 Running Create - Attach - Detach - Delete volumes in a loop
    DEBUG: 2019/11/27 00:37:56 Running iteration no : 1
    DEBUG: 2019/11/27 00:37:56 Assigning mirror label to nodes : [appserv53 appserv54 appserv55] to schedule mirrored volume plexes
    DEBUG: 2019/11/27 00:37:56 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 00:37:56 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 00:37:56 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 00:37:56 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:38:01 Removing mirror label from the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:38:01 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 00:38:01 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 00:38:02 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 00:38:02 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:38:02 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:38:02 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:38:42 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:38:42 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:38:42 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:38:46 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 00:38:46 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 00:38:46 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 00:39:48 Checking Drive Usage on node : appserv53
    DEBUG: 2019/11/27 00:39:49 Checking Drive Usage on node : appserv54
    DEBUG: 2019/11/27 00:39:49 Checking Drive Usage on node : appserv55
    DEBUG: 2019/11/27 00:39:49 Running iteration no : 2
    DEBUG: 2019/11/27 00:39:49 Assigning mirror label to nodes : [appserv53 appserv54 appserv55] to schedule mirrored volume plexes
    DEBUG: 2019/11/27 00:39:49 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 00:39:49 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 00:39:49 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 00:39:49 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:39:58 Removing mirror label from the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:39:58 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 00:39:58 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 00:39:58 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 00:39:58 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:39:58 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:39:58 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:40:41 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:40:41 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:40:41 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:40:46 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 00:40:46 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 00:40:46 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 00:41:49 Checking Drive Usage on node : appserv53
    DEBUG: 2019/11/27 00:41:49 Checking Drive Usage on node : appserv54
    DEBUG: 2019/11/27 00:41:49 Checking Drive Usage on node : appserv55
    DEBUG: 2019/11/27 00:41:49 Running iteration no : 3
    DEBUG: 2019/11/27 00:41:49 Assigning mirror label to nodes : [appserv54 appserv55 appserv53] to schedule mirrored volume plexes
    DEBUG: 2019/11/27 00:41:49 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 00:41:49 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 00:41:49 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 00:41:49 Creating volumes on the nodes : [appserv54 appserv55 appserv53]
    DEBUG: 2019/11/27 00:41:59 Removing mirror label from the nodes : [appserv54 appserv55 appserv53]
    DEBUG: 2019/11/27 00:41:59 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 00:41:59 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 00:41:59 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 00:41:59 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:41:59 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:41:59 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:42:39 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:42:39 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:42:39 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:42:43 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 00:42:43 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 00:42:43 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 00:43:18 Checking Drive Usage on node : appserv53
    DEBUG: 2019/11/27 00:43:18 Checking Drive Usage on node : appserv54
    DEBUG: 2019/11/27 00:43:18 Checking Drive Usage on node : appserv55
    DEBUG: 2019/11/27 00:43:18 /**** Completed Operations: Create - Attach - Detach - Delete volumes in a loop. Total Iterations: 3 ****/
    DEBUG: 2019/11/27 00:43:18 Running Attach - Detach volumes in a loop
    DEBUG: 2019/11/27 00:43:18 Assigning mirror label to nodes : [appserv53 appserv54 appserv55] to schedule mirrored volume plexes
    DEBUG: 2019/11/27 00:43:18 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 00:43:18 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 00:43:18 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 00:43:18 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:43:25 Removing mirror label from the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:43:25 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 00:43:25 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 00:43:26 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 00:43:26 Running iteration no : 1
    DEBUG: 2019/11/27 00:43:26 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:43:26 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:43:26 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:44:05 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:44:05 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:44:05 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:44:09 Running iteration no : 2
    DEBUG: 2019/11/27 00:44:09 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:44:09 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:44:09 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:44:49 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:44:49 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:44:49 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:44:53 Running iteration no : 3
    DEBUG: 2019/11/27 00:44:53 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:44:53 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:44:53 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:45:33 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 00:45:33 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 00:45:33 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 00:45:37 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 00:45:37 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 00:45:37 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 00:46:19 Checking Drive Usage on node : appserv53
    DEBUG: 2019/11/27 00:46:19 Checking Drive Usage on node : appserv54
    DEBUG: 2019/11/27 00:46:19 Checking Drive Usage on node : appserv55
    DEBUG: 2019/11/27 00:46:19 /**** Completed Operations: Attach - Detach volumes in a loop. Total Iterations: 3 ****/
    DEBUG: 2019/11/27 00:46:19 Running Create - Delete volumes in a loop
    DEBUG: 2019/11/27 00:46:19 Running iteration no : 1
    DEBUG: 2019/11/27 00:46:19 Assigning mirror label to nodes : [appserv53 appserv54 appserv55] to schedule mirrored volume plexes
    DEBUG: 2019/11/27 00:46:19 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 00:46:20 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 00:46:20 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 00:46:20 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:46:30 Removing mirror label from the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:46:30 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 00:46:30 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 00:46:30 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 00:46:30 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 00:46:30 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 00:46:30 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 00:46:48 Running iteration no : 2
    DEBUG: 2019/11/27 00:46:48 Assigning mirror label to nodes : [appserv53 appserv54 appserv55] to schedule mirrored volume plexes
    DEBUG: 2019/11/27 00:46:48 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 00:46:49 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 00:46:49 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 00:46:49 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:46:55 Removing mirror label from the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:46:55 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 00:46:55 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 00:46:55 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 00:46:55 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 00:46:55 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 00:46:55 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 00:47:20 Running iteration no : 3
    DEBUG: 2019/11/27 00:47:20 Assigning mirror label to nodes : [appserv53 appserv54 appserv55] to schedule mirrored volume plexes
    DEBUG: 2019/11/27 00:47:20 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 00:47:20 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 00:47:20 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 00:47:20 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:47:26 Removing mirror label from the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 00:47:26 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 00:47:26 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 00:47:26 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 00:47:26 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 00:47:26 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 00:47:26 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 00:47:49 

/******************************************** Stress Test Summary ***************************************************************************/
    DEBUG: 2019/11/27 00:47:49 /**** Completed Operations: Attach - Detach volumes in a loop. Total Iterations: 3 ****/
    DEBUG: 2019/11/27 00:47:49 /**** Completed Operations: Create - Attach - Detach - Delete volumes in a loop. Total Iterations: 3 ****/
    DEBUG: 2019/11/27 00:47:49 /**** Completed Operation. Create - Delete volumes in a loop. Total Iterations: 3 ****/
    DEBUG: 2019/11/27 00:47:49 
/*********************************************************************************************************************************************/


[AfterEach] create maximum mirrored volumes and run attach detach delete in a loop
  /gocode/main/test/e2e/tests/mirroring.go:351
    DEBUG: 2019/11/27 00:47:49 END_TEST Mirroring.ConfigStress Time-taken : 612.205496955
    DEBUG: 2019/11/27 00:47:49 Checking stale resources
    DEBUG: 2019/11/27 00:47:49 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:47:49 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:47:49 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:612.355 seconds][0m
Mirroring.ConfigStress Daily SM_Stress-1.1 SM_Stress-1.2 SM_Stress-1.3 SM_Stress-1.4
[90m/gocode/main/test/e2e/tests/mirroring.go:332[0m
  create maximum mirrored volumes and run attach detach delete in a loop
  [90m/gocode/main/test/e2e/tests/mirroring.go:337[0m
    creates maximum mirrored volumes and runs attach detach delete in a loop
    [90m/gocode/main/test/e2e/tests/mirroring.go:356[0m
[90m------------------------------[0m
[0mMirroring.TwoWayMirroredVolOnlinePlexAdd  Daily SM_PlexAdd-1.2[0m [90mCreate several 2-way mirrored volumes and add [online] plexes to these volumes.[0m 
  [1mCreate 2-way mirrored volumes, do IOs, add plexes, validate data on each plex of each volume.[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:1895[0m
[BeforeEach] Create several 2-way mirrored volumes and add [online] plexes to these volumes.
  /gocode/main/test/e2e/tests/mirroring.go:1881
    DEBUG: 2019/11/27 00:47:49 START_TEST Mirroring.TwoWayMirroredVolOnlinePlexAdd
    DEBUG: 2019/11/27 00:47:49 Login to cluster
    DEBUG: 2019/11/27 00:47:50 Checking basic Vnic usage
    DEBUG: 2019/11/27 00:47:50 Updating inventory struct
    DEBUG: 2019/11/27 00:47:50 Checking stale resources
    DEBUG: 2019/11/27 00:47:50 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:47:50 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:47:50 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:47:58 Creating storage classes
[It] Create 2-way mirrored volumes, do IOs, add plexes, validate data on each plex of each volume.
  /gocode/main/test/e2e/tests/mirroring.go:1895
    DEBUG: 2019/11/27 00:48:08 Creating 8 volumes. Mirror Count: 2:
    DEBUG: 2019/11/27 00:48:08 Mirror Count: 2
    DEBUG: 2019/11/27 00:48:12 Attaching volumes: 
    DEBUG: 2019/11/27 00:48:48 Running write fio job on all volumes: 
    DEBUG: 2019/11/27 00:48:49 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 00:48:49 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 00:48:49 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 00:50:50 Adding new plex to each simple volumes. Expected PlexCount 3: 
    DEBUG: 2019/11/27 00:50:52 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:50:52 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:50:52 Volume name & Plex : test-vol1.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 00:50:53 Volume "test-vol1" has index "0" in embedded.
    DEBUG: 2019/11/27 00:50:54 Volume: test-vol1. Resync offset: 65

    DEBUG: 2019/11/27 00:50:55 Volume: test-vol1. Resync offset: 76

    DEBUG: 2019/11/27 00:50:55 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 00:51:25 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:51:25 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:51:25 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/27 00:51:25 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 00:51:25 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:51:25 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:51:25 Volume name & Plex : test-vol2.p2. Plex State : InUse
    DEBUG: 2019/11/27 00:51:25 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 00:51:25 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:51:25 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:51:25 Volume name & Plex : test-vol3.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 00:51:26 Volume "test-vol3" has index "1" in embedded.
    DEBUG: 2019/11/27 00:51:27 Volume: test-vol3. Resync offset: 76

    DEBUG: 2019/11/27 00:51:28 Volume: test-vol3. Resync offset: 77

    DEBUG: 2019/11/27 00:51:28 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 00:51:58 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:51:58 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:51:58 Volume name & Plex : test-vol3.p2. Plex State : InUse
    DEBUG: 2019/11/27 00:51:58 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/27 00:51:58 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:51:58 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:51:58 Volume name & Plex : test-vol4.p2. Plex State : InUse
    DEBUG: 2019/11/27 00:51:58 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/27 00:51:58 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:51:58 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:51:58 Volume name & Plex : test-vol5.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 00:51:59 Volume "test-vol5" has index "2" in embedded.
    DEBUG: 2019/11/27 00:52:00 Volume: test-vol5. Resync offset: 87

    DEBUG: 2019/11/27 00:52:01 Volume: test-vol5. Resync offset: 88

    DEBUG: 2019/11/27 00:52:01 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 00:52:31 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:52:31 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:52:31 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/27 00:52:31 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/27 00:52:31 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:52:31 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:52:31 Volume name & Plex : test-vol6.p2. Plex State : InUse
    DEBUG: 2019/11/27 00:52:31 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/27 00:52:31 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:52:31 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:52:31 Volume name & Plex : test-vol7.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 00:52:32 Volume "test-vol7" has index "4" in embedded.
    DEBUG: 2019/11/27 00:52:33 Volume: test-vol7. Resync offset: 99

    DEBUG: 2019/11/27 00:52:34 Volume: test-vol7. Resync offset: 
    DEBUG: 2019/11/27 00:52:34 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 00:53:05 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:53:05 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:53:05 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/27 00:53:05 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/27 00:53:05 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 00:53:05 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 00:53:05 Volume name & Plex : test-vol8.p2. Plex State : InUse
    DEBUG: 2019/11/27 00:53:05 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/27 00:53:05 Running read fio job on all volumes: 
    DEBUG: 2019/11/27 00:53:06 Changing preferred plex of volume: test-vol5. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 00:53:06 Changing preferred plex of volume: test-vol2. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 00:53:06 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 00:53:08 Changing preferred plex of volume: test-vol6. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 00:53:08 Changing preferred plex of volume: test-vol4. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 00:53:08 Changing preferred plex of volume: test-vol3. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 00:53:08 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 00:53:09 Changing preferred plex of volume: test-vol7. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 00:53:09 Changing preferred plex of volume: test-vol8. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 00:53:10 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 00:53:10 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 00:53:56 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 00:53:57 Changing preferred plex of volume: test-vol3. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 00:53:58 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 00:54:25 Changing preferred plex of volume: test-vol2. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 00:54:26 Changing preferred plex of volume: test-vol4. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 00:54:26 Changing preferred plex of volume: test-vol5. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 00:54:27 Changing preferred plex of volume: test-vol8. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 00:54:28 Changing preferred plex of volume: test-vol6. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 00:54:28 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 00:54:29 Changing preferred plex of volume: test-vol7. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 00:54:30 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 00:55:12 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 00:55:14 Changing preferred plex of volume: test-vol3. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 00:55:14 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 00:55:21 Changing preferred plex of volume: test-vol5. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 00:55:22 Changing preferred plex of volume: test-vol6. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/27 00:55:24 Changing preferred plex of volume: test-vol7. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 00:55:25 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 00:55:44 Changing preferred plex of volume: test-vol2. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 00:55:45 Changing preferred plex of volume: test-vol4. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 00:55:47 Changing preferred plex of volume: test-vol8. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 00:55:47 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 00:57:06 Detach & delete all volumes: 
[AfterEach] Create several 2-way mirrored volumes and add [online] plexes to these volumes.
  /gocode/main/test/e2e/tests/mirroring.go:1891
    DEBUG: 2019/11/27 00:57:50 END_TEST Mirroring.TwoWayMirroredVolOnlinePlexAdd Time-taken : 601.494124183
    DEBUG: 2019/11/27 00:57:50 Checking stale resources
    DEBUG: 2019/11/27 00:57:50 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:57:50 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:57:50 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:601.649 seconds][0m
Mirroring.TwoWayMirroredVolOnlinePlexAdd  Daily SM_PlexAdd-1.2
[90m/gocode/main/test/e2e/tests/mirroring.go:1874[0m
  Create several 2-way mirrored volumes and add [online] plexes to these volumes.
  [90m/gocode/main/test/e2e/tests/mirroring.go:1875[0m
    Create 2-way mirrored volumes, do IOs, add plexes, validate data on each plex of each volume.
    [90m/gocode/main/test/e2e/tests/mirroring.go:1895[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries to create perf tier with invalid name.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:174[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/27 00:57:51 Login to cluster
    DEBUG: 2019/11/27 00:57:51 Checking basic Vnic usage
    DEBUG: 2019/11/27 00:57:51 Updating inventory struct
    DEBUG: 2019/11/27 00:57:52 Checking stale resources
    DEBUG: 2019/11/27 00:57:52 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:57:52 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:57:52 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:58:00 Creating storage classes
    DEBUG: 2019/11/27 00:58:11 START_TEST PerfTier.NegativeTests
[It] tries to create perf tier with invalid name.
  /gocode/main/test/e2e/tests/perf-tier.go:174
    DEBUG: 2019/11/27 00:58:11 Try to create perf-tier with invalid name.
    ERROR: 2019/11/27 00:58:11  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create temp@$% -b 1G -i 50k', status:&{{Status } {  0} Failure Performance tier validation failed, error: [metadata.name: Invalid value: "temp@$%": a DNS-1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?')] BadRequest <nil> 400}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "Performance tier validation failed, error: [metadata.name: Invalid value: \"temp@$%\": a DNS-1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?')]",
 "reason": "BadRequest",
 "code": 400
}



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/27 00:58:11 END_TEST PerfTier.NegativeTests Time-taken: 0.051619228
    DEBUG: 2019/11/27 00:58:11 Checking stale resources
    DEBUG: 2019/11/27 00:58:11 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:58:11 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:58:11 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:20.816 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries to create perf tier with invalid name.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:174[0m
[90m------------------------------[0m
[0mNetwork.VFsScheduling Daily AT_Scheduling-3.0 AT_Qos-1.4 Qos Multizone[0m [90mOccupy 4 VFs on nicID 0 and start traffic for 4 more VFs, they should be scheduled on nicId 2[0m 
  [1mCreate pods and check scheduling on nicID(s)[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:1466[0m
[BeforeEach] Occupy 4 VFs on nicID 0 and start traffic for 4 more VFs, they should be scheduled on nicId 2
  /gocode/main/test/e2e/tests/network-pod.go:1446
    DEBUG: 2019/11/27 00:58:11 START_TEST Network.VFsScheduling
    DEBUG: 2019/11/27 00:58:11 Login to cluster
    DEBUG: 2019/11/27 00:58:12 Checking basic Vnic usage
    DEBUG: 2019/11/27 00:58:12 Updating inventory struct
    DEBUG: 2019/11/27 00:58:13 Checking stale resources
    DEBUG: 2019/11/27 00:58:13 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 00:58:13 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 00:58:13 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 00:58:21 Creating storage classes
[It] Create pods and check scheduling on nicID(s)
  /gocode/main/test/e2e/tests/network-pod.go:1466
    DEBUG: 2019/11/27 00:58:32 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/27 00:58:32 Creating 8 pods: 
    DEBUG: 2019/11/27 00:58:32 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/27 00:58:32 Getting node label of appserv53: 
    DEBUG: 2019/11/27 00:58:32 Creating iperf server pod: iperf-high-1
    DEBUG: 2019/11/27 00:58:32 Creating iperf server pod: iperf-high-2
    DEBUG: 2019/11/27 00:58:33 Creating iperf server pod: iperf-high-3
    DEBUG: 2019/11/27 00:58:33 Creating iperf server pod: iperf-high-4
    DEBUG: 2019/11/27 00:58:33 Creating iperf server pod: iperf-high-5
    DEBUG: 2019/11/27 00:58:34 Creating iperf server pod: iperf-high-6
    DEBUG: 2019/11/27 00:58:34 Creating iperf server pod: iperf-high-7
    DEBUG: 2019/11/27 00:58:34 Creating iperf server pod: iperf-high-8
    DEBUG: 2019/11/27 00:58:35 Checking if given pods are in Running state
    DEBUG: 2019/11/27 00:58:38 Checking distribution of pods across nicId(s)
    DEBUG: 2019/11/27 00:58:38 Delete all pods which are scheduled on nicId 2
    DEBUG: 2019/11/27 00:58:38 Deleting pods : 
    DEBUG: 2019/11/27 00:58:57 Creating 4 pods: 
    DEBUG: 2019/11/27 00:58:57 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/27 00:58:57 Getting node label of appserv53: 
    DEBUG: 2019/11/27 00:58:57 Creating iperf server pod: iperf-high-new-1
    DEBUG: 2019/11/27 00:59:00 Creating iperf server pod: iperf-high-new-2
    DEBUG: 2019/11/27 00:59:00 Creating iperf server pod: iperf-high-new-3
    DEBUG: 2019/11/27 00:59:00 Creating iperf server pod: iperf-high-new-4
    DEBUG: 2019/11/27 00:59:01 Checking if given pods are in Running state
    DEBUG: 2019/11/27 00:59:03 Deleting all the pods: 
[AfterEach] Occupy 4 VFs on nicID 0 and start traffic for 4 more VFs, they should be scheduled on nicId 2
  /gocode/main/test/e2e/tests/network-pod.go:1454
    DEBUG: 2019/11/27 01:00:21 END_TEST Network.VFsScheduling Time-taken : 129.690833614
    DEBUG: 2019/11/27 01:00:21 Checking stale resources
    DEBUG: 2019/11/27 01:00:21 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:00:21 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:00:21 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:129.881 seconds][0m
Network.VFsScheduling Daily AT_Scheduling-3.0 AT_Qos-1.4 Qos Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:1440[0m
  Occupy 4 VFs on nicID 0 and start traffic for 4 more VFs, they should be scheduled on nicId 2
  [90m/gocode/main/test/e2e/tests/network-pod.go:1441[0m
    Create pods and check scheduling on nicID(s)
    [90m/gocode/main/test/e2e/tests/network-pod.go:1466[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.VFsSchedulingWithBestEffortHighQos Daily AT_Scheduling-3.2 Qos Multizone[0m [90mPods with best-effort, high qos should distribute equally amongst the nicIDs[0m 
  [1mCreate pods with best-effort, high qos and check scheduling on nicID(s)[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:1688[0m
[BeforeEach] Pods with best-effort, high qos should distribute equally amongst the nicIDs
  /gocode/main/test/e2e/tests/network-pod.go:1672
    DEBUG: 2019/11/27 01:00:21 START_TEST Network.VFsSchedulingWithBestEffortHighQos
    DEBUG: 2019/11/27 01:00:21 Login to cluster
    DEBUG: 2019/11/27 01:00:22 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:00:22 Updating inventory struct
    DEBUG: 2019/11/27 01:00:22 Checking stale resources
    DEBUG: 2019/11/27 01:00:23 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:00:23 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:00:23 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:00:30 Creating storage classes
[It] Create pods with best-effort, high qos and check scheduling on nicID(s)
  /gocode/main/test/e2e/tests/network-pod.go:1688
    DEBUG: 2019/11/27 01:00:42 Creating 50 pods with best-effort qos: 
    DEBUG: 2019/11/27 01:00:42 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/27 01:00:42 Getting node label of appserv53: 
    DEBUG: 2019/11/27 01:00:42 Creating iperf server pod: iperf-best-effort-1
    DEBUG: 2019/11/27 01:00:42 Creating iperf server pod: iperf-best-effort-2
    DEBUG: 2019/11/27 01:00:42 Creating iperf server pod: iperf-best-effort-3
    DEBUG: 2019/11/27 01:00:43 Creating iperf server pod: iperf-best-effort-4
    DEBUG: 2019/11/27 01:00:43 Creating iperf server pod: iperf-best-effort-5
    DEBUG: 2019/11/27 01:00:43 Creating iperf server pod: iperf-best-effort-6
    DEBUG: 2019/11/27 01:00:44 Creating iperf server pod: iperf-best-effort-7
    DEBUG: 2019/11/27 01:00:44 Creating iperf server pod: iperf-best-effort-8
    DEBUG: 2019/11/27 01:00:44 Creating iperf server pod: iperf-best-effort-9
    DEBUG: 2019/11/27 01:00:45 Creating iperf server pod: iperf-best-effort-10
    DEBUG: 2019/11/27 01:00:45 Creating iperf server pod: iperf-best-effort-11
    DEBUG: 2019/11/27 01:00:45 Creating iperf server pod: iperf-best-effort-12
    DEBUG: 2019/11/27 01:00:46 Creating iperf server pod: iperf-best-effort-13
    DEBUG: 2019/11/27 01:00:46 Creating iperf server pod: iperf-best-effort-14
    DEBUG: 2019/11/27 01:00:46 Creating iperf server pod: iperf-best-effort-15
    DEBUG: 2019/11/27 01:00:47 Creating iperf server pod: iperf-best-effort-16
    DEBUG: 2019/11/27 01:00:47 Creating iperf server pod: iperf-best-effort-17
    DEBUG: 2019/11/27 01:00:47 Creating iperf server pod: iperf-best-effort-18
    DEBUG: 2019/11/27 01:00:48 Creating iperf server pod: iperf-best-effort-19
    DEBUG: 2019/11/27 01:00:48 Creating iperf server pod: iperf-best-effort-20
    DEBUG: 2019/11/27 01:00:48 Creating iperf server pod: iperf-best-effort-21
    DEBUG: 2019/11/27 01:00:49 Creating iperf server pod: iperf-best-effort-22
    DEBUG: 2019/11/27 01:00:49 Creating iperf server pod: iperf-best-effort-23
    DEBUG: 2019/11/27 01:00:49 Creating iperf server pod: iperf-best-effort-24
    DEBUG: 2019/11/27 01:00:50 Creating iperf server pod: iperf-best-effort-25
    DEBUG: 2019/11/27 01:00:50 Creating iperf server pod: iperf-best-effort-26
    DEBUG: 2019/11/27 01:00:50 Creating iperf server pod: iperf-best-effort-27
    DEBUG: 2019/11/27 01:00:51 Creating iperf server pod: iperf-best-effort-28
    DEBUG: 2019/11/27 01:00:51 Creating iperf server pod: iperf-best-effort-29
    DEBUG: 2019/11/27 01:00:51 Creating iperf server pod: iperf-best-effort-30
    DEBUG: 2019/11/27 01:00:52 Creating iperf server pod: iperf-best-effort-31
    DEBUG: 2019/11/27 01:00:52 Creating iperf server pod: iperf-best-effort-32
    DEBUG: 2019/11/27 01:00:52 Creating iperf server pod: iperf-best-effort-33
    DEBUG: 2019/11/27 01:00:52 Creating iperf server pod: iperf-best-effort-34
    DEBUG: 2019/11/27 01:00:53 Creating iperf server pod: iperf-best-effort-35
    DEBUG: 2019/11/27 01:00:53 Creating iperf server pod: iperf-best-effort-36
    DEBUG: 2019/11/27 01:00:53 Creating iperf server pod: iperf-best-effort-37
    DEBUG: 2019/11/27 01:00:54 Creating iperf server pod: iperf-best-effort-38
    DEBUG: 2019/11/27 01:00:54 Creating iperf server pod: iperf-best-effort-39
    DEBUG: 2019/11/27 01:00:54 Creating iperf server pod: iperf-best-effort-40
    DEBUG: 2019/11/27 01:00:55 Creating iperf server pod: iperf-best-effort-41
    DEBUG: 2019/11/27 01:00:55 Creating iperf server pod: iperf-best-effort-42
    DEBUG: 2019/11/27 01:00:55 Creating iperf server pod: iperf-best-effort-43
    DEBUG: 2019/11/27 01:00:56 Creating iperf server pod: iperf-best-effort-44
    DEBUG: 2019/11/27 01:00:56 Creating iperf server pod: iperf-best-effort-45
    DEBUG: 2019/11/27 01:00:56 Creating iperf server pod: iperf-best-effort-46
    DEBUG: 2019/11/27 01:00:57 Creating iperf server pod: iperf-best-effort-47
    DEBUG: 2019/11/27 01:00:57 Creating iperf server pod: iperf-best-effort-48
    DEBUG: 2019/11/27 01:00:57 Creating iperf server pod: iperf-best-effort-49
    DEBUG: 2019/11/27 01:00:58 Creating iperf server pod: iperf-best-effort-50
    DEBUG: 2019/11/27 01:00:58 Checking if given pods are in Running state
    DEBUG: 2019/11/27 01:01:17 Creating 10 pods with high qos: 
    DEBUG: 2019/11/27 01:01:17 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/27 01:01:17 Getting node label of appserv53: 
    DEBUG: 2019/11/27 01:01:17 Creating iperf server pod: iperf-high-1
    DEBUG: 2019/11/27 01:01:19 Creating iperf server pod: iperf-high-2
    DEBUG: 2019/11/27 01:01:20 Creating iperf server pod: iperf-high-3
    DEBUG: 2019/11/27 01:01:20 Creating iperf server pod: iperf-high-4
    DEBUG: 2019/11/27 01:01:20 Creating iperf server pod: iperf-high-5
    DEBUG: 2019/11/27 01:01:21 Creating iperf server pod: iperf-high-6
    DEBUG: 2019/11/27 01:01:21 Creating iperf server pod: iperf-high-7
    DEBUG: 2019/11/27 01:01:22 Creating iperf server pod: iperf-high-8
    DEBUG: 2019/11/27 01:01:22 Creating iperf server pod: iperf-high-9
    DEBUG: 2019/11/27 01:01:22 Creating iperf server pod: iperf-high-10
    DEBUG: 2019/11/27 01:01:23 Checking if given pods are in Running state
    DEBUG: 2019/11/27 01:01:26 Deleting all the pods: 
[AfterEach] Pods with best-effort, high qos should distribute equally amongst the nicIDs
  /gocode/main/test/e2e/tests/network-pod.go:1683
    DEBUG: 2019/11/27 01:11:26 END_TEST Network.VFsSchedulingWithBestEffortHighQos Time-taken : 665.124993473
    DEBUG: 2019/11/27 01:11:26 Checking stale resources
    DEBUG: 2019/11/27 01:11:26 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:11:26 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:11:26 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:665.289 seconds][0m
Network.VFsSchedulingWithBestEffortHighQos Daily AT_Scheduling-3.2 Qos Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:1666[0m
  Pods with best-effort, high qos should distribute equally amongst the nicIDs
  [90m/gocode/main/test/e2e/tests/network-pod.go:1667[0m
    Create pods with best-effort, high qos and check scheduling on nicID(s)
    [90m/gocode/main/test/e2e/tests/network-pod.go:1688[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mRbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 [0m [90mValidate all Edit and View roles[0m 
  [1mValidate volume-edit role[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:482[0m
[BeforeEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:447
    DEBUG: 2019/11/27 01:11:27 START_TEST Rbac.RoleValidation
    DEBUG: 2019/11/27 01:11:27 Login to cluster
    DEBUG: 2019/11/27 01:11:27 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:11:27 Updating inventory struct
    DEBUG: 2019/11/27 01:11:28 Checking stale resources
    DEBUG: 2019/11/27 01:11:28 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:11:28 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:11:28 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:11:36 Creating storage classes
[It] Validate volume-edit role
  /gocode/main/test/e2e/tests/rbac.go:482
    DEBUG: 2019/11/27 01:11:45 Validate volume-edit role
    DEBUG: 2019/11/27 01:11:46 Creating group grp1 with volume-edit role(s)
    DEBUG: 2019/11/27 01:11:46 Creating user1 user in grp1 group
    DEBUG: 2019/11/27 01:11:46 Login as user1 user
    DEBUG: 2019/11/27 01:11:46 Operation validated : perftier-list
    ERROR: 2019/11/27 01:11:46  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create temp-pf -b 2G -i 50K', status:&{{Status } {  0} Failure POST on perftiers for "user1" is forbidden: User user1 cannot perform POST on perftiers Forbidden 0xc0009f60f0 403}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "POST on perftiers for \"user1\" is forbidden: User user1 cannot perform POST on perftiers",
 "reason": "Forbidden",
 "details": {
  "name": "user1",
  "kind": "perftiers"
 },
 "code": 403
}



    DEBUG: 2019/11/27 01:11:46 Operation validated : perftier-create
    DEBUG: 2019/11/27 01:11:49 Operation validated : perftier-delete
    DEBUG: 2019/11/27 01:12:19 Operation validated : volume-list
    DEBUG: 2019/11/27 01:12:48 Operation validated : volume-create
    DEBUG: 2019/11/27 01:13:18 Operation validated : volume-delete
    DEBUG: 2019/11/27 01:13:21 Operation validated : user-list
    DEBUG: 2019/11/27 01:13:21 Operation validated : user-create
    DEBUG: 2019/11/27 01:13:25 Operation validated : user-delete
[AfterEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:456
    DEBUG: 2019/11/27 01:13:25 END_TEST Rbac.RoleValidation Time-taken : 118.948781033
    DEBUG: 2019/11/27 01:13:25 Checking stale resources
    DEBUG: 2019/11/27 01:13:26 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:13:26 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:13:26 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:119.103 seconds][0m
Rbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 
[90m/gocode/main/test/e2e/tests/rbac.go:437[0m
  Validate all Edit and View roles
  [90m/gocode/main/test/e2e/tests/rbac.go:438[0m
    Validate volume-edit role
    [90m/gocode/main/test/e2e/tests/rbac.go:482[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.SecureAuthWindowsLDAP Daily Rbac_WSecureAuth-1.0[0m [90mRbac : Windows Secure LDAP auth-server test[0m 
  [1mSecure LDAP auth-server test[0m
  [37m/gocode/main/test/e2e/tests/rbac-windows.go:143[0m
[BeforeEach] Rbac : Windows Secure LDAP auth-server test
  /gocode/main/test/e2e/tests/rbac-windows.go:129
    DEBUG: 2019/11/27 01:13:26 START_TEST Rbac.SecureAuthWindowsLDAP
    DEBUG: 2019/11/27 01:13:26 Login to cluster
    DEBUG: 2019/11/27 01:13:26 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:13:26 Updating inventory struct
    DEBUG: 2019/11/27 01:13:27 Checking stale resources
    DEBUG: 2019/11/27 01:13:27 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:13:27 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:13:27 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:13:35 Creating storage classes
[It] Secure LDAP auth-server test
  /gocode/main/test/e2e/tests/rbac-windows.go:143
    DEBUG: 2019/11/27 01:13:45 Removing ds1.example.com to IP mapping from /etc/hosts on all nodes.
    DEBUG: 2019/11/27 01:13:46 Windows LDAP server IP is :172.16.17.6
    DEBUG: 2019/11/27 01:13:46 Adding the ldap-server to IP mapping to /etc/hosts on node appserv53
    DEBUG: 2019/11/27 01:13:47 Adding the ldap-server to IP mapping to /etc/hosts on node appserv54
    DEBUG: 2019/11/27 01:13:47 Adding the ldap-server to IP mapping to /etc/hosts on node appserv55
    DEBUG: 2019/11/27 01:13:48 Create auth-server
    DEBUG: 2019/11/27 01:13:48 Create group container-admin-usersbucket with container-edit/usersbucket role(s)
    DEBUG: 2019/11/27 01:13:48 Create aduser1 remote user
    DEBUG: 2019/11/27 01:14:18 List auth-server
    DEBUG: 2019/11/27 01:14:18 Login as aduser1 remote user
    ERROR: 2019/11/27 01:14:18  login.go:111: Error: User login failed.
    DEBUG: 2019/11/27 01:14:19 Edit auth-server to enable
    DEBUG: 2019/11/27 01:14:24 Get auth-server
    DEBUG: 2019/11/27 01:14:24 Login as aduser1 remote user
    DEBUG: 2019/11/27 01:14:25 Create auth-server with the same name again
    DEBUG: 2019/11/27 01:14:26 Delete remote user aduser1
    DEBUG: 2019/11/27 01:14:26 Delete group container-admin-usersbucket
    DEBUG: 2019/11/27 01:14:26 Cleanup LDAP auth-server
    DEBUG: 2019/11/27 01:14:26 Delete auth-server
    DEBUG: 2019/11/27 01:14:26 Try to delete auth-server again
    DEBUG: 2019/11/27 01:14:26 Removing the LDAP server entry from /etc/hosts of each node.
    DEBUG: 2019/11/27 01:14:26 Removing ds1.example.com to IP mapping from /etc/hosts on all nodes.
[AfterEach] Rbac : Windows Secure LDAP auth-server test
  /gocode/main/test/e2e/tests/rbac-windows.go:137
    DEBUG: 2019/11/27 01:14:31 END_TEST Rbac.SecureAuthWindowsLDAP Time-taken : 65.508114149
    DEBUG: 2019/11/27 01:14:31 Checking stale resources
    DEBUG: 2019/11/27 01:14:31 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:14:31 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:14:31 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:65.659 seconds][0m
Rbac.SecureAuthWindowsLDAP Daily Rbac_WSecureAuth-1.0
[90m/gocode/main/test/e2e/tests/rbac-windows.go:114[0m
  Rbac : Windows Secure LDAP auth-server test
  [90m/gocode/main/test/e2e/tests/rbac-windows.go:116[0m
    Secure LDAP auth-server test
    [90m/gocode/main/test/e2e/tests/rbac-windows.go:143[0m
[90m------------------------------[0m
[0mMirroring.RebootTestSHA512Check Daily SM_Basic-2.0 SM_Basic-2.1 SM_Reboot-2.0 SM_Reboot-2.1 SM_Verify-1.0[0m [90mreboot test with sha512 checksum on mirrored volumes for sanity[0m 
  [1mreboot test with sha512 checksum on mirrored volumes for sanity[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:115[0m
[BeforeEach] reboot test with sha512 checksum on mirrored volumes for sanity
  /gocode/main/test/e2e/tests/mirroring.go:101
    DEBUG: 2019/11/27 01:14:31 START_TEST Mirroring.RebootTestSHA512Check
    DEBUG: 2019/11/27 01:14:31 Login to cluster
    DEBUG: 2019/11/27 01:14:32 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:14:32 Updating inventory struct
    DEBUG: 2019/11/27 01:14:32 Checking stale resources
    DEBUG: 2019/11/27 01:14:33 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:14:33 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:14:33 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:14:40 Creating storage classes
[It] reboot test with sha512 checksum on mirrored volumes for sanity
  /gocode/main/test/e2e/tests/mirroring.go:115
    DEBUG: 2019/11/27 01:14:50 Verifying whether FBM and L1 usage is zero across all nodes
    DEBUG: 2019/11/27 01:14:56 FBM and L1 usage is Zero across all nodes

    DEBUG: 2019/11/27 01:14:56 Assigning label to nodes where the plexes of mirrored volumes should get scheduled
    DEBUG: 2019/11/27 01:14:56 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 01:14:56 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 01:14:57 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 01:14:57 Creating 3 volumes of random sizes between 1073741824 and 107374182400
    DEBUG: 2019/11/27 01:14:57 Mirror Count: 3
    DEBUG: 2019/11/27 01:14:59 Attaching all 3 volumes
    DEBUG: 2019/11/27 01:15:14 Volume2uuid_mapping: 
test-vol1 6176ba78-10f6-11ea-b2ea-a4bf01194d67
test-vol2 623555c5-10f6-11ea-b2ea-a4bf01194d67
test-vol3 624b0719-10f6-11ea-b2ea-a4bf01194d67

    DEBUG: 2019/11/27 01:15:14 Running fio job on all the storage volumes
    DEBUG: 2019/11/27 01:15:14 FIO Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --runtime=60 --time_based --thread=1 --numjobs=1 --iodepth=16 --bs=4K --rw=randwrite --verify=pattern --verify_pattern=0xff%o\"pqrs\"-12 --verify_interval=4096 --do_verify=0 --verify_state_save=1 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --clat_percentiles=0 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1

    DEBUG: 2019/11/27 01:16:18 LDFS Verification Output for all nodes Before Reboot is : 
sputil -i output for : appserv55
	FBM is consistent on drive: PHFT6401002U800HGN, used: 9217352
	L1 bitmap is consistent on drive: PHFT6401002U800HGN, used: 4566
	FBM is consistent on drive: PHFT639300BS800HGN, used: 8825752
	L1 bitmap is consistent on drive: PHFT639300BS800HGN, used: 4566
	FBM is consistent on drive: PHFT6401001Z800HGN, used: 9916952
	L1 bitmap is consistent on drive: PHFT6401001Z800HGN, used: 4566
	FBM is consistent on drive: PHFT640000C1800HGN, used: 11870416
	L1 bitmap is consistent on drive: PHFT640000C1800HGN, used: 4566
sputil -i output for : appserv53
	FBM is consistent on drive: CVFT5236009G800HGN, used: 9217352
	L1 bitmap is consistent on drive: CVFT5236009G800HGN, used: 4566
	FBM is consistent on drive: CVFT52360003800HGN, used: 11870416
	L1 bitmap is consistent on drive: CVFT52360003800HGN, used: 4566
	FBM is consistent on drive: CVFT4395000K800HGN, used: 8825752
	L1 bitmap is consistent on drive: CVFT4395000K800HGN, used: 4566
	FBM is consistent on drive: CVFT5236000J800HGN, used: 9916952
	L1 bitmap is consistent on drive: CVFT5236000J800HGN, used: 4566
sputil -i output for : appserv54
	FBM is consistent on drive: CVFT439500CK800HGN, used: 9916952
	L1 bitmap is consistent on drive: CVFT439500CK800HGN, used: 4566
	FBM is consistent on drive: PHFT6393000P800HGN, used: 9217352
	L1 bitmap is consistent on drive: PHFT6393000P800HGN, used: 4566
	FBM is consistent on drive: PHFT64010038800HGN, used: 8825752
	L1 bitmap is consistent on drive: PHFT64010038800HGN, used: 4566
	FBM is consistent on drive: CVFT43950077800HGN, used: 11870416
	L1 bitmap is consistent on drive: CVFT43950077800HGN, used: 4566


    DEBUG: 2019/11/27 01:16:18 Verifying whether FBM Usage and L1 usage is the same across all nodes before reboot
    DEBUG: 2019/11/27 01:16:25 Calculating sha512sum on every plex of each mirrored volume.
    DEBUG: 2019/11/27 01:16:26 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 01:16:27 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 01:16:29 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 01:16:29 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 01:16:29 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 01:16:30 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 01:18:04 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 01:18:05 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 01:18:07 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 01:18:08 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 01:18:08 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 01:18:08 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 01:19:41 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 01:19:43 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 01:19:44 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 01:19:45 Calculating cksum for volume test-vol1 and plex p2
    DEBUG: 2019/11/27 01:19:45 Calculating cksum for volume test-vol2 and plex p2
    DEBUG: 2019/11/27 01:19:45 Calculating cksum for volume test-vol3 and plex p2
    DEBUG: 2019/11/27 01:21:20 Getting cluster quorum nodes
    DEBUG: 2019/11/27 01:21:20 Powering OFF the node appserv54
    DEBUG: 2019/11/27 01:21:20 Node 172.16.6.154 took 0 seconds to power off
    DEBUG: 2019/11/27 01:21:20 Ensuring that appserv54 node is unreachable: 
    DEBUG: 2019/11/27 01:21:20 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:21:29 Polling to check until node: appserv54 goes down
    DEBUG: 2019/11/27 01:23:13 Powering ON the node appserv54
    DEBUG: 2019/11/27 01:23:14 Node 172.16.6.154 took 1 seconds to power on
    DEBUG: 2019/11/27 01:23:14 Checking if node appserv54 is reachable or not: 
    DEBUG: 2019/11/27 01:23:14 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:23:31 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:23:48 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:24:05 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:24:22 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:24:39 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:24:57 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:25:14 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:25:31 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:25:35 appserv54 is pingable from local machine
    DEBUG: 2019/11/27 01:25:35 Checking ssh port is up or not on node: appserv54
    DEBUG: 2019/11/27 01:26:05 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/27 01:26:05 Found '3' nodes
    DEBUG: 2019/11/27 01:26:05 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 01:26:05 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 01:26:50 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 01:27:00 After power cycle/reboot, updating timestamp of node : appserv54
    DEBUG: 2019/11/27 01:27:03 Getting cluster quorum nodes
    DEBUG: 2019/11/27 01:28:03 Updating inventory struct
    DEBUG: 2019/11/27 01:28:04 Volume2uuid_mapping: 
test-vol1 6176ba78-10f6-11ea-b2ea-a4bf01194d67
test-vol2 623555c5-10f6-11ea-b2ea-a4bf01194d67
test-vol3 624b0719-10f6-11ea-b2ea-a4bf01194d67

    DEBUG: 2019/11/27 01:28:04 Comparing Volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/27 01:28:05 Comparing the UUID before and after reboot for all volumes
    DEBUG: 2019/11/27 01:28:06 Number of volumes : 3
    DEBUG: 2019/11/27 01:28:06 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 01:28:06 Volume name & Plex : test-vol3.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 01:28:07 Volume "test-vol3" has index "2" in embedded.
    DEBUG: 2019/11/27 01:28:08 Volume: test-vol3. Resync offset: 82

    DEBUG: 2019/11/27 01:28:08 Volume: test-vol3. Resync offset: 82

    DEBUG: 2019/11/27 01:28:14 Volume: test-vol3. Resync offset: 88

    DEBUG: 2019/11/27 01:28:14 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:28:14 Volume name & Plex : test-vol3.p2. Plex State : InUse
    DEBUG: 2019/11/27 01:28:14 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 01:28:44 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:28:44 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:28:44 Volume name & Plex : test-vol3.p2. Plex State : InUse
    DEBUG: 2019/11/27 01:28:44 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/27 01:28:44 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 01:28:44 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:28:44 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:28:44 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/27 01:28:44 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 01:28:44 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 01:28:44 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:28:44 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:28:44 Volume name & Plex : test-vol2.p2. Plex State : InUse
    DEBUG: 2019/11/27 01:28:44 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 01:30:29 LDFS Verification Output for all nodes After Reboot is : 
sputil -i output for : appserv55
	FBM is consistent on drive: PHFT6401002U800HGN, used: 9217352
	L1 bitmap is consistent on drive: PHFT6401002U800HGN, used: 4566
	FBM is consistent on drive: PHFT639300BS800HGN, used: 8825752
	L1 bitmap is consistent on drive: PHFT639300BS800HGN, used: 4566
	FBM is consistent on drive: PHFT6401001Z800HGN, used: 9916952
	L1 bitmap is consistent on drive: PHFT6401001Z800HGN, used: 4566
	FBM is consistent on drive: PHFT640000C1800HGN, used: 11870416
	L1 bitmap is consistent on drive: PHFT640000C1800HGN, used: 4566
sputil -i output for : appserv53
	FBM is consistent on drive: CVFT5236009G800HGN, used: 9217352
	L1 bitmap is consistent on drive: CVFT5236009G800HGN, used: 4566
	FBM is consistent on drive: CVFT52360003800HGN, used: 11870416
	L1 bitmap is consistent on drive: CVFT52360003800HGN, used: 4566
	FBM is consistent on drive: CVFT4395000K800HGN, used: 8825752
	L1 bitmap is consistent on drive: CVFT4395000K800HGN, used: 4566
	FBM is consistent on drive: CVFT5236000J800HGN, used: 9916952
	L1 bitmap is consistent on drive: CVFT5236000J800HGN, used: 4566
sputil -i output for : appserv54
	FBM is consistent on drive: CVFT439500CK800HGN, used: 9916952
	L1 bitmap is consistent on drive: CVFT439500CK800HGN, used: 4566
	FBM is consistent on drive: PHFT6393000P800HGN, used: 9217352
	L1 bitmap is consistent on drive: PHFT6393000P800HGN, used: 4566
	FBM is consistent on drive: PHFT64010038800HGN, used: 8825752
	L1 bitmap is consistent on drive: PHFT64010038800HGN, used: 4566
	FBM is consistent on drive: CVFT43950077800HGN, used: 11870416
	L1 bitmap is consistent on drive: CVFT43950077800HGN, used: 4566


    DEBUG: 2019/11/27 01:30:29 Verifying LDFS Verification Output before and after reboot for all the nodes
    DEBUG: 2019/11/27 01:30:29 Verifying whether FBM Usage and L1 usage is the same across all nodes after reboot
    DEBUG: 2019/11/27 01:30:36 Verifying whether FBM Usage and L1 usage is the same before and after reboot for all the nodes
    DEBUG: 2019/11/27 01:30:36 After rebooting initiator node, calculating sha512sum on every plex of each mirrored volume.
    DEBUG: 2019/11/27 01:30:37 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 01:30:38 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 01:30:40 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 01:30:40 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 01:30:40 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 01:30:41 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 01:32:16 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 01:32:17 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 01:32:19 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 01:32:20 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 01:32:20 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 01:32:20 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 01:33:59 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 01:34:01 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 01:34:02 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 01:34:03 Calculating cksum for volume test-vol1 and plex p2
    DEBUG: 2019/11/27 01:34:03 Calculating cksum for volume test-vol2 and plex p2
    DEBUG: 2019/11/27 01:34:03 Calculating cksum for volume test-vol3 and plex p2
    DEBUG: 2019/11/27 01:35:33 Comparing the SHA512 checksum before and after reboot for all volumes
    DEBUG: 2019/11/27 01:35:33 Detach & Delete all volumes
    DEBUG: 2019/11/27 01:36:21 Removing label from the nodes where the plexes of mirrored volumes were scheduled
    DEBUG: 2019/11/27 01:36:21 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 01:36:21 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 01:36:21 Removed label : mirror from node : appserv54
[AfterEach] reboot test with sha512 checksum on mirrored volumes for sanity
  /gocode/main/test/e2e/tests/mirroring.go:110
    DEBUG: 2019/11/27 01:36:21 END_TEST Mirroring.RebootTestSHA512Check Time-taken : 1309.833889237
    DEBUG: 2019/11/27 01:36:21 Checking stale resources
    DEBUG: 2019/11/27 01:36:21 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:36:21 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:36:21 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:1309.982 seconds][0m
Mirroring.RebootTestSHA512Check Daily SM_Basic-2.0 SM_Basic-2.1 SM_Reboot-2.0 SM_Reboot-2.1 SM_Verify-1.0
[90m/gocode/main/test/e2e/tests/mirroring.go:93[0m
  reboot test with sha512 checksum on mirrored volumes for sanity
  [90m/gocode/main/test/e2e/tests/mirroring.go:96[0m
    reboot test with sha512 checksum on mirrored volumes for sanity
    [90m/gocode/main/test/e2e/tests/mirroring.go:115[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.EditView Daily Rbac_Local_Basic-2.0[0m [90mUser can edit/view in it's namespace[0m 
  [1mUser can edit/view volume(s)[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:554[0m
[BeforeEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:528
    DEBUG: 2019/11/27 01:36:21 START_TEST Rbac.EditView
    DEBUG: 2019/11/27 01:36:21 Login to cluster
    DEBUG: 2019/11/27 01:36:22 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:36:22 Updating inventory struct
    DEBUG: 2019/11/27 01:36:22 Checking stale resources
    DEBUG: 2019/11/27 01:36:22 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:36:22 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:36:22 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:36:30 Creating storage classes
    DEBUG: 2019/11/27 01:36:40 User Logout
[It] User can edit/view volume(s)
  /gocode/main/test/e2e/tests/rbac.go:554
    DEBUG: 2019/11/27 01:36:41 Creating group, user with role(s)
    DEBUG: 2019/11/27 01:36:42 Creating group jacksgroup with volume-edit role(s)
    DEBUG: 2019/11/27 01:36:42 Creating jack user in jacksgroup group
    DEBUG: 2019/11/27 01:36:42 Login as jack user
    DEBUG: 2019/11/27 01:36:42 Creating the volume with volume-edit role.
    DEBUG: 2019/11/27 01:36:42 Listing volumes
    DEBUG: 2019/11/27 01:36:42 Editing group role(s)
    DEBUG: 2019/11/27 01:36:43 Editing jacksgroup group with volume-view role(s)
    DEBUG: 2019/11/27 01:36:43 Login as jack user
    DEBUG: 2019/11/27 01:36:44 Listing volumes
    DEBUG: 2019/11/27 01:36:44 Creating the volume with volume-view role.
    DEBUG: 2019/11/27 01:36:44 Editing group role(s)
    DEBUG: 2019/11/27 01:36:45 Editing jacksgroup group with volume-edit role(s)
    DEBUG: 2019/11/27 01:36:45 Login as jack user
    DEBUG: 2019/11/27 01:36:45 Deleting volume
[AfterEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:539
    DEBUG: 2019/11/27 01:37:21 User Logout
    DEBUG: 2019/11/27 01:37:22 END_TEST Rbac.EditView Time-taken : 60.313972179
    DEBUG: 2019/11/27 01:37:22 Checking stale resources
    DEBUG: 2019/11/27 01:37:22 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:37:22 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:37:22 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:60.470 seconds][0m
Rbac.EditView Daily Rbac_Local_Basic-2.0
[90m/gocode/main/test/e2e/tests/rbac.go:518[0m
  User can edit/view in it's namespace
  [90m/gocode/main/test/e2e/tests/rbac.go:520[0m
    User can edit/view volume(s)
    [90m/gocode/main/test/e2e/tests/rbac.go:554[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries creating qos template with invalid value for max iops.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:224[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/27 01:37:22 Login to cluster
    DEBUG: 2019/11/27 01:37:22 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:37:22 Updating inventory struct
    DEBUG: 2019/11/27 01:37:23 Checking stale resources
    DEBUG: 2019/11/27 01:37:23 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:37:23 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:37:23 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:37:31 Creating storage classes
    DEBUG: 2019/11/27 01:37:40 START_TEST PerfTier.NegativeTests
[It] tries creating qos template with invalid value for max iops.
  /gocode/main/test/e2e/tests/perf-tier.go:224
    ERROR: 2019/11/27 01:37:40  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b 1G -i 50k -I invalidIOPs@3!', output:, error:Error: Invalid --max-storage-iops/-I specification.



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/27 01:37:40 END_TEST PerfTier.NegativeTests Time-taken: 0.021941745
    DEBUG: 2019/11/27 01:37:40 Checking stale resources
    DEBUG: 2019/11/27 01:37:40 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:37:40 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:37:40 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:18.092 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries creating qos template with invalid value for max iops.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:224[0m
[90m------------------------------[0m
[0mNetwork.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9[0m [90mPing an external IP from from a pod[0m 
  [1mPing external IP from a pod created without using any network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:601[0m
[BeforeEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:487
    DEBUG: 2019/11/27 01:37:40 START_TEST Network.PingExternalIP
    DEBUG: 2019/11/27 01:37:40 Login to cluster
    DEBUG: 2019/11/27 01:37:40 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:37:40 Updating inventory struct
    DEBUG: 2019/11/27 01:37:41 Checking stale resources
    DEBUG: 2019/11/27 01:37:41 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:37:41 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:37:41 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:37:49 Creating storage classes
[It] Ping external IP from a pod created without using any network
  /gocode/main/test/e2e/tests/network-pod.go:601
    DEBUG: 2019/11/27 01:37:58 Creating 1 pods of docker.io/redis:3.0.5 image with network : none and qos : 
    DEBUG: 2019/11/27 01:38:01 Trying to ping google-public-dns-a.google.com from pod e2etest-pod-1
    DEBUG: 2019/11/27 01:38:01 google-public-dns-a.google.com is pingable from pod e2etest-pod-1 (172.20.0.6)
    DEBUG: 2019/11/27 01:38:01 Getting managment inteface from pod host ( appserv54 ) 
    DEBUG: 2019/11/27 01:38:02 Management interface of appserv54 is 172.20.0.1

    DEBUG: 2019/11/27 01:38:02 Matching default gateway of pod e2etest-pod-1 with 172.20.0.1 
    DEBUG: 2019/11/27 01:38:02 Default gateway of e2etest-pod-1 is 172.20.0.1
    DEBUG: 2019/11/27 01:38:02 Deleting the pod: e2etest-pod-1
[AfterEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:496
    DEBUG: 2019/11/27 01:38:10 END_TEST Network.PingExternalIP Time-taken : 30.078842081
    DEBUG: 2019/11/27 01:38:10 Checking stale resources
    DEBUG: 2019/11/27 01:38:10 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:38:10 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:38:10 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:30.222 seconds][0m
Network.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9
[90m/gocode/main/test/e2e/tests/network-pod.go:480[0m
  Ping an external IP from from a pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:481[0m
    Ping external IP from a pod created without using any network
    [90m/gocode/main/test/e2e/tests/network-pod.go:601[0m
[90m------------------------------[0m
[0mNetwork.GatewayPing Daily N_Basic-1.15 N_Basic-1.16 Multizone[0m [90mPing gateway of a network from a pod[0m 
  [1mPing gateway of a network(invalid vlan) from a pod[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:399[0m
[BeforeEach] Ping gateway of a network from a pod
  /gocode/main/test/e2e/tests/network-pod.go:373
    DEBUG: 2019/11/27 01:38:10 START_TEST Network.GatewayPing
    DEBUG: 2019/11/27 01:38:10 Login to cluster
    DEBUG: 2019/11/27 01:38:11 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:38:11 Updating inventory struct
    DEBUG: 2019/11/27 01:38:11 Checking stale resources
    DEBUG: 2019/11/27 01:38:11 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:38:11 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:38:11 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:38:19 Creating storage classes
    DEBUG: 2019/11/27 01:38:29 Creating a test pod with docker.io/redis:3.0.5 image, default network and with valid VLAN
[It] Ping gateway of a network(invalid vlan) from a pod
  /gocode/main/test/e2e/tests/network-pod.go:399
    DEBUG: 2019/11/27 01:38:32 Creating test network : networkname1 with invalid vlan
    DEBUG: 2019/11/27 01:38:32 Gateway IP of networkname1 network is 56.12.100.1
    DEBUG: 2019/11/27 01:38:32 Trying to ping the gateway of network with invalid VLAN from pod e2etest-pod
    DEBUG: 2019/11/27 01:38:37 Output : PING 56.12.100.1 (56.12.100.1): 48 data bytes
....--- 56.12.100.1 ping statistics ---
5 packets transmitted, 0 packets received, 100% packet loss
, Error : failed to run commmand 'kubectl exec e2etest-pod -- ping -f -c 5 -W 5 56.12.100.1', output:PING 56.12.100.1 (56.12.100.1): 48 data bytes
....--- 56.12.100.1 ping statistics ---
5 packets transmitted, 0 packets received, 100% packet loss
, error:command terminated with exit code 1

, command terminated with exit code 1

    DEBUG: 2019/11/27 01:38:37 Delete network networkname1
[AfterEach] Ping gateway of a network from a pod
  /gocode/main/test/e2e/tests/network-pod.go:427
    DEBUG: 2019/11/27 01:38:37 Deleting the pod
    DEBUG: 2019/11/27 01:38:46 END_TEST Network.GatewayPing Time-taken : 36.308509077
    DEBUG: 2019/11/27 01:38:46 Checking stale resources
    DEBUG: 2019/11/27 01:38:46 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:38:46 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:38:46 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:36.468 seconds][0m
Network.GatewayPing Daily N_Basic-1.15 N_Basic-1.16 Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:366[0m
  Ping gateway of a network from a pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:367[0m
    Ping gateway of a network(invalid vlan) from a pod
    [90m/gocode/main/test/e2e/tests/network-pod.go:399[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mRbac.Cluster Daily Rbac_Cluster-1.0 Rbac_Cluster-1.1[0m [90mrbac cluster test[0m 
  [1mcluster-edit test[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:276[0m
[BeforeEach] rbac cluster test
  /gocode/main/test/e2e/tests/rbac.go:215
    DEBUG: 2019/11/27 01:38:47 START_TEST Rbac.Cluster
    DEBUG: 2019/11/27 01:38:47 Login to cluster
    DEBUG: 2019/11/27 01:38:47 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:38:47 Updating inventory struct
    DEBUG: 2019/11/27 01:38:48 Checking stale resources
    DEBUG: 2019/11/27 01:38:48 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:38:48 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:38:48 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:38:56 Creating storage classes
[It] cluster-edit test
  /gocode/main/test/e2e/tests/rbac.go:276
    DEBUG: 2019/11/27 01:39:05 Create group
    DEBUG: 2019/11/27 01:39:05 Create user
    DEBUG: 2019/11/27 01:39:05 Login as user
    DEBUG: 2019/11/27 01:39:06 Create Network
    DEBUG: 2019/11/27 01:39:06 List Networks
    DEBUG: 2019/11/27 01:39:06 List users
    DEBUG: 2019/11/27 01:39:06 List groups
    DEBUG: 2019/11/27 01:39:06 List roles
    DEBUG: 2019/11/27 01:39:06 List auth-server
    DEBUG: 2019/11/27 01:39:06 Create user
    DEBUG: 2019/11/27 01:39:06 Create group
    DEBUG: 2019/11/27 01:39:06 Create role
    DEBUG: 2019/11/27 01:39:06 Create auth-server
[AfterEach] rbac cluster test
  /gocode/main/test/e2e/tests/rbac.go:222
    DEBUG: 2019/11/27 01:39:08 END_TEST Rbac.Cluster Time-taken : 21.105860984
    DEBUG: 2019/11/27 01:39:08 Checking stale resources
    DEBUG: 2019/11/27 01:39:08 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:39:08 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:39:08 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:21.273 seconds][0m
Rbac.Cluster Daily Rbac_Cluster-1.0 Rbac_Cluster-1.1
[90m/gocode/main/test/e2e/tests/rbac.go:207[0m
  rbac cluster test
  [90m/gocode/main/test/e2e/tests/rbac.go:210[0m
    cluster-edit test
    [90m/gocode/main/test/e2e/tests/rbac.go:276[0m
[90m------------------------------[0m
[0mHostNetwork.Basic Daily N_HostNetwork-1.0 N_HostNetwork-1.2 N_HostNetwork-3.0 N_HostNetwork-1.4 N_HostNetwork-1.6 N_HostNetwork-2.0 N_HostNetwork-2.2[0m [90mCreate single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted[0m 
  [1mShould create multiple host-networks, check if endpoints got created, delete host network, check if endpoints deleted[0m
  [37m/gocode/main/test/e2e/tests/network.go:536[0m
[BeforeEach] Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  /gocode/main/test/e2e/tests/network.go:525
    DEBUG: 2019/11/27 01:39:08 START_TEST HostNetwork.Basic
    DEBUG: 2019/11/27 01:39:08 Login to cluster
    DEBUG: 2019/11/27 01:39:08 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:39:08 Updating inventory struct
    DEBUG: 2019/11/27 01:39:09 Checking stale resources
    DEBUG: 2019/11/27 01:39:09 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:39:09 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:39:09 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:39:17 Creating storage classes
[It] Should create multiple host-networks, check if endpoints got created, delete host network, check if endpoints deleted
  /gocode/main/test/e2e/tests/network.go:536
    DEBUG: 2019/11/27 01:39:26 Creating perf-tier : template
    DEBUG: 2019/11/27 01:39:26 Disabling helm feature
    DEBUG: 2019/11/27 01:39:27 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:39:30 Updating non default network as host network
    DEBUG: 2019/11/27 01:39:31 Vnic allocation for host-network interface is successful
    DEBUG: 2019/11/27 01:39:31 Waiting for allocation of VF to endpoints of network : blue
    DEBUG: 2019/11/27 01:39:32 VF enp129s2f6d2 allocated to endpoint blue.appserv53 successfully
    DEBUG: 2019/11/27 01:39:32 VF enp129s6f2d2 allocated to endpoint blue.appserv54 successfully
    DEBUG: 2019/11/27 01:39:32 VF enp129s3f1 allocated to endpoint blue.appserv55 successfully
    DEBUG: 2019/11/27 01:39:32 Updating default network as host network
    DEBUG: 2019/11/27 01:39:32 Vnic allocation for host-network interface is successful
    DEBUG: 2019/11/27 01:39:32 Waiting for allocation of VF to endpoints of network : default
    DEBUG: 2019/11/27 01:39:33 VF enp129s2 allocated to endpoint default.appserv53 successfully
    DEBUG: 2019/11/27 01:39:33 VF enp129s3 allocated to endpoint default.appserv54 successfully
    DEBUG: 2019/11/27 01:39:33 VF enp129s8f6d2 allocated to endpoint default.appserv55 successfully
    DEBUG: 2019/11/27 01:39:33 Checking enpoint are provisioned and attached or not for hostnetwork : blue
    DEBUG: 2019/11/27 01:39:34 Creating 1 pairs of iperf client-server pods with high QoS
    DEBUG: 2019/11/27 01:39:34 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/27 01:39:37 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/27 01:40:07 Creating iperf Client pod: iperf-client-1
    DEBUG: 2019/11/27 01:40:10 Getting host-network interface of a cluster node where client pods scheduled
    DEBUG: 2019/11/27 01:40:10 tx_bytes on host-network interface enp129s6f2d2 when data transfer starts is : 1745399281 bytes
    DEBUG: 2019/11/27 01:40:41 tx_bytes on host-network interface enp129s6f2d2 after waiting time is : 34017067965 bytes
    DEBUG: 2019/11/27 01:40:41 Data is going through host-network interface. Byte direction :  tx_bytes
    DEBUG: 2019/11/27 01:40:41 Deleting pods:
    DEBUG: 2019/11/27 01:40:41 Deleting pods : 
    DEBUG: 2019/11/27 01:40:45 Deleting services: 
    DEBUG: 2019/11/27 01:40:45 Deleting service(s)
    DEBUG: 2019/11/27 01:40:45 Checking enpoint are provisioned and attached or not for hostnetwork : default
    DEBUG: 2019/11/27 01:40:46 Creating 1 pairs of iperf client-server pods with high QoS
    DEBUG: 2019/11/27 01:40:46 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/27 01:40:49 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/27 01:41:19 Creating iperf Client pod: iperf-client-1
    DEBUG: 2019/11/27 01:41:21 Getting host-network interface of a cluster node where client pods scheduled
    DEBUG: 2019/11/27 01:41:22 tx_bytes on host-network interface enp129s3 when data transfer starts is : 616466876 bytes
    DEBUG: 2019/11/27 01:41:52 tx_bytes on host-network interface enp129s3 after waiting time is : 36152254806 bytes
    DEBUG: 2019/11/27 01:41:52 Data is going through host-network interface. Byte direction :  tx_bytes
    DEBUG: 2019/11/27 01:41:52 Deleting pods:
    DEBUG: 2019/11/27 01:41:52 Deleting pods : 
    DEBUG: 2019/11/27 01:42:06 Deleting services: 
    DEBUG: 2019/11/27 01:42:06 Deleting service(s)
    DEBUG: 2019/11/27 01:42:07 Deleting host-network blue
    DEBUG: 2019/11/27 01:42:10 Endpoint blue.appserv53 deleted successfully
    DEBUG: 2019/11/27 01:42:10 Endpoint blue.appserv54 deleted successfully
    DEBUG: 2019/11/27 01:42:10 Endpoint blue.appserv55 deleted successfully
    DEBUG: 2019/11/27 01:42:10 Endpoints created by host-network got deleted
    DEBUG: 2019/11/27 01:42:10 Deleting host-network default
    DEBUG: 2019/11/27 01:42:12 Endpoint default.appserv53 deleted successfully
    DEBUG: 2019/11/27 01:42:12 Endpoint default.appserv54 deleted successfully
    DEBUG: 2019/11/27 01:42:12 Endpoint default.appserv55 deleted successfully
    DEBUG: 2019/11/27 01:42:12 Endpoints created by host-network got deleted
    DEBUG: 2019/11/27 01:42:12 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:42:13 VNICS usage is : 0
    DEBUG: 2019/11/27 01:42:13 After deleting host networks, vnics released by host-network endpoints
    DEBUG: 2019/11/27 01:42:13 Creating networks deleted by this TC
    DEBUG: 2019/11/27 01:42:13 Deleting the perf-tier : template
    DEBUG: 2019/11/27 01:42:13 Enabling helm feature.
[AfterEach] Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  /gocode/main/test/e2e/tests/network.go:531
    DEBUG: 2019/11/27 01:42:23 END_TEST HostNetwork.Basic Time-taken : 195.3568867
    DEBUG: 2019/11/27 01:42:23 Checking stale resources
    DEBUG: 2019/11/27 01:42:23 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:42:23 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:42:23 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:195.516 seconds][0m
HostNetwork.Basic Daily N_HostNetwork-1.0 N_HostNetwork-1.2 N_HostNetwork-3.0 N_HostNetwork-1.4 N_HostNetwork-1.6 N_HostNetwork-2.0 N_HostNetwork-2.2
[90m/gocode/main/test/e2e/tests/network.go:516[0m
  Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  [90m/gocode/main/test/e2e/tests/network.go:517[0m
    Should create multiple host-networks, check if endpoints got created, delete host network, check if endpoints deleted
    [90m/gocode/main/test/e2e/tests/network.go:536[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14[0m [90mCheck if a pod's IP is pingable from other pod[0m 
  [1mCheck if a pod's IP is pingable from other pod. Both pods are on different networks (public and valid)[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:923[0m
[BeforeEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:848
    DEBUG: 2019/11/27 01:42:23 START_TEST Network.PingBetweenTwoPods
    DEBUG: 2019/11/27 01:42:23 Login to cluster
    DEBUG: 2019/11/27 01:42:24 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:42:24 Updating inventory struct
    DEBUG: 2019/11/27 01:42:24 Checking stale resources
    DEBUG: 2019/11/27 01:42:25 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:42:25 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:42:25 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:42:33 Creating storage classes
[It] Check if a pod's IP is pingable from other pod. Both pods are on different networks (public and valid)
  /gocode/main/test/e2e/tests/network-pod.go:923
    DEBUG: 2019/11/27 01:42:42 Creating 1 pods of docker.io/redis:3.0.5 image with network : default and qos : high
    DEBUG: 2019/11/27 01:42:44 Creating 1 pods of docker.io/redis:3.0.5 image with network : blue and qos : high
    DEBUG: 2019/11/27 01:42:47 IP address ( 172.16.179.6 ) of e2etest-pod-default1 is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/27 01:42:48 IP address ( 172.16.180.4 ) of e2etest-pod-blue1 is between 172.16.180.4 and 172.16.180.253

    DEBUG: 2019/11/27 01:42:48 Trying to ping the e2etest-pod-blue1's IP from pod e2etest-pod-default1
    DEBUG: 2019/11/27 01:42:48 172.16.180.4 is pingable from pod e2etest-pod-default1 (172.16.179.6)
    DEBUG: 2019/11/27 01:42:48 Deleting pods : 
[AfterEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:857
    DEBUG: 2019/11/27 01:43:03 END_TEST Network.PingBetweenTwoPods Time-taken : 39.969071315
    DEBUG: 2019/11/27 01:43:03 Checking stale resources
    DEBUG: 2019/11/27 01:43:03 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:43:03 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:43:03 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:40.143 seconds][0m
Network.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14
[90m/gocode/main/test/e2e/tests/network-pod.go:842[0m
  Check if a pod's IP is pingable from other pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:843[0m
    Check if a pod's IP is pingable from other pod. Both pods are on different networks (public and valid)
    [90m/gocode/main/test/e2e/tests/network-pod.go:923[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mEndpoint.Basic Daily N_Endpoint-1.4 N_Endpoint-1.5[0m [90mEndpoint Basic testcases[0m 
  [1mCreate a endpoint[0m
  [37m/gocode/main/test/e2e/tests/endpoint.go:46[0m
[BeforeEach] Endpoint Basic testcases
  /gocode/main/test/e2e/tests/endpoint.go:31
    DEBUG: 2019/11/27 01:43:03 Login to cluster
    DEBUG: 2019/11/27 01:43:04 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:43:04 Updating inventory struct
    DEBUG: 2019/11/27 01:43:05 Checking stale resources
    DEBUG: 2019/11/27 01:43:05 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:43:05 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:43:05 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:43:13 Creating storage classes
    DEBUG: 2019/11/27 01:43:21 START_TEST Endpoint.Basic
[It] Create a endpoint
  /gocode/main/test/e2e/tests/endpoint.go:46
    DEBUG: 2019/11/27 01:43:21 Try to create a endpoint.
    DEBUG: 2019/11/27 01:43:21 Delete the endpoint.
[AfterEach] Endpoint Basic testcases
  /gocode/main/test/e2e/tests/endpoint.go:41
    DEBUG: 2019/11/27 01:43:21 END_TEST Endpoint.Basic Time-taken : 0.204603581
    DEBUG: 2019/11/27 01:43:21 Checking stale resources
    DEBUG: 2019/11/27 01:43:22 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:43:22 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:43:22 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:18.152 seconds][0m
Endpoint.Basic Daily N_Endpoint-1.4 N_Endpoint-1.5
[90m/gocode/main/test/e2e/tests/endpoint.go:23[0m
  Endpoint Basic testcases
  [90m/gocode/main/test/e2e/tests/endpoint.go:30[0m
    Create a endpoint
    [90m/gocode/main/test/e2e/tests/endpoint.go:46[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mMirroring.DetachOnePlexBeforeIOAttachAfterIO Daily SM_PlexDetach-1.0[0m [90mdetaching one plex before IO & attaching it after IO. Verifying resync is successful[0m 
  [1mdetaches one plex before IO & attaches it after IO. Verifying resync is successful[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:1144[0m
[BeforeEach] detaching one plex before IO & attaching it after IO. Verifying resync is successful
  /gocode/main/test/e2e/tests/mirroring.go:1130
    DEBUG: 2019/11/27 01:43:22 START_TEST Mirroring.DetachOnePlexBeforeIOAttachAfterIO
    DEBUG: 2019/11/27 01:43:22 Login to cluster
    DEBUG: 2019/11/27 01:43:22 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:43:22 Updating inventory struct
    DEBUG: 2019/11/27 01:43:23 Checking stale resources
    DEBUG: 2019/11/27 01:43:23 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:43:23 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:43:23 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:43:31 Creating storage classes
[It] detaches one plex before IO & attaches it after IO. Verifying resync is successful
  /gocode/main/test/e2e/tests/mirroring.go:1144
    DEBUG: 2019/11/27 01:43:39 Verifying whether FBM and L1 usage is zero across all nodes
    DEBUG: 2019/11/27 01:43:46 FBM and L1 usage is Zero across all nodes

    DEBUG: 2019/11/27 01:43:46 Assigning label to nodes where the plexes of mirrored volumes should get scheduled
    DEBUG: 2019/11/27 01:43:46 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 01:43:46 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 01:43:46 Creating 8 volumes of random sizes
    DEBUG: 2019/11/27 01:43:46 Mirror Count: 2
    DEBUG: 2019/11/27 01:43:50 Attaching all 8 volumes to node appserv55
    DEBUG: 2019/11/27 01:44:27 Detaching first non initiator plex from all the volumes
    DEBUG: 2019/11/27 01:44:38 Running WRITE fio job on: appserv55. FIO Command : echo -e '#!/bin/bash\n\nsudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"wxyz\"-12 --verify_interval=4096 --runtime=200 --blocksize=512K --iodepth=8  --time_based  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 > /tmp/fio_result.txt & > /dev/null 2>&1' > /tmp/fio_run.sh

    DEBUG: 2019/11/27 01:44:41 Number of running fio process: 11

    DEBUG: 2019/11/27 01:48:27 Reattaching first non initiator plex to all the volumes
    DEBUG: 2019/11/27 01:48:28 Verifying if Resync of plexes has started
    DEBUG: 2019/11/27 01:48:28 Number of volumes : 8
    DEBUG: 2019/11/27 01:48:28 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/27 01:48:28 Volume name & Plex : test-vol8.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 01:48:29 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 01:48:30 Volume: test-vol8. Resync offset: 8

    DEBUG: 2019/11/27 01:48:30 Volume: test-vol8. Resync offset: 11

    DEBUG: 2019/11/27 01:48:30 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:30 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/27 01:48:31 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:31 Volume name & Plex : test-vol4.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 01:48:32 Volume "test-vol4" has index "3" in embedded.
    DEBUG: 2019/11/27 01:48:33 Volume: test-vol4. Resync offset: 37

    DEBUG: 2019/11/27 01:48:33 Volume: test-vol4. Resync offset: 42

    DEBUG: 2019/11/27 01:48:33 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 01:48:33 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:33 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:33 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 01:48:33 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 01:48:33 Volume name & Plex : test-vol2.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 01:48:35 Volume "test-vol2" has index "1" in embedded.
    DEBUG: 2019/11/27 01:48:35 Volume: test-vol2. Resync offset: 96

    DEBUG: 2019/11/27 01:48:36 Volume: test-vol2. Resync offset: 
    DEBUG: 2019/11/27 01:48:37 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:37 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 01:48:37 Volume name & Plex : test-vol3.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 01:48:38 Volume "test-vol3" has index "2" in embedded.
    DEBUG: 2019/11/27 01:48:39 Volume: test-vol3. Resync offset: 
    DEBUG: 2019/11/27 01:48:40 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:40 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/27 01:48:40 Volume name & Plex : test-vol6.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 01:48:41 Volume "test-vol6" has index "5" in embedded.
    DEBUG: 2019/11/27 01:48:42 Volume: test-vol6. Resync offset: 79

    DEBUG: 2019/11/27 01:48:42 Volume: test-vol6. Resync offset: 83

    DEBUG: 2019/11/27 01:48:42 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:42 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/27 01:48:42 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:42 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:42 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/27 01:48:42 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/27 01:48:43 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:43 Volume name & Plex : test-vol7.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 01:48:44 Volume "test-vol7" has index "6" in embedded.
    DEBUG: 2019/11/27 01:48:44 Volume: test-vol7. Resync offset: 88

    DEBUG: 2019/11/27 01:48:45 Volume: test-vol7. Resync offset: 93

    DEBUG: 2019/11/27 01:48:45 Comparing Volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/27 01:48:50 Waiting for fio job to complete on the node
    DEBUG: 2019/11/27 01:48:50 Verifying if Resync of plexes has completed
    DEBUG: 2019/11/27 01:48:50 Number of volumes : 8
    DEBUG: 2019/11/27 01:48:50 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/27 01:48:50 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:50 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:50 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/27 01:48:50 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/27 01:48:50 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:50 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:50 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/27 01:48:50 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 01:48:50 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:50 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:50 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 01:48:50 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 01:48:51 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:51 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:51 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 01:48:51 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 01:48:51 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:51 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:51 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/27 01:48:51 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/27 01:48:51 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:51 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:51 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/27 01:48:51 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/27 01:48:51 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:51 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:51 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/27 01:48:51 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/27 01:48:51 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 01:48:51 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/27 01:48:51 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/27 01:48:53 Verifying sha512sum across all plexes
    DEBUG: 2019/11/27 01:48:55 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 01:48:56 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 01:48:57 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 01:48:59 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 01:49:00 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 01:49:01 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 01:49:03 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 01:49:04 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 01:49:05 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 01:49:05 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 01:49:05 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 01:49:05 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 01:49:05 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 01:49:05 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 01:49:05 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 01:49:06 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 01:49:20 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 01:49:22 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 01:49:23 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 01:49:24 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 01:49:26 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 01:49:27 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 01:49:28 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 01:49:30 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 01:49:31 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 01:49:31 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 01:49:31 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 01:49:31 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 01:49:31 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 01:49:31 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 01:49:31 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 01:49:31 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 01:49:44 Sha512sum matched for all volumes across all plexes
    DEBUG: 2019/11/27 01:49:44 Detach & Delete all volumes
    DEBUG: 2019/11/27 01:50:20 Removing label from the nodes where the plexes of mirrored volumes were scheduled
    DEBUG: 2019/11/27 01:50:20 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 01:50:20 Removed label : mirror from node : appserv54
[AfterEach] detaching one plex before IO & attaching it after IO. Verifying resync is successful
  /gocode/main/test/e2e/tests/mirroring.go:1139
    DEBUG: 2019/11/27 01:50:20 END_TEST Mirroring.DetachOnePlexBeforeIOAttachAfterIO Time-taken : 418.52169428
    DEBUG: 2019/11/27 01:50:20 Checking stale resources
    DEBUG: 2019/11/27 01:50:20 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:50:20 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:50:20 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:418.672 seconds][0m
Mirroring.DetachOnePlexBeforeIOAttachAfterIO Daily SM_PlexDetach-1.0
[90m/gocode/main/test/e2e/tests/mirroring.go:1123[0m
  detaching one plex before IO & attaching it after IO. Verifying resync is successful
  [90m/gocode/main/test/e2e/tests/mirroring.go:1125[0m
    detaches one plex before IO & attaches it after IO. Verifying resync is successful
    [90m/gocode/main/test/e2e/tests/mirroring.go:1144[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mCluster.FailedNodes Management Daily M_Cluster-1.20[0m [90mwhen a node fails in a cluster created with all nodes[0m 
  [1mshould be created and destroyed[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:223[0m
[BeforeEach] when a node fails in a cluster created with all nodes
  /gocode/main/test/e2e/tests/cluster.go:209
    DEBUG: 2019/11/27 01:50:20 START_TEST Cluster.FailedNodes
[AfterEach] when a node fails in a cluster created with all nodes
  /gocode/main/test/e2e/tests/cluster.go:218
    DEBUG: 2019/11/27 01:50:20 END_TEST Cluster.FailedNodes Time-taken : 0.001017484
    DEBUG: 2019/11/27 01:50:20 Checking stale resources
    DEBUG: 2019/11/27 01:50:20 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:50:20 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:50:20 Checking stale resources on the node: appserv55

[36m[1mS [SKIPPING] in Spec Setup (BeforeEach) [0.143 seconds][0m
Cluster.FailedNodes Management Daily M_Cluster-1.20
[90m/gocode/main/test/e2e/tests/cluster.go:202[0m
  when a node fails in a cluster created with all nodes
  [90m/gocode/main/test/e2e/tests/cluster.go:204[0m
    [36m[1mshould be created and destroyed [BeforeEach][0m
    [90m/gocode/main/test/e2e/tests/cluster.go:223[0m

    [36mSkipping for smaller testbeds[0m

    /gocode/main/test/e2e/tests/cluster.go:213
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14[0m [90mCheck if a pod's IP is pingable from other pod[0m 
  [1mCheck if a pod's endpoint is pingable from other pod using public network. Pods are created using endpoints.[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:881[0m
[BeforeEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:848
    DEBUG: 2019/11/27 01:50:20 START_TEST Network.PingBetweenTwoPods
    DEBUG: 2019/11/27 01:50:20 Login to cluster
    DEBUG: 2019/11/27 01:50:21 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:50:21 Updating inventory struct
    DEBUG: 2019/11/27 01:50:22 Checking stale resources
    DEBUG: 2019/11/27 01:50:22 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:50:22 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:50:22 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:50:30 Creating storage classes
[It] Check if a pod's endpoint is pingable from other pod using public network. Pods are created using endpoints.
  /gocode/main/test/e2e/tests/network-pod.go:881
    DEBUG: 2019/11/27 01:50:39 Creating two endpoints : test-ep1, test-ep2 
    DEBUG: 2019/11/27 01:50:40 Creating 2 pod(s) of docker.io/redis:3.0.5 image with provided endpoint(s)
    DEBUG: 2019/11/27 01:50:45 IP address ( 172.16.179.6 ) of eptest-pod-1 is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/27 01:50:45 IP address ( 172.16.179.7 ) of eptest-pod-2 is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/27 01:50:45 Trying to ping the eptest-pod-2's IP from pod eptest-pod-1
    DEBUG: 2019/11/27 01:50:46 172.16.179.7 is pingable from pod eptest-pod-1 (172.16.179.6)
    DEBUG: 2019/11/27 01:50:46 Deleting pods : 
    DEBUG: 2019/11/27 01:50:53 Deleting endpoints : test-ep1, test-ep2 
[AfterEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:857
    DEBUG: 2019/11/27 01:50:53 END_TEST Network.PingBetweenTwoPods Time-taken : 33.096840204
    DEBUG: 2019/11/27 01:50:53 Checking stale resources
    DEBUG: 2019/11/27 01:50:54 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:50:54 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:50:54 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:33.252 seconds][0m
Network.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14
[90m/gocode/main/test/e2e/tests/network-pod.go:842[0m
  Check if a pod's IP is pingable from other pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:843[0m
    Check if a pod's endpoint is pingable from other pod using public network. Pods are created using endpoints.
    [90m/gocode/main/test/e2e/tests/network-pod.go:881[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mLocalStorage.LargeSizeVolumesRebootTestWithoutVerificationShort Daily S_Reboot-2.7[0m [90mStorage volume reboot test with large local volumes[0m 
  [1mreboot test on large volumes[0m
  [37m/gocode/main/test/e2e/tests/volume.go:1717[0m
[BeforeEach] Storage volume reboot test with large local volumes
  /gocode/main/test/e2e/tests/volume.go:1706
    DEBUG: 2019/11/27 01:50:54 START_TEST LocalStorage.LargeSizeVolumesRebootTestWithoutVerificationShort
    DEBUG: 2019/11/27 01:50:54 Login to cluster
    DEBUG: 2019/11/27 01:50:54 Checking basic Vnic usage
    DEBUG: 2019/11/27 01:50:54 Updating inventory struct
    DEBUG: 2019/11/27 01:50:55 Checking stale resources
    DEBUG: 2019/11/27 01:50:55 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 01:50:55 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 01:50:55 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 01:51:03 Creating storage classes
[It] reboot test on large volumes
  /gocode/main/test/e2e/tests/volume.go:1717
    DEBUG: 2019/11/27 01:51:12 Computing total available storage space on node appserv53
    DEBUG: 2019/11/27 01:51:12 Create volumes until vol-size is >  max free space on appserv53 or maxvols created
    DEBUG: 2019/11/27 01:51:13 Creating last volume of size = available storage space on the node or max allowed size; whichever is lower
    DEBUG: 2019/11/27 01:51:13 Attach all 5 volumes
    DEBUG: 2019/11/27 01:51:36 Computing total available storage space on node appserv54
    DEBUG: 2019/11/27 01:51:36 Create volumes until vol-size is >  max free space on appserv54 or maxvols created
    DEBUG: 2019/11/27 01:51:36 Creating last volume of size = available storage space on the node or max allowed size; whichever is lower
    DEBUG: 2019/11/27 01:51:36 Attach all 5 volumes
    DEBUG: 2019/11/27 01:51:58 Computing total available storage space on node appserv55
    DEBUG: 2019/11/27 01:51:58 Create volumes until vol-size is >  max free space on appserv55 or maxvols created
    DEBUG: 2019/11/27 01:51:58 Creating last volume of size = available storage space on the node or max allowed size; whichever is lower
    DEBUG: 2019/11/27 01:51:59 Attach all 5 volumes
    DEBUG: 2019/11/27 01:52:21 Total number of volumes: 15

    DEBUG: 2019/11/27 01:52:21 Running iteration no : 1
    DEBUG: 2019/11/27 01:52:21 Running WRITE IOs on node : appserv55

    DEBUG: 2019/11/27 01:52:21 Running WRITE IOs on node : appserv53

    DEBUG: 2019/11/27 01:52:21 Running WRITE IOs on node : appserv54

    DEBUG: 2019/11/27 01:52:21 Running WRITE fio job on: appserv55. FIO Command : echo -e '#!/bin/bash\n\nsudo /usr/local/bin/fio --ioengine=libaio --rw=randwrite --iodepth=16 --numjobs=16 --gtod_reduce=1 --group_reporting --time_based --runtime=1000 --blocksize=4K  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 > /tmp/fio_result.txt & > /dev/null 2>&1' > /tmp/fio_run.sh

    DEBUG: 2019/11/27 01:52:21 Running WRITE fio job on: appserv53. FIO Command : echo -e '#!/bin/bash\n\nsudo /usr/local/bin/fio --ioengine=libaio --rw=randwrite --iodepth=16 --numjobs=16 --gtod_reduce=1 --group_reporting --time_based --runtime=1000 --blocksize=4K  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 > /tmp/fio_result.txt & > /dev/null 2>&1' > /tmp/fio_run.sh

    DEBUG: 2019/11/27 01:52:21 Running WRITE fio job on: appserv54. FIO Command : echo -e '#!/bin/bash\n\nsudo /usr/local/bin/fio --ioengine=libaio --rw=randwrite --iodepth=16 --numjobs=16 --gtod_reduce=1 --group_reporting --time_based --runtime=1000 --blocksize=4K  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 > /tmp/fio_result.txt & > /dev/null 2>&1' > /tmp/fio_run.sh

    DEBUG: 2019/11/27 01:57:57 Getting cluster quorum nodes
    DEBUG: 2019/11/27 01:57:57 Powering OFF the node appserv53
    DEBUG: 2019/11/27 01:57:57 Node 172.16.6.153 took 0 seconds to power off
    DEBUG: 2019/11/27 01:57:57 Powering OFF the node appserv54
    DEBUG: 2019/11/27 01:57:58 Node 172.16.6.154 took 0 seconds to power off
    DEBUG: 2019/11/27 01:57:58 Powering OFF the node appserv55
    DEBUG: 2019/11/27 01:57:58 Node 172.16.6.155 took 0 seconds to power off
    DEBUG: 2019/11/27 01:57:58 Ensuring that appserv53 node is unreachable: 
    DEBUG: 2019/11/27 01:57:58 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 01:58:07 Ensuring that appserv54 node is unreachable: 
    DEBUG: 2019/11/27 01:58:07 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 01:58:16 Ensuring that appserv55 node is unreachable: 
    DEBUG: 2019/11/27 01:58:16 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 02:00:25 Powering ON the node appserv53
    DEBUG: 2019/11/27 02:00:25 Node 172.16.6.153 took 0 seconds to power on
    DEBUG: 2019/11/27 02:00:25 Powering ON the node appserv54
    DEBUG: 2019/11/27 02:00:25 Node 172.16.6.154 took 0 seconds to power on
    DEBUG: 2019/11/27 02:00:25 Powering ON the node appserv55
    DEBUG: 2019/11/27 02:00:25 Node 172.16.6.155 took 0 seconds to power on
    DEBUG: 2019/11/27 02:00:25 Checking if node appserv53 is reachable or not: 
    DEBUG: 2019/11/27 02:00:25 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:00:42 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:00:59 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:01:16 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:01:33 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:01:50 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:02:07 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:02:24 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:02:41 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:02:45 appserv53 is pingable from local machine
    DEBUG: 2019/11/27 02:02:45 Checking ssh port is up or not on node: appserv53
    DEBUG: 2019/11/27 02:02:45 Checking if node appserv54 is reachable or not: 
    DEBUG: 2019/11/27 02:02:45 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 02:02:49 appserv54 is pingable from local machine
    DEBUG: 2019/11/27 02:02:49 Checking ssh port is up or not on node: appserv54
    DEBUG: 2019/11/27 02:02:49 Checking if node appserv55 is reachable or not: 
    DEBUG: 2019/11/27 02:02:49 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 02:02:53 appserv55 is pingable from local machine
    DEBUG: 2019/11/27 02:02:53 Checking ssh port is up or not on node: appserv55
    DEBUG: 2019/11/27 02:03:23 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/27 02:03:26 Error: . Retrying once again...
    DEBUG: 2019/11/27 02:03:34 Found '3' nodes
    DEBUG: 2019/11/27 02:03:34 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 02:04:00 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 02:04:00 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 02:04:12 After power cycle/reboot, updating timestamp of node : appserv53
    DEBUG: 2019/11/27 02:04:15 After power cycle/reboot, updating timestamp of node : appserv54
    DEBUG: 2019/11/27 02:04:18 After power cycle/reboot, updating timestamp of node : appserv55
    DEBUG: 2019/11/27 02:04:20 Getting cluster quorum nodes
    DEBUG: 2019/11/27 02:04:20 Updating timestamp of master, because cluster was not in quorum majority
    DEBUG: 2019/11/27 02:05:24 Updating inventory struct
    DEBUG: 2019/11/27 02:05:24 Waiting for nodes to come up, will wait upto 800 seconds
    DEBUG: 2019/11/27 02:05:36 Nodes are up, waiting for armada to start
.
    DEBUG: 2019/11/27 02:05:46 Waiting for all the nodes to go into Ready state
    DEBUG: 2019/11/27 02:05:46 Found '3' nodes
    DEBUG: 2019/11/27 02:05:46 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 02:05:46 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 02:05:47 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 02:05:47 Waiting for volumes to come into Attached state after rebooting all cluster nodes.
    DEBUG: 2019/11/27 02:05:47 Running iteration no : 2
    DEBUG: 2019/11/27 02:05:47 Running WRITE IOs on node : appserv55

    DEBUG: 2019/11/27 02:05:47 Running WRITE fio job on: appserv55. FIO Command : echo -e '#!/bin/bash\n\nsudo /usr/local/bin/fio --ioengine=libaio --rw=randwrite --iodepth=16 --numjobs=16 --gtod_reduce=1 --group_reporting --time_based --runtime=1000 --blocksize=4K  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 > /tmp/fio_result.txt & > /dev/null 2>&1' > /tmp/fio_run.sh

    DEBUG: 2019/11/27 02:05:47 Running WRITE IOs on node : appserv53

    DEBUG: 2019/11/27 02:05:47 Running WRITE IOs on node : appserv54

    DEBUG: 2019/11/27 02:05:47 Running WRITE fio job on: appserv53. FIO Command : echo -e '#!/bin/bash\n\nsudo /usr/local/bin/fio --ioengine=libaio --rw=randwrite --iodepth=16 --numjobs=16 --gtod_reduce=1 --group_reporting --time_based --runtime=1000 --blocksize=4K  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 > /tmp/fio_result.txt & > /dev/null 2>&1' > /tmp/fio_run.sh

    DEBUG: 2019/11/27 02:05:47 Running WRITE fio job on: appserv54. FIO Command : echo -e '#!/bin/bash\n\nsudo /usr/local/bin/fio --ioengine=libaio --rw=randwrite --iodepth=16 --numjobs=16 --gtod_reduce=1 --group_reporting --time_based --runtime=1000 --blocksize=4K  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 > /tmp/fio_result.txt & > /dev/null 2>&1' > /tmp/fio_run.sh

    DEBUG: 2019/11/27 02:11:23 Getting cluster quorum nodes
    DEBUG: 2019/11/27 02:11:23 Powering OFF the node appserv53
    DEBUG: 2019/11/27 02:11:23 Node 172.16.6.153 took 0 seconds to power off
    DEBUG: 2019/11/27 02:11:23 Powering OFF the node appserv54
    DEBUG: 2019/11/27 02:11:23 Node 172.16.6.154 took 0 seconds to power off
    DEBUG: 2019/11/27 02:11:23 Powering OFF the node appserv55
    DEBUG: 2019/11/27 02:11:23 Node 172.16.6.155 took 0 seconds to power off
    DEBUG: 2019/11/27 02:11:23 Ensuring that appserv53 node is unreachable: 
    DEBUG: 2019/11/27 02:11:23 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:11:32 Ensuring that appserv54 node is unreachable: 
    DEBUG: 2019/11/27 02:11:32 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 02:11:41 Ensuring that appserv55 node is unreachable: 
    DEBUG: 2019/11/27 02:11:41 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 02:13:50 Powering ON the node appserv53
    DEBUG: 2019/11/27 02:13:52 Node 172.16.6.153 took 1 seconds to power on
    DEBUG: 2019/11/27 02:13:52 Powering ON the node appserv54
    DEBUG: 2019/11/27 02:13:53 Node 172.16.6.154 took 1 seconds to power on
    DEBUG: 2019/11/27 02:13:53 Powering ON the node appserv55
    DEBUG: 2019/11/27 02:13:54 Node 172.16.6.155 took 1 seconds to power on
    DEBUG: 2019/11/27 02:13:54 Checking if node appserv53 is reachable or not: 
    DEBUG: 2019/11/27 02:13:54 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:14:11 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:14:28 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:14:45 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:15:02 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:15:19 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:15:36 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:15:53 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 02:15:59 appserv53 is pingable from local machine
    DEBUG: 2019/11/27 02:15:59 Checking ssh port is up or not on node: appserv53
    DEBUG: 2019/11/27 02:16:09 Checking if node appserv54 is reachable or not: 
    DEBUG: 2019/11/27 02:16:09 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 02:16:13 appserv54 is pingable from local machine
    DEBUG: 2019/11/27 02:16:13 Checking ssh port is up or not on node: appserv54
    DEBUG: 2019/11/27 02:16:13 Checking if node appserv55 is reachable or not: 
    DEBUG: 2019/11/27 02:16:13 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 02:16:17 appserv55 is pingable from local machine
    DEBUG: 2019/11/27 02:16:17 Checking ssh port is up or not on node: appserv55
    DEBUG: 2019/11/27 02:16:47 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/27 02:16:50 Error: . Retrying once again...
    DEBUG: 2019/11/27 02:16:55 Error: . Retrying once again...
    DEBUG: 2019/11/27 02:17:01 Found '3' nodes
    DEBUG: 2019/11/27 02:17:01 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 02:17:17 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 02:17:24 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 02:17:36 After power cycle/reboot, updating timestamp of node : appserv53
    DEBUG: 2019/11/27 02:17:40 After power cycle/reboot, updating timestamp of node : appserv54
    DEBUG: 2019/11/27 02:17:42 After power cycle/reboot, updating timestamp of node : appserv55
    DEBUG: 2019/11/27 02:17:44 Getting cluster quorum nodes
    DEBUG: 2019/11/27 02:17:44 Updating timestamp of master, because cluster was not in quorum majority
    DEBUG: 2019/11/27 02:18:48 Updating inventory struct
    DEBUG: 2019/11/27 02:18:48 Waiting for nodes to come up, will wait upto 800 seconds
    DEBUG: 2019/11/27 02:19:01 Nodes are up, waiting for armada to start
.
    DEBUG: 2019/11/27 02:19:11 Waiting for all the nodes to go into Ready state
    DEBUG: 2019/11/27 02:19:11 Found '3' nodes
    DEBUG: 2019/11/27 02:19:11 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 02:19:11 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 02:19:11 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 02:19:11 Waiting for volumes to come into Attached state after rebooting all cluster nodes.
    DEBUG: 2019/11/27 02:19:11 Detach & Delete all volumes
[AfterEach] Storage volume reboot test with large local volumes
  /gocode/main/test/e2e/tests/volume.go:1712
    DEBUG: 2019/11/27 02:24:59 END_TEST LocalStorage.LargeSizeVolumesRebootTestWithoutVerificationShort Time-taken : 2044.906781181
    DEBUG: 2019/11/27 02:24:59 Checking stale resources
    DEBUG: 2019/11/27 02:24:59 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:24:59 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:24:59 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:2045.064 seconds][0m
LocalStorage.LargeSizeVolumesRebootTestWithoutVerificationShort Daily S_Reboot-2.7
[90m/gocode/main/test/e2e/tests/volume.go:1699[0m
  Storage volume reboot test with large local volumes
  [90m/gocode/main/test/e2e/tests/volume.go:1701[0m
    reboot test on large volumes
    [90m/gocode/main/test/e2e/tests/volume.go:1717[0m
[90m------------------------------[0m
[0mCluster.AddNodes Management Daily M_Cluster-1.3 M_Cluster-1.4[0m [90mwhen cluster is created with few nodes[0m 
  [1mshould be created and destroyed[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:168[0m
[BeforeEach] when cluster is created with few nodes
  /gocode/main/test/e2e/tests/cluster.go:144
    DEBUG: 2019/11/27 02:24:59 START_TEST Cluster.AddNodes
[AfterEach] when cluster is created with few nodes
  /gocode/main/test/e2e/tests/cluster.go:163
    DEBUG: 2019/11/27 02:24:59 END_TEST Cluster.AddNodes Time-taken : 0.000387826
    DEBUG: 2019/11/27 02:24:59 Checking stale resources
    DEBUG: 2019/11/27 02:24:59 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:24:59 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:24:59 Checking stale resources on the node: appserv55

[36m[1mS [SKIPPING] in Spec Setup (BeforeEach) [0.146 seconds][0m
Cluster.AddNodes Management Daily M_Cluster-1.3 M_Cluster-1.4
[90m/gocode/main/test/e2e/tests/cluster.go:136[0m
  when cluster is created with few nodes
  [90m/gocode/main/test/e2e/tests/cluster.go:138[0m
    [36m[1mshould be created and destroyed [BeforeEach][0m
    [90m/gocode/main/test/e2e/tests/cluster.go:168[0m

    [36mSkipping for smaller testbeds[0m

    /gocode/main/test/e2e/tests/cluster.go:151
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4[0m [90mPing pod's IP from outside world[0m 
  [1mPing pod's IP from outside world using private network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:721[0m
[BeforeEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:700
    DEBUG: 2019/11/27 02:24:59 START_TEST Network.PingPodFromOutside
    DEBUG: 2019/11/27 02:24:59 Login to cluster
    DEBUG: 2019/11/27 02:24:59 Checking basic Vnic usage
    DEBUG: 2019/11/27 02:24:59 Updating inventory struct
    DEBUG: 2019/11/27 02:25:00 Checking stale resources
    DEBUG: 2019/11/27 02:25:00 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 02:25:00 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:25:00 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:25:08 Creating storage classes
[It] Ping pod's IP from outside world using private network
  /gocode/main/test/e2e/tests/network-pod.go:721
    DEBUG: 2019/11/27 02:25:21 Creating private network : blue
    DEBUG: 2019/11/27 02:25:21 Creating 1 pods of docker.io/redis:3.0.5 image with network : blue and qos : high
    DEBUG: 2019/11/27 02:25:24 IP address ( 172.16.180.4 ) of e2etest-pod-1 is between 172.16.180.4 and 172.16.180.253

    DEBUG: 2019/11/27 02:25:24 Trying to ping the e2etest-pod-1
    DEBUG: 2019/11/27 02:25:24 Executing ping command: ping  -c 5 -W 5 172.16.180.4
    DEBUG: 2019/11/27 02:25:33 Deleting pods : 
    DEBUG: 2019/11/27 02:25:44 Deleting private network : blue
[AfterEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:709
    DEBUG: 2019/11/27 02:25:48 END_TEST Network.PingPodFromOutside Time-taken : 49.585210742
    DEBUG: 2019/11/27 02:25:48 Checking stale resources
    DEBUG: 2019/11/27 02:25:49 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:25:49 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:25:49 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:49.770 seconds][0m
Network.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4
[90m/gocode/main/test/e2e/tests/network-pod.go:694[0m
  Ping pod's IP from outside world
  [90m/gocode/main/test/e2e/tests/network-pod.go:695[0m
    Ping pod's IP from outside world using private network
    [90m/gocode/main/test/e2e/tests/network-pod.go:721[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries deleting QoS template with no name.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:247[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/27 02:25:49 Login to cluster
    DEBUG: 2019/11/27 02:25:49 Checking basic Vnic usage
    DEBUG: 2019/11/27 02:25:49 Updating inventory struct
    DEBUG: 2019/11/27 02:25:50 Checking stale resources
    DEBUG: 2019/11/27 02:25:50 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:25:50 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:25:50 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 02:25:58 Creating storage classes
    DEBUG: 2019/11/27 02:26:07 START_TEST PerfTier.NegativeTests
[It] tries deleting QoS template with no name.
  /gocode/main/test/e2e/tests/perf-tier.go:247
[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/27 02:26:07 END_TEST PerfTier.NegativeTests Time-taken: 0.024732832
    DEBUG: 2019/11/27 02:26:07 Checking stale resources
    DEBUG: 2019/11/27 02:26:07 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:26:07 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 02:26:07 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:18.788 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries deleting QoS template with no name.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:247[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.UniDirectionalQosValidationAcrossBothNics Daily AT_Qos-1.5 AT_Qos-1.6 Qos Multizone[0m [90mValidate Qos on two node with iperfpod[0m 
  [1mNetwork uni-directional port 0 with high QoS and port 2 with medium QoS, QoS should be honoured[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:3502[0m
[BeforeEach] Validate Qos on two node with iperfpod
  /gocode/main/test/e2e/tests/network-pod.go:3488
    DEBUG: 2019/11/27 02:26:07 START_TEST Network.UniDirectionalQosValidationAcrossBothNics
    DEBUG: 2019/11/27 02:26:07 Login to cluster
    DEBUG: 2019/11/27 02:26:08 Checking basic Vnic usage
    DEBUG: 2019/11/27 02:26:08 Updating inventory struct
    DEBUG: 2019/11/27 02:26:09 Checking stale resources
    DEBUG: 2019/11/27 02:26:09 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:26:09 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:26:09 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 02:26:17 Creating storage classes
[It] Network uni-directional port 0 with high QoS and port 2 with medium QoS, QoS should be honoured
  /gocode/main/test/e2e/tests/network-pod.go:3502
    DEBUG: 2019/11/27 02:26:26 Creating perf-tier : template
    DEBUG: 2019/11/27 02:26:26 Creating 2 pairs of iperf client-server pods with template QoS
    DEBUG: 2019/11/27 02:26:26 Creating iperf server pod: iperf-server-template1
    DEBUG: 2019/11/27 02:26:29 Creating service with name: iperf-server-template1
    DEBUG: 2019/11/27 02:26:29 Creating iperf server pod: iperf-server-template2
    DEBUG: 2019/11/27 02:26:32 Creating service with name: iperf-server-template2
    DEBUG: 2019/11/27 02:27:02 Creating iperf Client pod: iperf-client-template1
    DEBUG: 2019/11/27 02:27:05 Creating iperf Client pod: iperf-client-template2
    DEBUG: 2019/11/27 02:27:12 Deleting pods scheduled on nic 0 with template QoS
    DEBUG: 2019/11/27 02:27:12 Deleting pods : 
    DEBUG: 2019/11/27 02:27:28 Deleting pods : 
    DEBUG: 2019/11/27 02:27:32 Creating 4 pairs of iperf client-server pods with high QoS on nic 0
    DEBUG: 2019/11/27 02:27:32 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/27 02:27:48 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/27 02:27:48 Creating iperf server pod: iperf-server-high2
    DEBUG: 2019/11/27 02:27:59 Creating service with name: iperf-server-high2
    DEBUG: 2019/11/27 02:27:59 Creating iperf server pod: iperf-server-high3
    DEBUG: 2019/11/27 02:28:09 Creating service with name: iperf-server-high3
    DEBUG: 2019/11/27 02:28:09 Creating iperf server pod: iperf-server-high4
    DEBUG: 2019/11/27 02:28:22 Creating service with name: iperf-server-high4
    DEBUG: 2019/11/27 02:28:52 Creating iperf Client pod: iperf-client-high1
    DEBUG: 2019/11/27 02:28:55 Creating iperf Client pod: iperf-client-high2
    DEBUG: 2019/11/27 02:29:00 Creating iperf Client pod: iperf-client-high3
    DEBUG: 2019/11/27 02:29:04 Creating iperf Client pod: iperf-client-high4
    DEBUG: 2019/11/27 02:29:07 Getting all pods which are scheduled on nicId 0 
    DEBUG: 2019/11/27 02:29:07 Getting all pods which are scheduled on nicId 0 
    DEBUG: 2019/11/27 02:29:07 Deleting pods scheduled on nic 2 with template QoS
    DEBUG: 2019/11/27 02:29:07 Deleting pods : 
    DEBUG: 2019/11/27 02:29:18 Deleting pods : 
    DEBUG: 2019/11/27 02:29:24 Creating 4 pairs of iperf client-server pods with medium QoS on nic 2
    DEBUG: 2019/11/27 02:29:24 Creating iperf server pod: iperf-server-medium1
    DEBUG: 2019/11/27 02:29:28 Creating service with name: iperf-server-medium1
    DEBUG: 2019/11/27 02:29:28 Creating iperf server pod: iperf-server-medium2
    DEBUG: 2019/11/27 02:29:30 Creating service with name: iperf-server-medium2
    DEBUG: 2019/11/27 02:29:30 Creating iperf server pod: iperf-server-medium3
    DEBUG: 2019/11/27 02:29:34 Creating service with name: iperf-server-medium3
    DEBUG: 2019/11/27 02:29:34 Creating iperf server pod: iperf-server-medium4
    DEBUG: 2019/11/27 02:29:38 Creating service with name: iperf-server-medium4
    DEBUG: 2019/11/27 02:30:08 Creating iperf Client pod: iperf-client-medium1
    DEBUG: 2019/11/27 02:30:11 Creating iperf Client pod: iperf-client-medium2
    DEBUG: 2019/11/27 02:30:15 Creating iperf Client pod: iperf-client-medium3
    DEBUG: 2019/11/27 02:30:18 Creating iperf Client pod: iperf-client-medium4
    DEBUG: 2019/11/27 02:30:20 Getting all pods which are scheduled on nicId 2 
    DEBUG: 2019/11/27 02:30:20 Getting all pods which are scheduled on nicId 2 
    DEBUG: 2019/11/27 02:30:20 Scheduling of all pods on both nics is validated
    DEBUG: 2019/11/27 02:30:20 Sleeping for 180 seconds, so that prometheus database will have some stats.
    DEBUG: 2019/11/27 02:33:20 Validating if bandwidth is honored or not for server pods:
    DEBUG: 2019/11/27 02:33:21 QoS honored for pod: iperf-server-high1
    DEBUG: 2019/11/27 02:33:21 QoS honored for pod: iperf-server-high2
    DEBUG: 2019/11/27 02:33:21 QoS honored for pod: iperf-server-high3
    DEBUG: 2019/11/27 02:33:22 QoS honored for pod: iperf-server-high4
    DEBUG: 2019/11/27 02:33:22 QoS honored for pod: iperf-server-medium1
    DEBUG: 2019/11/27 02:33:23 QoS honored for pod: iperf-server-medium2
    DEBUG: 2019/11/27 02:33:23 QoS honored for pod: iperf-server-medium3
    DEBUG: 2019/11/27 02:33:24 QoS honored for pod: iperf-server-medium4
    DEBUG: 2019/11/27 02:33:24 Validating if bandwidth is honored or not for client pods:
    DEBUG: 2019/11/27 02:33:24 QoS honored for pod: iperf-client-high1
    DEBUG: 2019/11/27 02:33:24 QoS honored for pod: iperf-client-high2
    DEBUG: 2019/11/27 02:33:25 QoS honored for pod: iperf-client-high3
    DEBUG: 2019/11/27 02:33:25 QoS honored for pod: iperf-client-high4
    DEBUG: 2019/11/27 02:33:26 QoS honored for pod: iperf-client-medium1
    DEBUG: 2019/11/27 02:33:26 QoS honored for pod: iperf-client-medium2
    DEBUG: 2019/11/27 02:33:26 QoS honored for pod: iperf-client-medium3
    DEBUG: 2019/11/27 02:33:27 QoS honored for pod: iperf-client-medium4
    DEBUG: 2019/11/27 02:33:27 Measuring throughput. num of links used: 2
    DEBUG: 2019/11/27 02:33:27 Node: appserv53. Expected Throughput: 16200000000. RX Throughput: 18611937339. TX Throughput: 0
    DEBUG: 2019/11/27 02:33:27 Node: appserv54. Expected Throughput: 16200000000. RX Throughput: 0. TX Throughput: 18664120338
    DEBUG: 2019/11/27 02:33:27 Deleting pods :
    DEBUG: 2019/11/27 02:33:27 Deleting pods : 
    DEBUG: 2019/11/27 02:33:51 Deleting pods : 
    DEBUG: 2019/11/27 02:34:19 Deleting perf-tier : template
    DEBUG: 2019/11/27 02:34:19 Deleting services : 
    DEBUG: 2019/11/27 02:34:19 Deleting service(s)
[AfterEach] Validate Qos on two node with iperfpod
  /gocode/main/test/e2e/tests/network-pod.go:3497
    DEBUG: 2019/11/27 02:34:20 END_TEST UniDirectionalQosValidationAcrossBothNics Time-taken : 492.359826911
    DEBUG: 2019/11/27 02:34:20 Checking stale resources
    DEBUG: 2019/11/27 02:34:20 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:34:20 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:34:20 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:492.518 seconds][0m
Network.UniDirectionalQosValidationAcrossBothNics Daily AT_Qos-1.5 AT_Qos-1.6 Qos Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:3480[0m
  Validate Qos on two node with iperfpod
  [90m/gocode/main/test/e2e/tests/network-pod.go:3481[0m
    Network uni-directional port 0 with high QoS and port 2 with medium QoS, QoS should be honoured
    [90m/gocode/main/test/e2e/tests/network-pod.go:3502[0m
[90m------------------------------[0m
[0mRemoteStorage.NicVFsSchedulingWithCustomQos Daily AT_Scheduling-2.1 Qos[0m [90mPod with custom qos(180K iops) should schedule on one nicID and pods with high qos should schedule on other nicID[0m 
  [1mCreate pods and check scheduling on nicID(s)[0m
  [37m/gocode/main/test/e2e/tests/volume.go:3416[0m
[BeforeEach] Pod with custom qos(180K iops) should schedule on one nicID and pods with high qos should schedule on other nicID
  /gocode/main/test/e2e/tests/volume.go:3401
    DEBUG: 2019/11/27 02:34:20 START_TEST RemoteStorage.NicVFsSchedulingWithCustomQos
    DEBUG: 2019/11/27 02:34:20 Login to cluster
    DEBUG: 2019/11/27 02:34:20 Checking basic Vnic usage
    DEBUG: 2019/11/27 02:34:21 Updating inventory struct
    DEBUG: 2019/11/27 02:34:21 Checking stale resources
    DEBUG: 2019/11/27 02:34:21 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:34:21 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 02:34:21 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:34:29 Creating storage classes
[It] Create pods and check scheduling on nicID(s)
  /gocode/main/test/e2e/tests/volume.go:3416
    DEBUG: 2019/11/27 02:34:38 Create the perf-tier custom with 180K iops
    DEBUG: 2019/11/27 02:34:43 Create storage class: sc-custom-ext4-1-node0
    DEBUG: 2019/11/27 02:34:45 Creating 1 pod and 1 remote volume with custom qos: 
    DEBUG: 2019/11/27 02:34:45 Create 1 fio pod(s):
    DEBUG: 2019/11/27 02:34:45 Creating dynamic pvc : fio-pod-customtest-vol1
    DEBUG: 2019/11/27 02:34:45 Created PVC successfully.
    DEBUG: 2019/11/27 02:34:45 Creating fio pod: fio-pod-custom-1
    DEBUG: 2019/11/27 02:34:45 Checking if given pods are in Running state
    DEBUG: 2019/11/27 02:34:54 Creating 3 pods and 3 remote volumes with high qos: 
    DEBUG: 2019/11/27 02:34:54 Create 3 fio pod(s):
    DEBUG: 2019/11/27 02:34:54 Creating dynamic pvc : fio-pod-hightest-vol1
    DEBUG: 2019/11/27 02:34:55 Created PVC successfully.
    DEBUG: 2019/11/27 02:34:55 Creating fio pod: fio-pod-high-1
    DEBUG: 2019/11/27 02:34:55 Creating dynamic pvc : fio-pod-hightest-vol2
    DEBUG: 2019/11/27 02:34:55 Created PVC successfully.
    DEBUG: 2019/11/27 02:34:55 Creating fio pod: fio-pod-high-2
    DEBUG: 2019/11/27 02:34:56 Creating dynamic pvc : fio-pod-hightest-vol3
    DEBUG: 2019/11/27 02:34:56 Created PVC successfully.
    DEBUG: 2019/11/27 02:34:56 Creating fio pod: fio-pod-high-3
    DEBUG: 2019/11/27 02:34:56 Checking if given pods are in Running state
    DEBUG: 2019/11/27 02:35:09 Pod scheduled as expected
    DEBUG: 2019/11/27 02:35:09 Deleting all the pods: 
    DEBUG: 2019/11/27 02:35:44 Delete the perf-tier custom
    DEBUG: 2019/11/27 02:35:44 Waitting for volume to move to "Available" state
    DEBUG: 2019/11/27 02:35:44 Delete PVCs: 
    DEBUG: 2019/11/27 02:35:45 Waiting for volumes to get deleted: 
    DEBUG: 2019/11/27 02:36:29 Delete storage class: sc-custom-ext4-1-node0
[AfterEach] Pod with custom qos(180K iops) should schedule on one nicID and pods with high qos should schedule on other nicID
  /gocode/main/test/e2e/tests/volume.go:3411
    DEBUG: 2019/11/27 02:36:29 END_TEST RemoteStorage.NicVFsSchedulingWithCustomQos Time-taken : 129.065775105
    DEBUG: 2019/11/27 02:36:29 Checking stale resources
    DEBUG: 2019/11/27 02:36:29 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:36:29 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:36:29 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:129.210 seconds][0m
RemoteStorage.NicVFsSchedulingWithCustomQos Daily AT_Scheduling-2.1 Qos
[90m/gocode/main/test/e2e/tests/volume.go:3388[0m
  Pod with custom qos(180K iops) should schedule on one nicID and pods with high qos should schedule on other nicID
  [90m/gocode/main/test/e2e/tests/volume.go:3390[0m
    Create pods and check scheduling on nicID(s)
    [90m/gocode/main/test/e2e/tests/volume.go:3416[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mLocalStorage.MultiplePodsOneVolumeEach Daily SP_Basic-1.1 Qos[0m [90mMultiple pods, One simple volume each.[0m 
  [1mMultiple Pods, One Simple volume each.[0m
  [37m/gocode/main/test/e2e/tests/volume.go:3100[0m
[BeforeEach] Multiple pods, One simple volume each.
  /gocode/main/test/e2e/tests/volume.go:3088
    DEBUG: 2019/11/27 02:36:29 START_TEST LocalStorage.MultiplePodsOneVolumeEach
    DEBUG: 2019/11/27 02:36:29 Login to cluster
    DEBUG: 2019/11/27 02:36:30 Checking basic Vnic usage
    DEBUG: 2019/11/27 02:36:30 Updating inventory struct
    DEBUG: 2019/11/27 02:36:30 Checking stale resources
    DEBUG: 2019/11/27 02:36:30 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:36:30 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:36:30 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 02:36:38 Creating storage classes
[It] Multiple Pods, One Simple volume each.
  /gocode/main/test/e2e/tests/volume.go:3100
    DEBUG: 2019/11/27 02:36:48 Creating 10 Dynamic Persistent Volume Claims (PVCs). Mirror count: 1. Selector: node=node0
    DEBUG: 2019/11/27 02:36:48 Created PVC successfully.
    DEBUG: 2019/11/27 02:36:48 Created PVC successfully.
    DEBUG: 2019/11/27 02:36:49 Created PVC successfully.
    DEBUG: 2019/11/27 02:36:49 Created PVC successfully.
    DEBUG: 2019/11/27 02:36:49 Created PVC successfully.
    DEBUG: 2019/11/27 02:36:50 Created PVC successfully.
    DEBUG: 2019/11/27 02:36:50 Created PVC successfully.
    DEBUG: 2019/11/27 02:36:50 Created PVC successfully.
    DEBUG: 2019/11/27 02:36:51 Created PVC successfully.
    DEBUG: 2019/11/27 02:36:51 Created PVC successfully.
    DEBUG: 2019/11/27 02:36:53 Creating 10 fio pods: 
    DEBUG: 2019/11/27 02:36:56 Checking if given pods are in Running state
    DEBUG: 2019/11/27 02:37:25 Wait for volumes to move into attached state: 
    DEBUG: 2019/11/27 02:37:25 Waitting for volume to move to "Attached" state
    DEBUG: 2019/11/27 02:37:26 Sleeping for 180 seconds, so that prometheus will have some stats
    DEBUG: 2019/11/27 02:40:26 Validating qos associated with each volume: 
    DEBUG: 2019/11/27 02:40:27 Deleting pods : 
    DEBUG: 2019/11/27 02:40:42 Wait for volumes to come in Available state: 
    DEBUG: 2019/11/27 02:40:42 Delete PVCs: 
    DEBUG: 2019/11/27 02:40:44 Waiting for volumes to get deleted: 
[AfterEach] Multiple pods, One simple volume each.
  /gocode/main/test/e2e/tests/volume.go:3095
    DEBUG: 2019/11/27 02:43:29 END_TEST LocalStorage.MultiplePodsOneVolumeEach Time-taken : 419.759098783
    DEBUG: 2019/11/27 02:43:29 Checking stale resources
    DEBUG: 2019/11/27 02:43:29 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:43:29 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:43:29 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:419.915 seconds][0m
LocalStorage.MultiplePodsOneVolumeEach Daily SP_Basic-1.1 Qos
[90m/gocode/main/test/e2e/tests/volume.go:3081[0m
  Multiple pods, One simple volume each.
  [90m/gocode/main/test/e2e/tests/volume.go:3083[0m
    Multiple Pods, One Simple volume each.
    [90m/gocode/main/test/e2e/tests/volume.go:3100[0m
[90m------------------------------[0m
[0mBenchmarking.NetworkBiDirectionalTwoPorts Daily AT_Benchmark-1.4 Qos Multizone[0m [90mNetwork bi-directional benchmarking with two network ports[0m 
  [1mNetwork bi-directional benchmarking with two network ports, should get 18G bandwidth[0m
  [37m/gocode/main/test/e2e/tests/benchmarking.go:169[0m
[BeforeEach] Network bi-directional benchmarking with two network ports
  /gocode/main/test/e2e/tests/benchmarking.go:155
    DEBUG: 2019/11/27 02:43:29 START_TEST Benchmarking.NetworkBiDirectionalTwoPorts
    DEBUG: 2019/11/27 02:43:29 Login to cluster
    DEBUG: 2019/11/27 02:43:30 Checking basic Vnic usage
    DEBUG: 2019/11/27 02:43:30 Updating inventory struct
    DEBUG: 2019/11/27 02:43:30 Checking stale resources
    DEBUG: 2019/11/27 02:43:30 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:43:30 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:43:30 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 02:43:38 Creating storage classes
[It] Network bi-directional benchmarking with two network ports, should get 18G bandwidth
  /gocode/main/test/e2e/tests/benchmarking.go:169
    DEBUG: 2019/11/27 02:43:47 Creating four pairs of iperf client-server pods
    DEBUG: 2019/11/27 02:43:47 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/27 02:43:50 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/27 02:43:50 Creating iperf server pod: iperf-server-high2
    DEBUG: 2019/11/27 02:43:53 Creating service with name: iperf-server-high2
    DEBUG: 2019/11/27 02:43:53 Creating iperf server pod: iperf-server-high3
    DEBUG: 2019/11/27 02:43:55 Creating service with name: iperf-server-high3
    DEBUG: 2019/11/27 02:43:56 Creating iperf server pod: iperf-server-high4
    DEBUG: 2019/11/27 02:43:59 Creating service with name: iperf-server-high4
    DEBUG: 2019/11/27 02:44:29 Creating iperf Client pod: iperf-client-high1
    DEBUG: 2019/11/27 02:44:32 Creating iperf Client pod: iperf-client-high2
    DEBUG: 2019/11/27 02:44:35 Creating iperf Client pod: iperf-client-high3
    DEBUG: 2019/11/27 02:44:38 Creating iperf Client pod: iperf-client-high4
    DEBUG: 2019/11/27 02:44:41 Creating four pairs of iperf client-server pods
    DEBUG: 2019/11/27 02:44:41 Creating iperf server pod: iperf-servernew-high1
    DEBUG: 2019/11/27 02:44:45 Creating service with name: iperf-servernew-high1
    DEBUG: 2019/11/27 02:44:45 Creating iperf server pod: iperf-servernew-high2
    DEBUG: 2019/11/27 02:44:48 Creating service with name: iperf-servernew-high2
    DEBUG: 2019/11/27 02:44:48 Creating iperf server pod: iperf-servernew-high3
    DEBUG: 2019/11/27 02:44:52 Creating service with name: iperf-servernew-high3
    DEBUG: 2019/11/27 02:44:52 Creating iperf server pod: iperf-servernew-high4
    DEBUG: 2019/11/27 02:44:56 Creating service with name: iperf-servernew-high4
    DEBUG: 2019/11/27 02:45:26 Creating iperf Client pod: iperf-clientnew-high1
    DEBUG: 2019/11/27 02:45:29 Creating iperf Client pod: iperf-clientnew-high2
    DEBUG: 2019/11/27 02:45:34 Creating iperf Client pod: iperf-clientnew-high3
    DEBUG: 2019/11/27 02:45:38 Creating iperf Client pod: iperf-clientnew-high4
    DEBUG: 2019/11/27 02:45:42 Sleeping for 180 seconds, so that prometheus database will have some stats.
    DEBUG: 2019/11/27 02:48:42 Validating if bandwidth is honored or not for server pods:
    DEBUG: 2019/11/27 02:48:43 QoS honored for pod: iperf-server-high1
    DEBUG: 2019/11/27 02:48:43 QoS honored for pod: iperf-server-high2
    DEBUG: 2019/11/27 02:48:44 QoS honored for pod: iperf-server-high3
    DEBUG: 2019/11/27 02:48:44 QoS honored for pod: iperf-server-high4
    DEBUG: 2019/11/27 02:48:45 QoS honored for pod: iperf-servernew-high1
    DEBUG: 2019/11/27 02:48:45 QoS honored for pod: iperf-servernew-high2
    DEBUG: 2019/11/27 02:48:45 QoS honored for pod: iperf-servernew-high3
    DEBUG: 2019/11/27 02:48:46 QoS honored for pod: iperf-servernew-high4
    DEBUG: 2019/11/27 02:48:46 Validating if bandwidth is honored or not for client pods:
    DEBUG: 2019/11/27 02:48:46 QoS honored for pod: iperf-client-high1
    DEBUG: 2019/11/27 02:48:47 QoS honored for pod: iperf-client-high2
    DEBUG: 2019/11/27 02:48:47 QoS honored for pod: iperf-client-high3
    DEBUG: 2019/11/27 02:48:48 QoS honored for pod: iperf-client-high4
    DEBUG: 2019/11/27 02:48:48 QoS honored for pod: iperf-clientnew-high1
    DEBUG: 2019/11/27 02:48:48 QoS honored for pod: iperf-clientnew-high2
    DEBUG: 2019/11/27 02:48:49 QoS honored for pod: iperf-clientnew-high3
    DEBUG: 2019/11/27 02:48:49 QoS honored for pod: iperf-clientnew-high4
    DEBUG: 2019/11/27 02:48:49 Measuring throughput. num of links used: 2
    DEBUG: 2019/11/27 02:48:49 Node: appserv53. Expected Throughput: 16200000000. RX Throughput: 17690721445. TX Throughput: 0
    DEBUG: 2019/11/27 02:48:49 Node: appserv54. Expected Throughput: 16200000000. RX Throughput: 0. TX Throughput: 17736865701
    DEBUG: 2019/11/27 02:48:49 Deleting pods:
    DEBUG: 2019/11/27 02:48:49 Deleting pods : 
    DEBUG: 2019/11/27 02:49:16 Delete services: 
    DEBUG: 2019/11/27 02:49:16 Deleting service(s)
[AfterEach] Network bi-directional benchmarking with two network ports
  /gocode/main/test/e2e/tests/benchmarking.go:164
    DEBUG: 2019/11/27 02:49:17 END_TEST Benchmarking.NetworkBiDirectionalTwoPorts Time-taken : 347.977818446
    DEBUG: 2019/11/27 02:49:17 Checking stale resources
    DEBUG: 2019/11/27 02:49:17 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:49:17 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:49:17 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:348.128 seconds][0m
Benchmarking.NetworkBiDirectionalTwoPorts Daily AT_Benchmark-1.4 Qos Multizone
[90m/gocode/main/test/e2e/tests/benchmarking.go:146[0m
  Network bi-directional benchmarking with two network ports
  [90m/gocode/main/test/e2e/tests/benchmarking.go:147[0m
    Network bi-directional benchmarking with two network ports, should get 18G bandwidth
    [90m/gocode/main/test/e2e/tests/benchmarking.go:169[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mClusterIp.BasicWithHostNetwork Daily CI_basic-1.1[0m [90mCreate nginx & httperf pod, create clusterIp service, Check packets on host-network interface[0m 
  [1mCreate nginx & httperf pod, create clusterIp service, Check packets on host-network interface[0m
  [37m/gocode/main/test/e2e/tests/kubeservices.go:196[0m
[BeforeEach] Create nginx & httperf pod, create clusterIp service, Check packets on host-network interface
  /gocode/main/test/e2e/tests/kubeservices.go:185
    DEBUG: 2019/11/27 02:49:17 START_TEST ClusterIp.BasiciWithHostNetwork
    DEBUG: 2019/11/27 02:49:17 Login to cluster
    DEBUG: 2019/11/27 02:49:18 Checking basic Vnic usage
    DEBUG: 2019/11/27 02:49:18 Updating inventory struct
    DEBUG: 2019/11/27 02:49:18 Checking stale resources
    DEBUG: 2019/11/27 02:49:18 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:49:18 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:49:18 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 02:49:26 Creating storage classes
[It] Create nginx & httperf pod, create clusterIp service, Check packets on host-network interface
  /gocode/main/test/e2e/tests/kubeservices.go:196
    DEBUG: 2019/11/27 02:49:35 Updating non-default network blue to host network
    DEBUG: 2019/11/27 02:49:35 Creating replication-controller nginx-rc
    DEBUG: 2019/11/27 02:49:41 List the pods.
    DEBUG: 2019/11/27 02:49:41 Create nginx clusterIp service
    DEBUG: 2019/11/27 02:49:41 Creating httperf pod
    DEBUG: 2019/11/27 02:49:44 Getting interface of node where httperf pod scheduled
    DEBUG: 2019/11/27 02:49:44 Checking rx_packets on host-network interface enp129s1f4
    DEBUG: 2019/11/27 02:49:44 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/27 02:50:18 Data is going through host-network. packet direction:  rx_packets
    DEBUG: 2019/11/27 02:50:19 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/27 02:50:52 Data is going through host-network. packet direction:  rx_packets
    DEBUG: 2019/11/27 02:50:53 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/27 02:51:27 Data is going through host-network. packet direction:  rx_packets
    DEBUG: 2019/11/27 02:51:27 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/27 02:52:01 Data is going through host-network. packet direction:  rx_packets
    DEBUG: 2019/11/27 02:52:02 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/27 02:52:36 Data is going through host-network. packet direction:  rx_packets
    DEBUG: 2019/11/27 02:52:36 Checking tx_packets on host-network interface enp129s1f4
    DEBUG: 2019/11/27 02:52:36 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/27 02:53:11 Data is going through host-network. packet direction:  tx_packets
    DEBUG: 2019/11/27 02:53:11 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/27 02:53:45 Data is going through host-network. packet direction:  tx_packets
    DEBUG: 2019/11/27 02:53:45 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/27 02:54:20 Data is going through host-network. packet direction:  tx_packets
    DEBUG: 2019/11/27 02:54:20 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/27 02:54:55 Data is going through host-network. packet direction:  tx_packets
    DEBUG: 2019/11/27 02:54:55 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/27 02:55:30 Data is going through host-network. packet direction:  tx_packets
    DEBUG: 2019/11/27 02:55:30 Delete rc nginx-rc 
    DEBUG: 2019/11/27 02:55:30 List the pods.
    DEBUG: 2019/11/27 02:55:47 Deleting httperf pod: 
    DEBUG: 2019/11/27 02:56:22 Deleting nginx service: 
    DEBUG: 2019/11/27 02:56:22 Waiting 30 sec for vnic usage to get reduced
    DEBUG: 2019/11/27 02:56:53 Delete the host network.
    DEBUG: 2019/11/27 02:56:58 Create the non-default network.
[AfterEach] Create nginx & httperf pod, create clusterIp service, Check packets on host-network interface
  /gocode/main/test/e2e/tests/kubeservices.go:191
    DEBUG: 2019/11/27 02:56:58 END_TEST ClusterIp.BasicWithHostNetwork Time-taken : 460.439555625
    DEBUG: 2019/11/27 02:56:58 Checking stale resources
    DEBUG: 2019/11/27 02:56:58 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:56:58 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:56:58 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:460.597 seconds][0m
ClusterIp.BasicWithHostNetwork Daily CI_basic-1.1
[90m/gocode/main/test/e2e/tests/kubeservices.go:168[0m
  Create nginx & httperf pod, create clusterIp service, Check packets on host-network interface
  [90m/gocode/main/test/e2e/tests/kubeservices.go:169[0m
    Create nginx & httperf pod, create clusterIp service, Check packets on host-network interface
    [90m/gocode/main/test/e2e/tests/kubeservices.go:196[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mMirroring.DetachTwoPlexesDuringIOAttachDuringIO Daily SM_PlexDetach-1.6[0m [90mdetaching two plexes during IO & attaching them during IO. Verifying resync is successful[0m 
  [1mdetaches two plexes during IO & attaches them during IO. Verifying resync is successful[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:1292[0m
[BeforeEach] detaching two plexes during IO & attaching them during IO. Verifying resync is successful
  /gocode/main/test/e2e/tests/mirroring.go:1279
    DEBUG: 2019/11/27 02:56:58 START_TEST Mirroring.DetachTwoPlexesDuringIOAttachDuringIO
    DEBUG: 2019/11/27 02:56:58 Login to cluster
    DEBUG: 2019/11/27 02:56:58 Checking basic Vnic usage
    DEBUG: 2019/11/27 02:56:58 Updating inventory struct
    DEBUG: 2019/11/27 02:56:59 Checking stale resources
    DEBUG: 2019/11/27 02:56:59 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 02:56:59 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 02:56:59 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 02:57:07 Creating storage classes
[It] detaches two plexes during IO & attaches them during IO. Verifying resync is successful
  /gocode/main/test/e2e/tests/mirroring.go:1292
    DEBUG: 2019/11/27 02:57:16 Verifying whether FBM and L1 usage is zero across all nodes
    DEBUG: 2019/11/27 02:57:22 FBM and L1 usage is Zero across all nodes

    DEBUG: 2019/11/27 02:57:22 Assigning label to nodes where the plexes of mirrored volumes should get scheduled
    DEBUG: 2019/11/27 02:57:23 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 02:57:23 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 02:57:23 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 02:57:23 Creating 8 volumes of random sizes
    DEBUG: 2019/11/27 02:57:23 Mirror Count: 3
    DEBUG: 2019/11/27 02:57:32 Attaching all 8 volumes to node appserv54
    DEBUG: 2019/11/27 02:58:10 Running WRITE fio job on: appserv54. FIO Command : echo -e '#!/bin/bash\n\nsudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"akpz\"-12 --verify_interval=4096 --runtime=200 --blocksize=512K --iodepth=8  --time_based  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 > /tmp/fio_result.txt & > /dev/null 2>&1' > /tmp/fio_run.sh

    DEBUG: 2019/11/27 02:58:13 Number of running fio process: 11

    DEBUG: 2019/11/27 02:59:14 Detaching first non initiator plex from all the volumes
    DEBUG: 2019/11/27 02:59:53 Detaching second non initiator plex from all the volumes
    DEBUG: 2019/11/27 03:00:19 Reattaching first non initiator plex to all the volumes
    DEBUG: 2019/11/27 03:00:35 Reattaching second non initiator plex to all the volumes
    DEBUG: 2019/11/27 03:00:36 Verifying if Resync of plexes has started
    DEBUG: 2019/11/27 03:00:36 Number of volumes : 8
    DEBUG: 2019/11/27 03:00:36 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/27 03:00:36 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:00:36 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 03:00:37 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:00:38 Volume: test-vol8. Resync offset: 0

    DEBUG: 2019/11/27 03:00:39 Volume: test-vol8. Resync offset: 0

    DEBUG: 2019/11/27 03:00:44 Volume: test-vol8. Resync offset: 0

    DEBUG: 2019/11/27 03:00:50 Volume: test-vol8. Resync offset: 1

    DEBUG: 2019/11/27 03:00:50 Volume name & Plex : test-vol8.p2. Plex State : Detached
    DEBUG: 2019/11/27 03:00:50 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/27 03:00:50 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:00:50 Volume name & Plex : test-vol4.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 03:00:52 Volume "test-vol4" has index "3" in embedded.
    DEBUG: 2019/11/27 03:00:52 Volume: test-vol4. Resync offset: 1

    DEBUG: 2019/11/27 03:00:53 Volume: test-vol4. Resync offset: 1

    DEBUG: 2019/11/27 03:00:59 Volume: test-vol4. Resync offset: 2

    DEBUG: 2019/11/27 03:00:59 Volume name & Plex : test-vol4.p2. Plex State : Detached
    DEBUG: 2019/11/27 03:00:59 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 03:00:59 Volume name & Plex : test-vol1.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 03:01:00 Volume "test-vol1" has index "0" in embedded.
    DEBUG: 2019/11/27 03:01:01 Volume: test-vol1. Resync offset: 41

    DEBUG: 2019/11/27 03:01:01 Volume: test-vol1. Resync offset: 42

    DEBUG: 2019/11/27 03:01:01 Volume name & Plex : test-vol1.p1. Plex State : Detached
    DEBUG: 2019/11/27 03:01:01 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:01:01 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 03:01:02 Volume name & Plex : test-vol2.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 03:01:03 Volume "test-vol2" has index "1" in embedded.
    DEBUG: 2019/11/27 03:01:04 Volume: test-vol2. Resync offset: 6

    DEBUG: 2019/11/27 03:01:04 Volume: test-vol2. Resync offset: 6

    DEBUG: 2019/11/27 03:01:10 Volume: test-vol2. Resync offset: 7

    DEBUG: 2019/11/27 03:01:10 Volume name & Plex : test-vol2.p1. Plex State : Detached
    DEBUG: 2019/11/27 03:01:10 Volume name & Plex : test-vol2.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:01:10 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 03:01:10 Volume name & Plex : test-vol3.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 03:01:11 Volume "test-vol3" has index "2" in embedded.
    DEBUG: 2019/11/27 03:01:12 Volume: test-vol3. Resync offset: 4

    DEBUG: 2019/11/27 03:01:13 Volume: test-vol3. Resync offset: 4

    DEBUG: 2019/11/27 03:01:19 Volume: test-vol3. Resync offset: 4

    DEBUG: 2019/11/27 03:01:24 Volume: test-vol3. Resync offset: 5

    DEBUG: 2019/11/27 03:01:24 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:01:24 Volume name & Plex : test-vol3.p2. Plex State : Detached
    DEBUG: 2019/11/27 03:01:24 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/27 03:01:24 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:01:24 Volume name & Plex : test-vol6.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 03:01:26 Volume "test-vol6" has index "5" in embedded.
    DEBUG: 2019/11/27 03:01:26 Volume: test-vol6. Resync offset: 2

    DEBUG: 2019/11/27 03:01:27 Volume: test-vol6. Resync offset: 2

    DEBUG: 2019/11/27 03:01:33 Volume: test-vol6. Resync offset: 2

    DEBUG: 2019/11/27 03:01:39 Volume: test-vol6. Resync offset: 6

    DEBUG: 2019/11/27 03:01:39 Volume name & Plex : test-vol6.p2. Plex State : Detached
    DEBUG: 2019/11/27 03:01:39 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/27 03:01:39 Volume name & Plex : test-vol5.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 03:01:40 Volume "test-vol5" has index "4" in embedded.
    DEBUG: 2019/11/27 03:01:41 Volume: test-vol5. Resync offset: 9

    DEBUG: 2019/11/27 03:01:41 Volume: test-vol5. Resync offset: 10

    DEBUG: 2019/11/27 03:01:41 Volume name & Plex : test-vol5.p1. Plex State : Detached
    DEBUG: 2019/11/27 03:01:41 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:01:41 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/27 03:01:42 Volume name & Plex : test-vol7.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 03:01:43 Volume "test-vol7" has index "6" in embedded.
    DEBUG: 2019/11/27 03:01:43 Volume: test-vol7. Resync offset: 8

    DEBUG: 2019/11/27 03:01:44 Volume: test-vol7. Resync offset: 8

    DEBUG: 2019/11/27 03:01:50 Volume: test-vol7. Resync offset: 12

    DEBUG: 2019/11/27 03:01:50 Volume name & Plex : test-vol7.p1. Plex State : Detached
    DEBUG: 2019/11/27 03:01:50 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:01:50 Comparing Volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/27 03:01:55 Waiting for fio job to complete on the node
    DEBUG: 2019/11/27 03:01:55 Verifying if Resync of plexes has completed
    DEBUG: 2019/11/27 03:01:55 Number of volumes : 8
    DEBUG: 2019/11/27 03:01:55 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/27 03:01:55 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:01:55 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 03:01:57 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:01:57 Volume: test-vol8. Resync offset: 13

    DEBUG: 2019/11/27 03:01:58 Volume: test-vol8. Resync offset: 14

    DEBUG: 2019/11/27 03:01:58 Volume name & Plex : test-vol8.p2. Plex State : Detached
    DEBUG: 2019/11/27 03:01:58 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 03:02:28 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:02:28 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 03:02:29 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:02:30 Volume: test-vol8. Resync offset: 29

    DEBUG: 2019/11/27 03:02:31 Volume: test-vol8. Resync offset: 29

    DEBUG: 2019/11/27 03:02:37 Volume: test-vol8. Resync offset: 32

    DEBUG: 2019/11/27 03:02:37 Volume name & Plex : test-vol8.p2. Plex State : Detached
    DEBUG: 2019/11/27 03:02:37 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 03:03:07 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:03:07 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 03:03:08 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:03:09 Volume: test-vol8. Resync offset: 49

    DEBUG: 2019/11/27 03:03:09 Volume: test-vol8. Resync offset: 50

    DEBUG: 2019/11/27 03:03:09 Volume name & Plex : test-vol8.p2. Plex State : Detached
    DEBUG: 2019/11/27 03:03:09 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 03:03:39 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:03:39 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 03:03:41 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:03:41 Volume: test-vol8. Resync offset: 67

    DEBUG: 2019/11/27 03:03:42 Volume: test-vol8. Resync offset: 68

    DEBUG: 2019/11/27 03:03:42 Volume name & Plex : test-vol8.p2. Plex State : Detached
    DEBUG: 2019/11/27 03:03:42 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 03:04:12 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:04:12 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 03:04:14 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:04:14 Volume: test-vol8. Resync offset: 85

    DEBUG: 2019/11/27 03:04:15 Volume: test-vol8. Resync offset: 86

    DEBUG: 2019/11/27 03:04:15 Volume name & Plex : test-vol8.p2. Plex State : Detached
    DEBUG: 2019/11/27 03:04:15 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 03:04:45 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:04:45 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:04:45 Volume name & Plex : test-vol8.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 03:04:46 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:04:47 Volume: test-vol8. Resync offset: 4

    DEBUG: 2019/11/27 03:04:48 Volume: test-vol8. Resync offset: 4

    DEBUG: 2019/11/27 03:04:53 Volume: test-vol8. Resync offset: 8

    DEBUG: 2019/11/27 03:04:53 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 03:05:24 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:05:24 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:05:24 Volume name & Plex : test-vol8.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 03:05:25 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:05:26 Volume: test-vol8. Resync offset: 27

    DEBUG: 2019/11/27 03:05:26 Volume: test-vol8. Resync offset: 27

    DEBUG: 2019/11/27 03:05:32 Volume: test-vol8. Resync offset: 31

    DEBUG: 2019/11/27 03:05:32 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 03:06:02 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:06:02 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:06:02 Volume name & Plex : test-vol8.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 03:06:03 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:06:04 Volume: test-vol8. Resync offset: 50

    DEBUG: 2019/11/27 03:06:05 Volume: test-vol8. Resync offset: 51

    DEBUG: 2019/11/27 03:06:05 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 03:06:35 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:06:35 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:06:35 Volume name & Plex : test-vol8.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 03:06:36 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:06:37 Volume: test-vol8. Resync offset: 71

    DEBUG: 2019/11/27 03:06:38 Volume: test-vol8. Resync offset: 71

    DEBUG: 2019/11/27 03:06:43 Volume: test-vol8. Resync offset: 75

    DEBUG: 2019/11/27 03:06:43 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 03:07:13 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:07:13 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:07:13 Volume name & Plex : test-vol8.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 03:07:15 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 03:07:16 Volume: test-vol8. Resync offset: 96

    DEBUG: 2019/11/27 03:07:16 Volume: test-vol8. Resync offset: 96

    DEBUG: 2019/11/27 03:07:22 Volume: test-vol8. Resync offset: 
    DEBUG: 2019/11/27 03:07:23 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol8.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/27 03:07:53 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol4.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/27 03:07:53 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 03:07:53 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol2.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 03:07:53 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol3.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/27 03:07:53 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol6.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/27 03:07:53 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/27 03:07:53 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/27 03:07:53 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/27 03:07:58 Verifying sha512sum across all plexes
    DEBUG: 2019/11/27 03:07:59 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 03:08:01 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 03:08:02 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 03:08:03 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 03:08:05 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 03:08:06 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 03:08:07 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 03:08:09 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 03:08:10 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 03:08:10 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 03:08:10 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 03:08:10 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 03:08:10 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 03:08:10 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 03:08:10 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 03:08:10 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 03:10:02 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 03:10:04 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 03:10:05 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 03:10:07 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 03:10:08 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 03:10:09 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 03:10:11 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 03:10:12 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 03:10:13 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 03:10:13 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 03:10:13 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 03:10:13 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 03:10:13 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 03:10:13 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 03:10:13 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 03:10:14 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 03:12:32 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 03:12:33 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 03:12:35 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 03:12:36 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/27 03:12:37 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 03:12:39 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/27 03:12:40 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/27 03:12:42 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/27 03:12:42 Calculating cksum for volume test-vol1 and plex p2
    DEBUG: 2019/11/27 03:12:43 Calculating cksum for volume test-vol2 and plex p2
    DEBUG: 2019/11/27 03:12:43 Calculating cksum for volume test-vol3 and plex p2
    DEBUG: 2019/11/27 03:12:43 Calculating cksum for volume test-vol4 and plex p2
    DEBUG: 2019/11/27 03:12:43 Calculating cksum for volume test-vol5 and plex p2
    DEBUG: 2019/11/27 03:12:43 Calculating cksum for volume test-vol6 and plex p2
    DEBUG: 2019/11/27 03:12:43 Calculating cksum for volume test-vol7 and plex p2
    DEBUG: 2019/11/27 03:12:43 Calculating cksum for volume test-vol8 and plex p2
    DEBUG: 2019/11/27 03:14:49 Sha512sum matched for all volumes across all plexes
    DEBUG: 2019/11/27 03:14:49 Detach & Delete all volumes
    DEBUG: 2019/11/27 03:16:01 Removing label from the nodes where the plexes of mirrored volumes were scheduled
    DEBUG: 2019/11/27 03:16:01 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 03:16:01 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 03:16:01 Removed label : mirror from node : appserv55
[AfterEach] detaching two plexes during IO & attaching them during IO. Verifying resync is successful
  /gocode/main/test/e2e/tests/mirroring.go:1288
    DEBUG: 2019/11/27 03:16:01 END_TEST Mirroring.DetachTwoPlexesDuringIOAttachDuringIO Time-taken : 1143.392497663
    DEBUG: 2019/11/27 03:16:01 Checking stale resources
    DEBUG: 2019/11/27 03:16:01 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 03:16:01 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 03:16:01 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:1143.538 seconds][0m
Mirroring.DetachTwoPlexesDuringIOAttachDuringIO Daily SM_PlexDetach-1.6
[90m/gocode/main/test/e2e/tests/mirroring.go:1272[0m
  detaching two plexes during IO & attaching them during IO. Verifying resync is successful
  [90m/gocode/main/test/e2e/tests/mirroring.go:1274[0m
    detaches two plexes during IO & attaches them during IO. Verifying resync is successful
    [90m/gocode/main/test/e2e/tests/mirroring.go:1292[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mRbacStaticProvision.PodWithLSAndValidateIOPSWithMultipleQosforLocalNRemoteAuth Daily PVC_Rbac_S-1.0 PVC_Rbac_S-1.1 PVC_Rbac_S-2.0 PVC_Rbac_S-2.2[0m [90mChecking Iops for different Qos in namespace for local storage[0m 
  [1mChecking Iops for differnt Qos in namespace for local storage with static provision[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:2055[0m
[BeforeEach] Checking Iops for different Qos in namespace for local storage
  /gocode/main/test/e2e/tests/rbac.go:2041
    DEBUG: 2019/11/27 03:16:01 START_TEST RbacStaticProvision.PodWithLocalStorageAndValidateIOPSWithMultipleQosforLocalNRemoteAuth
    DEBUG: 2019/11/27 03:16:01 Login to cluster
    DEBUG: 2019/11/27 03:16:02 Checking basic Vnic usage
    DEBUG: 2019/11/27 03:16:02 Updating inventory struct
    DEBUG: 2019/11/27 03:16:03 Checking stale resources
    DEBUG: 2019/11/27 03:16:03 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 03:16:03 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 03:16:03 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 03:16:11 Creating storage classes
[It] Checking Iops for differnt Qos in namespace for local storage with static provision
  /gocode/main/test/e2e/tests/rbac.go:2055
    DEBUG: 2019/11/27 03:16:22 oS is Linux 
    DEBUG: 2019/11/27 03:16:22 User is %!s(bool=false) 
    DEBUG: 2019/11/27 03:16:22 Creating group mygroup1 with container-edit/multipleqostest role(s)
    DEBUG: 2019/11/27 03:16:22 Creating myuser1 user in mygroup1 group
    DEBUG: 2019/11/27 03:16:22 Creating group mygroup2 with perftier-edit,volume-edit,volumeclaim-edit/multipleqostest role(s)
    DEBUG: 2019/11/27 03:16:22 Creating myuser2 user in mygroup2 group
    DEBUG: 2019/11/27 03:16:38 Create perf-tier custom with 1G network bandwidth 
    DEBUG: 2019/11/27 03:16:54 Creating volume and PVC: 
    DEBUG: 2019/11/27 03:16:54 Creating a volume test-vol1 on node=node0: 
    DEBUG: 2019/11/27 03:16:54 Creating pvc test-vol-claim1 for volume test-vol1: 
    DEBUG: 2019/11/27 03:16:55 Created PVC successfully.
    DEBUG: 2019/11/27 03:17:11 Creating fio pod: 
    DEBUG: 2019/11/27 03:17:11 Creating fio-pod1 in multipleqostest namespace on node=node0 
    DEBUG: 2019/11/27 03:17:33 Creating volume and PVC: 
    DEBUG: 2019/11/27 03:17:33 Creating a volume test-vol2 on node=node0: 
    DEBUG: 2019/11/27 03:17:33 Creating pvc test-vol-claim2 for volume test-vol2: 
    DEBUG: 2019/11/27 03:17:33 Created PVC successfully.
    DEBUG: 2019/11/27 03:17:49 Creating fio pod: 
    DEBUG: 2019/11/27 03:17:49 Creating fio-pod2 in multipleqostest namespace on node=node0 
    DEBUG: 2019/11/27 03:18:10 Creating volume and PVC: 
    DEBUG: 2019/11/27 03:18:10 Creating a volume test-vol3 on node=node0: 
    DEBUG: 2019/11/27 03:18:10 Creating pvc test-vol-claim3 for volume test-vol3: 
    DEBUG: 2019/11/27 03:18:10 Created PVC successfully.
    DEBUG: 2019/11/27 03:18:26 Creating fio pod: 
    DEBUG: 2019/11/27 03:18:26 Creating fio-pod3 in multipleqostest namespace on node=node0 
    DEBUG: 2019/11/27 03:18:48 Creating volume and PVC: 
    DEBUG: 2019/11/27 03:18:48 Creating a volume test-vol4 on node=node0: 
    DEBUG: 2019/11/27 03:18:48 Creating pvc test-vol-claim4 for volume test-vol4: 
    DEBUG: 2019/11/27 03:18:49 Created PVC successfully.
    DEBUG: 2019/11/27 03:19:04 Creating fio pod: 
    DEBUG: 2019/11/27 03:19:04 Creating fio-pod4 in multipleqostest namespace on node=node0 
    DEBUG: 2019/11/27 03:19:09 Sleeping for 180 seconds, so that prometheus database will have some stats.
    DEBUG: 2019/11/27 03:22:09 Logging as volume-edit user to validate Qos: 
    DEBUG: 2019/11/27 03:22:25 Getting volumes to validate Qos: 
    DEBUG: 2019/11/27 03:22:25 Validating qos for volume test-vol10: 
    DEBUG: 2019/11/27 03:22:25 Validating qos for volume test-vol21: 
    DEBUG: 2019/11/27 03:22:25 Validating qos for volume test-vol32: 
    DEBUG: 2019/11/27 03:22:26 Validating qos for volume test-vol43: 
    DEBUG: 2019/11/27 03:22:42 Getting Pods to delete
    DEBUG: 2019/11/27 03:22:42 Deleting Pods
    DEBUG: 2019/11/27 03:23:14 Deleting volumes and Pvc: 
    DEBUG: 2019/11/27 03:23:14 Deleting PVC
    DEBUG: 2019/11/27 03:23:14 Deleting PVC
    DEBUG: 2019/11/27 03:23:14 Deleting PVC
    DEBUG: 2019/11/27 03:23:14 Deleting PVC
    DEBUG: 2019/11/27 03:23:14 Wait for volumes to come in Available state
    DEBUG: 2019/11/27 03:23:14 Delete Volumes: 
    DEBUG: 2019/11/27 03:24:00 Deleting perfTier custom: 
    DEBUG: 2019/11/27 03:24:00 Deleting users and groups: 
    DEBUG: 2019/11/27 03:24:01 Delete users
    DEBUG: 2019/11/27 03:24:01 Delete groups
[AfterEach] Checking Iops for different Qos in namespace for local storage
  /gocode/main/test/e2e/tests/rbac.go:2049
    DEBUG: 2019/11/27 03:24:02 END_TEST RbacStaticProvision.PodWithLocalStorageAndValidateIOPSWithMultipleQosforLocalNRemoteAuth Time-taken : 480.978858198
    DEBUG: 2019/11/27 03:24:02 Checking stale resources
    DEBUG: 2019/11/27 03:24:02 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 03:24:02 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 03:24:02 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:481.222 seconds][0m
RbacStaticProvision.PodWithLSAndValidateIOPSWithMultipleQosforLocalNRemoteAuth Daily PVC_Rbac_S-1.0 PVC_Rbac_S-1.1 PVC_Rbac_S-2.0 PVC_Rbac_S-2.2
[90m/gocode/main/test/e2e/tests/rbac.go:2025[0m
  Checking Iops for different Qos in namespace for local storage
  [90m/gocode/main/test/e2e/tests/rbac.go:2027[0m
    Checking Iops for differnt Qos in namespace for local storage with static provision
    [90m/gocode/main/test/e2e/tests/rbac.go:2055[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mRbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 [0m [90mValidate all Edit and View roles[0m 
  [1mValidate user-edit role[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:496[0m
[BeforeEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:447
    DEBUG: 2019/11/27 03:24:03 START_TEST Rbac.RoleValidation
    DEBUG: 2019/11/27 03:24:03 Login to cluster
    DEBUG: 2019/11/27 03:24:03 Checking basic Vnic usage
    DEBUG: 2019/11/27 03:24:03 Updating inventory struct
    DEBUG: 2019/11/27 03:24:04 Checking stale resources
    DEBUG: 2019/11/27 03:24:04 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 03:24:04 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 03:24:04 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 03:24:13 Creating storage classes
[It] Validate user-edit role
  /gocode/main/test/e2e/tests/rbac.go:496
    DEBUG: 2019/11/27 03:24:22 Validate user-edit role
    DEBUG: 2019/11/27 03:24:23 Creating group grp1 with user-edit role(s)
    DEBUG: 2019/11/27 03:24:23 Creating user1 user in grp1 group
    DEBUG: 2019/11/27 03:24:23 Login as user1 user
    DEBUG: 2019/11/27 03:24:24 Operation validated : perftier-list
    ERROR: 2019/11/27 03:24:24  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create temp-pf -b 2G -i 50K', status:&{{Status } {  0} Failure POST on perftiers for "user1" is forbidden: User user1 cannot perform POST on perftiers Forbidden 0xc00014a780 403}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "POST on perftiers for \"user1\" is forbidden: User user1 cannot perform POST on perftiers",
 "reason": "Forbidden",
 "details": {
  "name": "user1",
  "kind": "perftiers"
 },
 "code": 403
}



    DEBUG: 2019/11/27 03:24:24 Operation validated : perftier-create
    DEBUG: 2019/11/27 03:24:27 Operation validated : perftier-delete
    DEBUG: 2019/11/27 03:25:00 Operation validated : volume-list
    DEBUG: 2019/11/27 03:25:00 Operation validated : volume-create
    DEBUG: 2019/11/27 03:25:30 Operation validated : volume-delete
    DEBUG: 2019/11/27 03:25:34 Operation validated : user-list
    DEBUG: 2019/11/27 03:25:34 Operation validated : user-create
    DEBUG: 2019/11/27 03:25:36 Operation validated : user-delete
[AfterEach] Validate all Edit and View roles
  /gocode/main/test/e2e/tests/rbac.go:456
    DEBUG: 2019/11/27 03:25:37 END_TEST Rbac.RoleValidation Time-taken : 94.161307211
    DEBUG: 2019/11/27 03:25:37 Checking stale resources
    DEBUG: 2019/11/27 03:25:37 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 03:25:37 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 03:25:37 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:94.407 seconds][0m
Rbac.RoleValidation Daily Rbac_Basic-1.8 Rbac_Basic-1.9 Rbac_Basic-1.11 Rbac_Basic-1.12 Rbac_Basic-1.13 Rbac_Basic-1.14 
[90m/gocode/main/test/e2e/tests/rbac.go:437[0m
  Validate all Edit and View roles
  [90m/gocode/main/test/e2e/tests/rbac.go:438[0m
    Validate user-edit role
    [90m/gocode/main/test/e2e/tests/rbac.go:496[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mClusterIp.Basic Daily CI_basic-1.0 CI_basic-2.0[0m [90mCluster ip basic tests[0m 
  [1mCluster ip basic tests[0m
  [37m/gocode/main/test/e2e/tests/kubeservices.go:58[0m
[BeforeEach] Cluster ip basic tests
  /gocode/main/test/e2e/tests/kubeservices.go:47
    DEBUG: 2019/11/27 03:25:37 START_TEST ClusterIp.Basic
    DEBUG: 2019/11/27 03:25:37 Login to cluster
    DEBUG: 2019/11/27 03:25:38 Checking basic Vnic usage
    DEBUG: 2019/11/27 03:25:38 Updating inventory struct
    DEBUG: 2019/11/27 03:25:39 Checking stale resources
    DEBUG: 2019/11/27 03:25:39 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 03:25:39 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 03:25:39 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 03:25:47 Creating storage classes
[It] Cluster ip basic tests
  /gocode/main/test/e2e/tests/kubeservices.go:58
    DEBUG: 2019/11/27 03:25:57 Create Replicator
    DEBUG: 2019/11/27 03:26:03 List the pods.
    DEBUG: 2019/11/27 03:26:03 Create nginx headless service
    DEBUG: 2019/11/27 03:26:03 Create nginx clusterIp service
    DEBUG: 2019/11/27 03:26:03 Getting pods 
    DEBUG: 2019/11/27 03:26:03 List the pods.
    DEBUG: 2019/11/27 03:26:03 Logging into pod and checking service: nginx1
    DEBUG: 2019/11/27 03:26:04 Success: Welcome to nginx! 
    DEBUG: 2019/11/27 03:26:04 Logging into pod and checking service: nginx2
    DEBUG: 2019/11/27 03:26:04 Success: Welcome to nginx! 
    DEBUG: 2019/11/27 03:26:04 Delete services: 
    DEBUG: 2019/11/27 03:26:04 Deleting service(s)
    DEBUG: 2019/11/27 03:26:04 Delete rc: 
    DEBUG: 2019/11/27 03:26:04 List the pods.
[AfterEach] Cluster ip basic tests
  /gocode/main/test/e2e/tests/kubeservices.go:53
    DEBUG: 2019/11/27 03:26:20 END_TEST ClusterIp.Basic Time-taken : 42.560030601
    DEBUG: 2019/11/27 03:26:20 Checking stale resources
    DEBUG: 2019/11/27 03:26:20 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 03:26:20 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 03:26:20 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:42.808 seconds][0m
ClusterIp.Basic Daily CI_basic-1.0 CI_basic-2.0
[90m/gocode/main/test/e2e/tests/kubeservices.go:33[0m
  Cluster ip basic tests
  [90m/gocode/main/test/e2e/tests/kubeservices.go:34[0m
    Cluster ip basic tests
    [90m/gocode/main/test/e2e/tests/kubeservices.go:58[0m
[90m------------------------------[0m
[0mCluster.Basic Management Sanity Daily M_Cluster-1.0 M_Cluster-1.7 M_Cluster-1.11[0m [90mwhen cluster is created with all nodes[0m 
  [1mshould be created and destroyed[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:61[0m
[BeforeEach] when cluster is created with all nodes
  /gocode/main/test/e2e/tests/cluster.go:48
    DEBUG: 2019/11/27 03:26:20 START_TEST Cluster.Basic
[It] should be created and destroyed
  /gocode/main/test/e2e/tests/cluster.go:61
    DEBUG: 2019/11/27 03:26:20 Login to cluster
    DEBUG: 2019/11/27 03:26:20 Checking basic Vnic usage
    DEBUG: 2019/11/27 03:26:20 Updating inventory struct
    DEBUG: 2019/11/27 03:26:21 Checking stale resources
    DEBUG: 2019/11/27 03:26:21 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 03:26:21 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 03:26:21 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 03:26:30 Creating storage classes
    DEBUG: 2019/11/27 03:26:39 Login to cluster
    DEBUG: 2019/11/27 03:26:40 Destroying the cluster: a66dcf95-10d6-11ea-8b52-a4bf01194d67, Master node is appserv53
    DEBUG: 2019/11/27 03:26:40 Checking in a loop for cluster status
    DEBUG: 2019/11/27 03:27:18 Doing sync all nodes.
    DEBUG: 2019/11/27 03:27:18 Doing sync all nodes.
    DEBUG: 2019/11/27 03:27:19 Doing sync all nodes.
    DEBUG: 2019/11/27 03:27:22 Checking for cluster-info.json on node :172.16.6.153
    DEBUG: 2019/11/27 03:27:22 Checking for cluster-info.json on node :172.16.6.154
    DEBUG: 2019/11/27 03:27:23 Checking for cluster-info.json on node :172.16.6.155
    DEBUG: 2019/11/27 03:27:23 Rebooting all nodes.
    DEBUG: 2019/11/27 03:27:23 Doing sync on 172.16.6.153
    DEBUG: 2019/11/27 03:27:24 Doing sync on 172.16.6.154
    DEBUG: 2019/11/27 03:27:25 Doing sync on 172.16.6.155
    DEBUG: 2019/11/27 03:27:26 Waiting for nodes to come up, will wait upto 800 seconds
..............    DEBUG: 2019/11/27 03:30:06 Nodes are up, waiting for armada to start
.....
[AfterEach] when cluster is created with all nodes
  /gocode/main/test/e2e/tests/cluster.go:56
    DEBUG: 2019/11/27 03:30:56 END_TEST Cluster.Basic Time-taken : 275.766066503

[32mâ€¢ [SLOW TEST:275.767 seconds][0m
Cluster.Basic Management Sanity Daily M_Cluster-1.0 M_Cluster-1.7 M_Cluster-1.11
[90m/gocode/main/test/e2e/tests/cluster.go:40[0m
  when cluster is created with all nodes
  [90m/gocode/main/test/e2e/tests/cluster.go:43[0m
    should be created and destroyed
    [90m/gocode/main/test/e2e/tests/cluster.go:61[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mSnapshot.RebootOneTarget Daily SS_Reboot-1.0[0m [90mreboot a target node after creating snapshot and LCV and verify data is consistent across all plexes after reboot[0m 
  [1mreboots a target node after creating snapshot and LCV and verifies data consistency across all plexes after reboot[0m
  [37m/gocode/main/test/e2e/tests/snapshot.go:508[0m
[BeforeEach] reboot a target node after creating snapshot and LCV and verify data is consistent across all plexes after reboot
  /gocode/main/test/e2e/tests/snapshot.go:494
    DEBUG: 2019/11/27 03:30:56 START_TEST Snapshot.RebootOneTarget
    DEBUG: 2019/11/27 03:30:56 Cluster Spec Node list is [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 03:30:56 Getting dns domain name
    DEBUG: 2019/11/27 03:30:56 FQDN : autotb7.eng.diamanti.com
    DEBUG: 2019/11/27 03:30:56 Generating certificates for the cluster: (Name: autotb7, VIP: 172.16.19.55, FQDN: autotb7.eng.diamanti.com)
    DEBUG: 2019/11/27 03:30:56 Clean up existing certs if any:
    DEBUG: 2019/11/27 03:30:56 Generate unique CA name with current date
    DEBUG: 2019/11/27 03:30:56 Integrate CA name in file
    DEBUG: 2019/11/27 03:30:56 Generate CA certs
    DEBUG: 2019/11/27 03:30:56 Create a CSR to generate a certificate using FQDN, VIP, Cluster Name for a server certs
    DEBUG: 2019/11/27 03:30:56 Generate server certificate:
    DEBUG: 2019/11/27 03:30:57 Getting CertificateAuthority from /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/server_certs/ca.pem file
    DEBUG: 2019/11/27 03:30:57 Getting ServerCertificate from /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/server_certs/server.pem file
    DEBUG: 2019/11/27 03:30:57 Getting ServerPrivateKey from /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/server_certs/server-key.pem file
    DEBUG: 2019/11/27 03:30:57 Creating the cluster
    DEBUG: 2019/11/27 03:31:12 Please import "/dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/server_certs/ca.pem" certificate to your client machine
    DEBUG: 2019/11/27 03:31:12 Sleeping for 60 sec
    DEBUG: 2019/11/27 03:32:12 Save cluster configuration: 
    DEBUG: 2019/11/27 03:32:13 Login to cluster
    DEBUG: 2019/11/27 03:32:13 Polling for cluster login for 300 seconds.
    DEBUG: 2019/11/27 03:32:14 Checking in a loop for cluster status
    DEBUG: 2019/11/27 03:32:14 Found '3' nodes
    DEBUG: 2019/11/27 03:32:14 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 03:32:14 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 03:32:14 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 03:32:14 Creating network default
    DEBUG: 2019/11/27 03:32:14 Creating network blue
    DEBUG: 2019/11/27 03:32:14 Add default tag to default network
    DEBUG: 2019/11/27 03:32:25 Labeled all nodes with node=node$

    DEBUG: 2019/11/27 03:32:25 Getting cluster ID
    DEBUG: 2019/11/27 03:32:25 Created test cluster: 6136201a-1109-11ea-9c78-a4bf01194d67
    DEBUG: 2019/11/27 03:32:25 Deleting all LCVs, volumes, snapshots from previous cluster if any.
    DEBUG: 2019/11/27 03:32:25 Recording timestamp of all services on all nodes
    DEBUG: 2019/11/27 03:32:34 Overwritting e2e parameter : ExpectedBasicVnicUsageCount
    DEBUG: 2019/11/27 03:32:35 Checking if given pods are in Running state
    DEBUG: 2019/11/27 03:32:35 Checking if given pods are in Running state
    DEBUG: 2019/11/27 03:32:37 Checking if given pods are in Running state
    DEBUG: 2019/11/27 03:32:37 Checking if given pods are in Running state
    DEBUG: 2019/11/27 03:32:38 Checking if given pods are in Running state
    DEBUG: 2019/11/27 03:32:38 Updating inventory struct
    DEBUG: 2019/11/27 03:32:39 Creating storage classes
[It] reboots a target node after creating snapshot and LCV and verifies data consistency across all plexes after reboot
  /gocode/main/test/e2e/tests/snapshot.go:508
    DEBUG: 2019/11/27 03:32:49 Mirror count not set in e2e_param.json
    DEBUG: 2019/11/27 03:32:49 Selecting mirror count as : 2
    DEBUG: 2019/11/27 03:32:49 Assigning label to nodes where the plexes of mirrored volumes should get scheduled
    DEBUG: 2019/11/27 03:32:49 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 03:32:49 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 03:32:49 Verifying whether FBM and L1 usage is zero across all nodes
    DEBUG: 2019/11/27 03:32:56 FBM and L1 usage is Zero across all nodes

    DEBUG: 2019/11/27 03:32:56 Creating 8 volumes of random sizes
    DEBUG: 2019/11/27 03:32:56 Mirror Count: 2
    DEBUG: 2019/11/27 03:32:57 Attaching all 8 volumes
    DEBUG: 2019/11/27 03:33:49 Running WRITE fio job on node : appserv54
    DEBUG: 2019/11/27 03:33:49 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"efgh\"-12 --verify_interval=4096 --runtime=200 --blocksize=512K --iodepth=8  --time_based  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/27 03:37:11 Calculating sha512sum of volumes before creating snapshots
    DEBUG: 2019/11/27 03:37:12 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 03:37:14 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 03:37:15 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 03:37:17 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 03:37:18 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 03:37:19 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 03:37:21 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 03:37:22 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 03:37:23 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 03:37:23 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 03:37:23 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 03:37:23 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 03:37:24 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 03:37:24 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 03:37:24 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 03:37:24 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 03:41:41 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 03:41:42 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 03:41:43 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 03:41:45 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 03:41:46 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 03:41:48 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 03:41:49 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 03:41:50 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 03:41:51 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 03:41:51 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 03:41:51 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 03:41:52 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 03:41:52 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 03:41:52 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 03:41:52 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 03:41:52 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 03:45:52 Creating snapshots for the respective volumes
    DEBUG: 2019/11/27 03:45:52 Mirror Count: 1
    DEBUG: 2019/11/27 03:46:04 Creating linked clone volumes from snapshots
    DEBUG: 2019/11/27 03:46:04 Mirror Count: 1
    DEBUG: 2019/11/27 03:46:06 Getting the list of linked clone volumes
    DEBUG: 2019/11/27 03:46:06 Attaching all the linked clone volumes
    DEBUG: 2019/11/27 03:46:48 Calculating sha512sum of linked clone volumes
    DEBUG: 2019/11/27 03:46:49 Calculating cksum for volume lcv-test-vol1 and plex p0
    DEBUG: 2019/11/27 03:46:49 Calculating cksum for volume lcv-test-vol2 and plex p0
    DEBUG: 2019/11/27 03:46:49 Calculating cksum for volume lcv-test-vol3 and plex p0
    DEBUG: 2019/11/27 03:46:49 Calculating cksum for volume lcv-test-vol4 and plex p0
    DEBUG: 2019/11/27 03:46:49 Calculating cksum for volume lcv-test-vol5 and plex p0
    DEBUG: 2019/11/27 03:46:49 Calculating cksum for volume lcv-test-vol6 and plex p0
    DEBUG: 2019/11/27 03:46:50 Calculating cksum for volume lcv-test-vol7 and plex p0
    DEBUG: 2019/11/27 03:46:50 Calculating cksum for volume lcv-test-vol8 and plex p0
    DEBUG: 2019/11/27 03:50:44 Calculating sha512sum of original volumes after creating linked clone volumes
    DEBUG: 2019/11/27 03:50:46 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 03:50:47 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 03:50:49 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 03:50:50 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 03:50:51 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 03:50:53 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 03:50:54 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 03:50:56 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 03:50:57 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 03:50:57 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 03:50:57 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 03:50:57 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 03:50:57 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 03:50:57 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 03:50:57 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 03:50:57 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 03:55:17 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 03:55:20 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 03:55:22 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 03:55:23 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 03:55:24 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 03:55:26 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 03:55:27 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 03:55:29 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 03:55:29 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 03:55:30 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 03:55:30 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 03:55:30 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 03:55:30 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 03:55:30 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 03:55:30 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 03:55:30 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 03:59:25 Comparing checksum of original volumes before & after creating linked clone volumes
    DEBUG: 2019/11/27 03:59:25 checksum of original volumes before and after creating linked clone volumes matched
    DEBUG: 2019/11/27 03:59:25 Comparing checksum of original volumes with the respective linked clone volumes
    DEBUG: 2019/11/27 03:59:25 checksum of original volumes with the respective linked clone volumes matched
    DEBUG: 2019/11/27 03:59:26 Running WRITE fio job on node : appserv54 on the linked clone volumes
    DEBUG: 2019/11/27 03:59:26 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"wxyz\"-12 --verify_interval=4096 --runtime=40 --blocksize=64K --iodepth=16  --time_based  --name=job1 --filename=/dev/nvme9n1 --name=job2 --filename=/dev/nvme10n1 --name=job3 --filename=/dev/nvme11n1 --name=job4 --filename=/dev/nvme12n1 --name=job5 --filename=/dev/nvme13n1 --name=job6 --filename=/dev/nvme14n1 --name=job7 --filename=/dev/nvme15n1 --name=job8 --filename=/dev/nvme16n1
    DEBUG: 2019/11/27 04:00:08 Calculating sha512sum of original volumes after writing on the linked clone volumes
    DEBUG: 2019/11/27 04:00:10 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 04:00:11 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 04:00:12 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 04:00:14 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 04:00:15 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 04:00:17 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 04:00:18 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 04:00:19 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 04:00:20 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 04:00:20 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 04:00:20 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 04:00:21 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 04:00:21 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 04:00:21 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 04:00:21 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 04:00:21 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 04:04:40 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 04:04:41 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 04:04:43 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 04:04:44 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 04:04:46 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 04:04:47 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 04:04:48 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 04:04:50 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 04:04:51 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 04:04:51 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 04:04:51 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 04:04:51 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 04:04:51 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 04:04:51 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 04:04:51 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 04:04:52 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 04:08:46 Calculating sha512sum of linked clone volumes after writing on the linked clone volumes
    DEBUG: 2019/11/27 04:08:47 Calculating cksum for volume lcv-test-vol1 and plex p0
    DEBUG: 2019/11/27 04:08:47 Calculating cksum for volume lcv-test-vol2 and plex p0
    DEBUG: 2019/11/27 04:08:47 Calculating cksum for volume lcv-test-vol3 and plex p0
    DEBUG: 2019/11/27 04:08:47 Calculating cksum for volume lcv-test-vol4 and plex p0
    DEBUG: 2019/11/27 04:08:47 Calculating cksum for volume lcv-test-vol5 and plex p0
    DEBUG: 2019/11/27 04:08:47 Calculating cksum for volume lcv-test-vol6 and plex p0
    DEBUG: 2019/11/27 04:08:48 Calculating cksum for volume lcv-test-vol7 and plex p0
    DEBUG: 2019/11/27 04:08:48 Calculating cksum for volume lcv-test-vol8 and plex p0
    DEBUG: 2019/11/27 04:12:33 Comparing checksum of original volumes before & after writing on linked clone volumes
    DEBUG: 2019/11/27 04:12:33 checksum of original volumes before and after writing on linked clone volumes matched
    DEBUG: 2019/11/27 04:12:33 checksum of linked clone volumes across plexes after writing on linked clone volumes matched
    DEBUG: 2019/11/27 04:12:33 Rebooting the nodes
    DEBUG: 2019/11/27 04:12:33 Getting cluster quorum nodes
    DEBUG: 2019/11/27 04:12:33 Powering OFF the node appserv55
    DEBUG: 2019/11/27 04:12:34 Node 172.16.6.155 took 0 seconds to power off
    DEBUG: 2019/11/27 04:12:34 Ensuring that appserv55 node is unreachable: 
    DEBUG: 2019/11/27 04:12:34 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 04:12:43 Polling to check until node: appserv55 goes down
    DEBUG: 2019/11/27 04:14:12 Powering ON the node appserv55
    DEBUG: 2019/11/27 04:14:13 Node 172.16.6.155 took 1 seconds to power on
    DEBUG: 2019/11/27 04:14:13 Checking if node appserv55 is reachable or not: 
    DEBUG: 2019/11/27 04:14:13 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 04:14:30 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 04:14:47 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 04:15:04 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 04:15:21 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 04:15:38 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 04:15:55 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 04:16:12 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 04:16:29 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 04:16:33 appserv55 is pingable from local machine
    DEBUG: 2019/11/27 04:16:33 Checking ssh port is up or not on node: appserv55
    DEBUG: 2019/11/27 04:17:03 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/27 04:17:03 Found '3' nodes
    DEBUG: 2019/11/27 04:17:03 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 04:17:03 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 04:17:03 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 04:17:58 After power cycle/reboot, updating timestamp of node : appserv55
    DEBUG: 2019/11/27 04:18:00 Getting cluster quorum nodes
    DEBUG: 2019/11/27 04:19:01 Updating inventory struct
    DEBUG: 2019/11/27 04:19:01 Waiting for volumes to come into Attached state after rebooting cluster nodes.
    DEBUG: 2019/11/27 04:19:02 Comparing original volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/27 04:19:07 Comparing LCV's UUID with nvme id-ns for all LCVs
    DEBUG: 2019/11/27 04:19:12 Waiting for Resync to complete on all volumes and LCVs
    DEBUG: 2019/11/27 04:19:12 Number of volumes : 16
    DEBUG: 2019/11/27 04:19:12 Checking resync progress on volume : lcv-test-vol2
    DEBUG: 2019/11/27 04:19:12 Volume name & Plex : lcv-test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:12 All plexes of volume "lcv-test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:12 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/27 04:19:12 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:12 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 04:19:12 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:12 Checking resync progress on volume : lcv-test-vol1
    DEBUG: 2019/11/27 04:19:12 Volume name & Plex : lcv-test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:12 All plexes of volume "lcv-test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:12 Checking resync progress on volume : lcv-test-vol3
    DEBUG: 2019/11/27 04:19:12 Volume name & Plex : lcv-test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:12 All plexes of volume "lcv-test-vol3" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:12 Checking resync progress on volume : lcv-test-vol4
    DEBUG: 2019/11/27 04:19:12 Volume name & Plex : lcv-test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:12 All plexes of volume "lcv-test-vol4" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:12 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:13 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:13 Checking resync progress on volume : lcv-test-vol5
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : lcv-test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 All plexes of volume "lcv-test-vol5" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:13 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:13 Checking resync progress on volume : lcv-test-vol6
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : lcv-test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 All plexes of volume "lcv-test-vol6" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:13 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:13 Checking resync progress on volume : lcv-test-vol7
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : lcv-test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 All plexes of volume "lcv-test-vol7" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:13 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:13 Checking resync progress on volume : lcv-test-vol8
    DEBUG: 2019/11/27 04:19:13 Volume name & Plex : lcv-test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:13 All plexes of volume "lcv-test-vol8" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:13 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/27 04:19:14 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:14 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/27 04:19:14 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:14 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 04:19:14 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 04:19:14 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 04:19:14 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 04:19:17 Calculating sha512sum of original volumes after reboot
    DEBUG: 2019/11/27 04:19:19 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 04:19:21 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 04:19:22 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 04:19:23 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 04:19:25 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 04:19:26 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 04:19:27 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 04:19:29 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 04:19:30 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 04:19:30 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 04:19:30 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 04:19:30 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 04:19:30 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 04:19:30 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 04:19:30 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 04:19:31 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 04:23:47 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 04:23:48 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 04:23:50 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 04:23:51 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 04:23:53 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 04:23:54 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 04:23:55 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 04:23:57 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 04:23:58 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 04:23:58 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 04:23:58 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 04:23:58 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 04:23:58 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 04:23:58 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 04:23:58 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 04:23:58 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 04:27:45 Calculating sha512sum of linked clone volumes after reboot
    DEBUG: 2019/11/27 04:27:47 Calculating cksum for volume lcv-test-vol1 and plex p0
    DEBUG: 2019/11/27 04:27:47 Calculating cksum for volume lcv-test-vol2 and plex p0
    DEBUG: 2019/11/27 04:27:47 Calculating cksum for volume lcv-test-vol3 and plex p0
    DEBUG: 2019/11/27 04:27:47 Calculating cksum for volume lcv-test-vol4 and plex p0
    DEBUG: 2019/11/27 04:27:47 Calculating cksum for volume lcv-test-vol5 and plex p0
    DEBUG: 2019/11/27 04:27:47 Calculating cksum for volume lcv-test-vol6 and plex p0
    DEBUG: 2019/11/27 04:27:47 Calculating cksum for volume lcv-test-vol7 and plex p0
    DEBUG: 2019/11/27 04:27:48 Calculating cksum for volume lcv-test-vol8 and plex p0
    DEBUG: 2019/11/27 04:31:45 Comparing checksum of original volumes before & after reboot
    DEBUG: 2019/11/27 04:31:45 checksum of original volumes before and after reboot matched
    DEBUG: 2019/11/27 04:31:45 Comparing checksum of LCVs before & after reboot
    DEBUG: 2019/11/27 04:31:45 checksum of linked clone volumes before and after reboot matched
    DEBUG: 2019/11/27 04:31:45 Detaching all 8 linked clone volumes
    DEBUG: 2019/11/27 04:31:48 Deleting all 8 linked clone volumes
    DEBUG: 2019/11/27 04:32:54 Detaching few volumes before deleting the snapshots
    DEBUG: 2019/11/27 04:33:05 Deleting all 8 snapshots
    DEBUG: 2019/11/27 04:33:54 Detaching all remaining volumes
    DEBUG: 2019/11/27 04:34:04 Delete all 8 volumes
    DEBUG: 2019/11/27 04:35:25 Removing label from the nodes where the plexes of mirrored volumes and snapshots were scheduled
    DEBUG: 2019/11/27 04:35:25 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 04:35:25 Removed label : mirror from node : appserv55
[AfterEach] reboot a target node after creating snapshot and LCV and verify data is consistent across all plexes after reboot
  /gocode/main/test/e2e/tests/snapshot.go:503
    DEBUG: 2019/11/27 04:35:25 END_TEST Snapshot.RebootOneTarget Time-taken : 3869.64031983
    DEBUG: 2019/11/27 04:35:25 Checking stale resources
    DEBUG: 2019/11/27 04:35:25 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:35:25 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:35:25 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:3869.908 seconds][0m
Snapshot.RebootOneTarget Daily SS_Reboot-1.0
[90m/gocode/main/test/e2e/tests/snapshot.go:488[0m
  reboot a target node after creating snapshot and LCV and verify data is consistent across all plexes after reboot
  [90m/gocode/main/test/e2e/tests/snapshot.go:489[0m
    reboots a target node after creating snapshot and LCV and verifies data consistency across all plexes after reboot
    [90m/gocode/main/test/e2e/tests/snapshot.go:508[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mRbac.ContainerEditView Daily Rbac_Local_Basic-2.1[0m [90mUser can edit/view in it's namespace[0m 
  [1mUser can edit/view container(s) in his namespace[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:890[0m
[BeforeEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:875
    DEBUG: 2019/11/27 04:35:25 START_TEST Rbac.ContainerEditView
    DEBUG: 2019/11/27 04:35:25 Login to cluster
    DEBUG: 2019/11/27 04:35:26 Checking basic Vnic usage
    DEBUG: 2019/11/27 04:35:26 Updating inventory struct
    DEBUG: 2019/11/27 04:35:27 Checking stale resources
    DEBUG: 2019/11/27 04:35:27 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:35:27 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:35:27 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 04:35:36 Creating storage classes
[It] User can edit/view container(s) in his namespace
  /gocode/main/test/e2e/tests/rbac.go:890
    DEBUG: 2019/11/27 04:35:48 Creating group, user with role(s)
    DEBUG: 2019/11/27 04:35:48 Creating group mygroup1 with container-edit/user1sns role(s)
    DEBUG: 2019/11/27 04:35:48 Creating myuser1 user in mygroup1 group
    DEBUG: 2019/11/27 04:35:49 Login as myuser1 user in user1sns namespace
    DEBUG: 2019/11/27 04:36:06 Creating group mygroup2 with volume-edit,volumeclaim-edit/user1sns role(s)
    DEBUG: 2019/11/27 04:36:06 Creating myuser2 user in mygroup2 group
    DEBUG: 2019/11/27 04:36:06 Login as myuser2 user in user1sns namespace
    DEBUG: 2019/11/27 04:36:22 Create volume
    DEBUG: 2019/11/27 04:36:22 Create pvc
    DEBUG: 2019/11/27 04:36:23 Created PVC successfully.
    DEBUG: 2019/11/27 04:36:23 Login as myuser1 user in user1sns namespace
    DEBUG: 2019/11/27 04:36:38 Creating a pair of iperf client-server pod.
    DEBUG: 2019/11/27 04:36:38 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/27 04:36:39 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/27 04:37:09 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/27 04:37:09 Checking if given pods are in Running state
    DEBUG: 2019/11/27 04:37:10 Checking if given pods are in Running state
    DEBUG: 2019/11/27 04:37:12 Creating fio pod fio-pod in user1sns namespace
    DEBUG: 2019/11/27 04:37:23 Listing pods
    DEBUG: 2019/11/27 04:37:24 List PVCs
    DEBUG: 2019/11/27 04:37:24 List Endpoints
    DEBUG: 2019/11/27 04:37:24 List Services
    DEBUG: 2019/11/27 04:37:24 Editing group role(s)
    DEBUG: 2019/11/27 04:37:24 Editing mygroup1 group with container-view/user1sns role(s)
    DEBUG: 2019/11/27 04:37:25 Login as myuser1 user in user1sns namespace
    DEBUG: 2019/11/27 04:37:40 List pods
    DEBUG: 2019/11/27 04:37:40 List PVCs: 
    DEBUG: 2019/11/27 04:37:41 List Endpoints
    DEBUG: 2019/11/27 04:37:41 List Services: 
    DEBUG: 2019/11/27 04:37:41 Try to create the pod with container-view role
    DEBUG: 2019/11/27 04:37:41 Creating 1 pods of docker.io/redis:3.0.5 image with network : default and qos : high
    DEBUG: 2019/11/27 04:37:41 Deleting pods : 
    DEBUG: 2019/11/27 04:37:41 Editing group role(s)
    DEBUG: 2019/11/27 04:37:42 Editing mygroup1 group with container-edit/user1sns role(s)
    DEBUG: 2019/11/27 04:37:42 Login as myuser1 user in user1sns namespace
    DEBUG: 2019/11/27 04:37:58 Delete pods
    DEBUG: 2019/11/27 04:38:08 Delete services in user1sns namespace
    DEBUG: 2019/11/27 04:38:08 List Endpoints in user1sns namespace
    DEBUG: 2019/11/27 04:38:09 Login as myuser2 user
    DEBUG: 2019/11/27 04:38:24 Delete PVC
    DEBUG: 2019/11/27 04:38:24 Wait for volumes to come in Available state
    DEBUG: 2019/11/27 04:38:25 Delete Volume
[AfterEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:884
    DEBUG: 2019/11/27 04:39:26 END_TEST Rbac.ContainerEditView Time-taken : 240.597692475
    DEBUG: 2019/11/27 04:39:26 Checking stale resources
    DEBUG: 2019/11/27 04:39:26 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 04:39:26 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:39:26 Checking stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:240.849 seconds][0m
Rbac.ContainerEditView Daily Rbac_Local_Basic-2.1
[90m/gocode/main/test/e2e/tests/rbac.go:861[0m
  User can edit/view in it's namespace
  [90m/gocode/main/test/e2e/tests/rbac.go:863[0m
    User can edit/view container(s) in his namespace
    [90m/gocode/main/test/e2e/tests/rbac.go:890[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mCluster.FailMaster Management Daily M_Cluster-1.14[0m [90mwhen the master node fails in a cluster master failover should happen[0m 
  [1mMater failover should happen[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:288[0m
[BeforeEach] when the master node fails in a cluster master failover should happen
  /gocode/main/test/e2e/tests/cluster.go:273
    DEBUG: 2019/11/27 04:39:26 START_TEST Cluster.FailMaster
    DEBUG: 2019/11/27 04:39:26 Login to cluster
    DEBUG: 2019/11/27 04:39:27 Checking basic Vnic usage
    DEBUG: 2019/11/27 04:39:27 Updating inventory struct
    DEBUG: 2019/11/27 04:39:28 Checking stale resources
    DEBUG: 2019/11/27 04:39:28 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:39:28 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 04:39:28 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:39:37 Creating storage classes
[It] Mater failover should happen
  /gocode/main/test/e2e/tests/cluster.go:288
    DEBUG: 2019/11/27 04:39:46 Getting master
    DEBUG: 2019/11/27 04:39:46 Rebooting/Halting master appserv53
    DEBUG: 2019/11/27 04:39:46 Doing sync on 172.16.6.153
    DEBUG: 2019/11/27 04:39:46 Getting cluster quorum nodes
    DEBUG: 2019/11/27 04:39:46 Powering OFF the node appserv53
    DEBUG: 2019/11/27 04:39:46 Node 172.16.6.153 took 0 seconds to power off
    DEBUG: 2019/11/27 04:39:46 Ensuring that appserv53 node is unreachable: 
    DEBUG: 2019/11/27 04:39:46 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 04:39:55 Polling to check until node: appserv53 goes down
    DEBUG: 2019/11/27 04:40:26 Error: . Retrying once again...
    DEBUG: 2019/11/27 04:42:25 Looking for a new master
    DEBUG: 2019/11/27 04:42:28 Getting the new master
    DEBUG: 2019/11/27 04:42:31 Powering on old master node appserv53
    DEBUG: 2019/11/27 04:42:31 Powering ON the node appserv53
    DEBUG: 2019/11/27 04:42:32 Node 172.16.6.153 took 1 seconds to power on
    DEBUG: 2019/11/27 04:42:32 Checking if node appserv53 is reachable or not: 
    DEBUG: 2019/11/27 04:42:32 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 04:42:51 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 04:43:08 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 04:43:25 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 04:43:42 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 04:43:59 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 04:44:16 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 04:44:33 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 04:44:37 appserv53 is pingable from local machine
    DEBUG: 2019/11/27 04:44:37 Checking ssh port is up or not on node: appserv53
    DEBUG: 2019/11/27 04:45:17 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/27 04:45:17 Found '3' nodes
    DEBUG: 2019/11/27 04:45:17 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 04:45:58 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 04:45:58 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 04:46:08 After power cycle/reboot, updating timestamp of node : appserv53
    DEBUG: 2019/11/27 04:46:11 Getting cluster quorum nodes
    DEBUG: 2019/11/27 04:47:11 Updating inventory struct
    DEBUG: 2019/11/27 04:47:12 Checking sanity for appserv53
    DEBUG: 2019/11/27 04:47:12 Waiting for node and armada to come up
    DEBUG: 2019/11/27 04:47:12 Found '3' nodes
    DEBUG: 2019/11/27 04:47:12 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 04:47:12 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 04:47:12 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 04:47:13 Checking that mastership did not change from appserv55
[AfterEach] when the master node fails in a cluster master failover should happen
  /gocode/main/test/e2e/tests/cluster.go:283
    DEBUG: 2019/11/27 04:47:13 END_TEST Cluster.FailMaster Time-taken : 466.416816235
    DEBUG: 2019/11/27 04:47:13 Checking stale resources
    DEBUG: 2019/11/27 04:47:13 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:47:13 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:47:13 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:466.660 seconds][0m
Cluster.FailMaster Management Daily M_Cluster-1.14
[90m/gocode/main/test/e2e/tests/cluster.go:266[0m
  when the master node fails in a cluster master failover should happen
  [90m/gocode/main/test/e2e/tests/cluster.go:268[0m
    Mater failover should happen
    [90m/gocode/main/test/e2e/tests/cluster.go:288[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries creating qos template with invalid value for max bandwidth.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:218[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/27 04:47:13 Login to cluster
    DEBUG: 2019/11/27 04:47:14 Checking basic Vnic usage
    DEBUG: 2019/11/27 04:47:14 Updating inventory struct
    DEBUG: 2019/11/27 04:47:15 Checking stale resources
    DEBUG: 2019/11/27 04:47:15 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:47:15 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:47:15 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 04:47:23 Creating storage classes
    DEBUG: 2019/11/27 04:47:33 START_TEST PerfTier.NegativeTests
[It] tries creating qos template with invalid value for max bandwidth.
  /gocode/main/test/e2e/tests/perf-tier.go:218
    ERROR: 2019/11/27 04:47:33  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b 1G -i 50k -B invalidBW@3!', output:, error:Error: Invalid --max-network-bw/-B specification. Input should be in K, M, G format



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/27 04:47:33 END_TEST PerfTier.NegativeTests Time-taken: 0.068345573
    DEBUG: 2019/11/27 04:47:33 Checking stale resources
    DEBUG: 2019/11/27 04:47:33 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:47:33 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:47:33 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:20.555 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries creating qos template with invalid value for max bandwidth.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:218[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.User Daily Rbac_User-1.0 Rbac_User-1.1[0m [90mrbac user test[0m 
  [1muser-edit test[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:141[0m
[BeforeEach] rbac user test
  /gocode/main/test/e2e/tests/rbac.go:82
    DEBUG: 2019/11/27 04:47:34 START_TEST Rbac.User
    DEBUG: 2019/11/27 04:47:34 Login to cluster
    DEBUG: 2019/11/27 04:47:34 Checking basic Vnic usage
    DEBUG: 2019/11/27 04:47:34 Updating inventory struct
    DEBUG: 2019/11/27 04:47:35 Checking stale resources
    DEBUG: 2019/11/27 04:47:35 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:47:35 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:47:35 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 04:47:44 Creating storage classes
[It] user-edit test
  /gocode/main/test/e2e/tests/rbac.go:141
    DEBUG: 2019/11/27 04:47:54 Create group
    DEBUG: 2019/11/27 04:47:54 Create user
    DEBUG: 2019/11/27 04:47:54 Login as user
    DEBUG: 2019/11/27 04:47:55 Create user
    DEBUG: 2019/11/27 04:47:55 Create group
    DEBUG: 2019/11/27 04:47:55 Create auth-server
    DEBUG: 2019/11/27 04:47:55 List users
    DEBUG: 2019/11/27 04:47:55 List groups
    DEBUG: 2019/11/27 04:47:55 List roles
    DEBUG: 2019/11/27 04:47:55 List auth-server
    DEBUG: 2019/11/27 04:47:55 Create Network
    DEBUG: 2019/11/27 04:47:56 Negative group reference count
    DEBUG: 2019/11/27 04:47:56 Positive group reference count
[AfterEach] rbac user test
  /gocode/main/test/e2e/tests/rbac.go:90
    DEBUG: 2019/11/27 04:47:58 END_TEST Rbac.User Time-taken : 24.35722954
    DEBUG: 2019/11/27 04:47:58 Checking stale resources
    DEBUG: 2019/11/27 04:47:58 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:47:58 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:47:58 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:24.596 seconds][0m
Rbac.User Daily Rbac_User-1.0 Rbac_User-1.1
[90m/gocode/main/test/e2e/tests/rbac.go:74[0m
  rbac user test
  [90m/gocode/main/test/e2e/tests/rbac.go:77[0m
    user-edit test
    [90m/gocode/main/test/e2e/tests/rbac.go:141[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries creating qos temaplate with min bw > max bw.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:211[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/27 04:47:58 Login to cluster
    DEBUG: 2019/11/27 04:47:59 Checking basic Vnic usage
    DEBUG: 2019/11/27 04:47:59 Updating inventory struct
    DEBUG: 2019/11/27 04:48:00 Checking stale resources
    DEBUG: 2019/11/27 04:48:00 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:48:00 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 04:48:00 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:48:08 Creating storage classes
    DEBUG: 2019/11/27 04:48:17 START_TEST PerfTier.NegativeTests
[It] tries creating qos temaplate with min bw > max bw.
  /gocode/main/test/e2e/tests/perf-tier.go:211
    DEBUG: 2019/11/27 04:48:17 Try creating qos template with min bw > max bw.
    ERROR: 2019/11/27 04:48:17  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b 5G -B 1G -i 50k', status:&{{Status } {  0} Failure Performance tier validation failed, error: [spec.net-bw: Invalid value: 1000000000: Maximum network bandwidth cannot be less than minimum network bandwidth] BadRequest <nil> 400}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "Performance tier validation failed, error: [spec.net-bw: Invalid value: 1000000000: Maximum network bandwidth cannot be less than minimum network bandwidth]",
 "reason": "BadRequest",
 "code": 400
}



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/27 04:48:17 END_TEST PerfTier.NegativeTests Time-taken: 0.111725337
    DEBUG: 2019/11/27 04:48:17 Checking stale resources
    DEBUG: 2019/11/27 04:48:17 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:48:17 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:48:17 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.426 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries creating qos temaplate with min bw > max bw.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:211[0m
[90m------------------------------[0m
[0mSnapshot.Basic Daily SS_Basic-1.0[0m [90mBasic test for snapshot for verifying data integrity[0m 
  [1mchecking data integrity of volume and it's snapshot[0m
  [37m/gocode/main/test/e2e/tests/snapshot.go:139[0m
[BeforeEach] Basic test for snapshot for verifying data integrity
  /gocode/main/test/e2e/tests/snapshot.go:125
    DEBUG: 2019/11/27 04:48:18 START_TEST Snapshot.Basic
    DEBUG: 2019/11/27 04:48:18 Login to cluster
    DEBUG: 2019/11/27 04:48:18 Checking basic Vnic usage
    DEBUG: 2019/11/27 04:48:18 Updating inventory struct
    DEBUG: 2019/11/27 04:48:19 Checking stale resources
    DEBUG: 2019/11/27 04:48:19 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 04:48:19 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 04:48:19 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 04:48:28 Creating storage classes
[It] checking data integrity of volume and it's snapshot
  /gocode/main/test/e2e/tests/snapshot.go:139
    DEBUG: 2019/11/27 04:48:37 Mirror count not set in e2e_param.json
    DEBUG: 2019/11/27 04:48:37 Selecting mirror count as : 3
    DEBUG: 2019/11/27 04:48:37 Assigning label to nodes where the plexes of mirrored volumes should get scheduled
    DEBUG: 2019/11/27 04:48:37 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 04:48:37 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 04:48:37 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 04:48:37 Creating 8 volumes of random sizes
    DEBUG: 2019/11/27 04:48:37 Mirror Count: 3
    DEBUG: 2019/11/27 04:48:39 Attaching all 8 volumes
    DEBUG: 2019/11/27 04:49:23 Running WRITE fio job on node : appserv55
    DEBUG: 2019/11/27 04:49:23 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"efgh\"-12 --verify_interval=4096 --runtime=90 --blocksize=512K --iodepth=8  --time_based  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/27 04:50:55 Calculating sha512sum of volumes before creating snapshots
    DEBUG: 2019/11/27 04:50:57 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 04:50:58 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 04:51:00 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 04:51:01 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 04:51:03 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 04:51:04 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 04:51:05 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 04:51:07 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 04:51:08 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 04:51:08 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 04:51:08 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 04:51:08 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 04:51:08 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 04:51:08 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 04:51:08 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 04:51:09 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 04:52:28 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 04:52:30 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 04:52:31 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 04:52:33 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 04:52:34 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 04:52:35 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 04:52:37 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 04:52:38 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 04:52:39 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 04:52:39 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 04:52:39 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 04:52:39 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 04:52:39 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 04:52:40 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 04:52:40 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 04:52:40 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 04:53:55 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 04:53:57 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 04:53:58 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 04:54:00 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/27 04:54:01 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 04:54:02 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/27 04:54:04 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/27 04:54:05 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/27 04:54:06 Calculating cksum for volume test-vol1 and plex p2
    DEBUG: 2019/11/27 04:54:06 Calculating cksum for volume test-vol2 and plex p2
    DEBUG: 2019/11/27 04:54:06 Calculating cksum for volume test-vol3 and plex p2
    DEBUG: 2019/11/27 04:54:06 Calculating cksum for volume test-vol4 and plex p2
    DEBUG: 2019/11/27 04:54:06 Calculating cksum for volume test-vol5 and plex p2
    DEBUG: 2019/11/27 04:54:07 Calculating cksum for volume test-vol6 and plex p2
    DEBUG: 2019/11/27 04:54:07 Calculating cksum for volume test-vol7 and plex p2
    DEBUG: 2019/11/27 04:54:07 Calculating cksum for volume test-vol8 and plex p2
    DEBUG: 2019/11/27 04:55:50 Creating snapshots for the respective volumes
    DEBUG: 2019/11/27 04:55:50 Mirror Count: 1
    DEBUG: 2019/11/27 04:56:02 Creating linked clone volumes from snapshots
    DEBUG: 2019/11/27 04:56:02 Mirror Count: 1
    DEBUG: 2019/11/27 04:56:04 Getting the list of linked clone volumes
    DEBUG: 2019/11/27 04:56:04 Attaching all the linked clone volumes
    DEBUG: 2019/11/27 04:56:47 Calculating sha512sum of linked clone volumes
    DEBUG: 2019/11/27 04:56:48 Calculating cksum for volume lcv-test-vol1 and plex p0
    DEBUG: 2019/11/27 04:56:48 Calculating cksum for volume lcv-test-vol2 and plex p0
    DEBUG: 2019/11/27 04:56:48 Calculating cksum for volume lcv-test-vol3 and plex p0
    DEBUG: 2019/11/27 04:56:48 Calculating cksum for volume lcv-test-vol4 and plex p0
    DEBUG: 2019/11/27 04:56:48 Calculating cksum for volume lcv-test-vol5 and plex p0
    DEBUG: 2019/11/27 04:56:49 Calculating cksum for volume lcv-test-vol6 and plex p0
    DEBUG: 2019/11/27 04:56:49 Calculating cksum for volume lcv-test-vol7 and plex p0
    DEBUG: 2019/11/27 04:56:49 Calculating cksum for volume lcv-test-vol8 and plex p0
    DEBUG: 2019/11/27 04:58:02 Calculating sha512sum of original volumes after creating linked clone volumes
    DEBUG: 2019/11/27 04:58:04 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 04:58:05 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 04:58:06 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 04:58:08 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 04:58:09 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 04:58:11 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 04:58:12 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 04:58:13 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 04:58:14 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 04:58:14 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 04:58:15 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 04:58:15 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 04:58:15 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 04:58:15 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 04:58:15 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 04:58:15 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 04:59:33 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 04:59:35 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 04:59:36 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 04:59:38 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 04:59:39 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 04:59:40 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 04:59:42 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 04:59:43 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 04:59:44 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 04:59:44 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 04:59:44 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 04:59:44 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 04:59:44 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 04:59:45 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 04:59:45 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 04:59:45 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 05:01:02 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 05:01:04 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 05:01:05 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 05:01:06 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/27 05:01:08 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 05:01:09 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/27 05:01:11 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/27 05:01:12 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/27 05:01:13 Calculating cksum for volume test-vol1 and plex p2
    DEBUG: 2019/11/27 05:01:13 Calculating cksum for volume test-vol2 and plex p2
    DEBUG: 2019/11/27 05:01:13 Calculating cksum for volume test-vol3 and plex p2
    DEBUG: 2019/11/27 05:01:13 Calculating cksum for volume test-vol4 and plex p2
    DEBUG: 2019/11/27 05:01:13 Calculating cksum for volume test-vol5 and plex p2
    DEBUG: 2019/11/27 05:01:13 Calculating cksum for volume test-vol6 and plex p2
    DEBUG: 2019/11/27 05:01:14 Calculating cksum for volume test-vol7 and plex p2
    DEBUG: 2019/11/27 05:01:14 Calculating cksum for volume test-vol8 and plex p2
    DEBUG: 2019/11/27 05:02:56 Comparing checksum of original volumes before & after creating linked clone volumes
    DEBUG: 2019/11/27 05:02:56 checksum of original volumes before and after creating linked clone volumes matched
    DEBUG: 2019/11/27 05:02:56 Comparing checksum of original volumes with the respective linked clone volumes
    DEBUG: 2019/11/27 05:02:56 checksum of original volumes with the respective linked clone volumes matched
    DEBUG: 2019/11/27 05:02:56 Running WRITE fio job on node : appserv55 for the second time on the original volumes
    DEBUG: 2019/11/27 05:02:56 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-12 --verify_interval=4096 --runtime=60 --blocksize=512K --iodepth=8  --time_based  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1
    DEBUG: 2019/11/27 05:03:58 Calculating sha512sum of original volumes before writing on the linked clone volumes
    DEBUG: 2019/11/27 05:04:00 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 05:04:02 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 05:04:03 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 05:04:04 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 05:04:06 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 05:04:07 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 05:04:09 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 05:04:10 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 05:04:11 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 05:04:11 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 05:04:11 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 05:04:11 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 05:04:11 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 05:04:12 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 05:04:12 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 05:04:12 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 05:05:31 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 05:05:32 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 05:05:33 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 05:05:35 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 05:05:36 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 05:05:37 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 05:05:39 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 05:05:40 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 05:05:41 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 05:05:41 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 05:05:41 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 05:05:42 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 05:05:42 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 05:05:42 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 05:05:42 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 05:05:42 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 05:06:57 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 05:06:58 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 05:07:00 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 05:07:02 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/27 05:07:03 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 05:07:04 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/27 05:07:06 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/27 05:07:07 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/27 05:07:08 Calculating cksum for volume test-vol1 and plex p2
    DEBUG: 2019/11/27 05:07:08 Calculating cksum for volume test-vol2 and plex p2
    DEBUG: 2019/11/27 05:07:08 Calculating cksum for volume test-vol3 and plex p2
    DEBUG: 2019/11/27 05:07:09 Calculating cksum for volume test-vol4 and plex p2
    DEBUG: 2019/11/27 05:07:09 Calculating cksum for volume test-vol5 and plex p2
    DEBUG: 2019/11/27 05:07:09 Calculating cksum for volume test-vol6 and plex p2
    DEBUG: 2019/11/27 05:07:09 Calculating cksum for volume test-vol7 and plex p2
    DEBUG: 2019/11/27 05:07:09 Calculating cksum for volume test-vol8 and plex p2
    DEBUG: 2019/11/27 05:08:51 Calculating sha512sum of linked clone volumes before writing on the linked clone volumes
    DEBUG: 2019/11/27 05:08:52 Calculating cksum for volume lcv-test-vol1 and plex p0
    DEBUG: 2019/11/27 05:08:53 Calculating cksum for volume lcv-test-vol2 and plex p0
    DEBUG: 2019/11/27 05:08:53 Calculating cksum for volume lcv-test-vol3 and plex p0
    DEBUG: 2019/11/27 05:08:53 Calculating cksum for volume lcv-test-vol4 and plex p0
    DEBUG: 2019/11/27 05:08:53 Calculating cksum for volume lcv-test-vol5 and plex p0
    DEBUG: 2019/11/27 05:08:53 Calculating cksum for volume lcv-test-vol6 and plex p0
    DEBUG: 2019/11/27 05:08:53 Calculating cksum for volume lcv-test-vol7 and plex p0
    DEBUG: 2019/11/27 05:08:53 Calculating cksum for volume lcv-test-vol8 and plex p0
    DEBUG: 2019/11/27 05:10:08 checksum of original volumes across plexes after running fio for 2nd time matched
    DEBUG: 2019/11/27 05:10:08 Comparing checksum of linked clone volumes before and after running fio for 2nd time on the original volumes
    DEBUG: 2019/11/27 05:10:08 checksum of linked clone volumes before and after running fio for 2nd time on the original volumes matched
    DEBUG: 2019/11/27 05:10:09 Running WRITE fio job on node : appserv55 on the linked clone volumes
    DEBUG: 2019/11/27 05:10:09 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"wxyz\"-12 --verify_interval=4096 --runtime=40 --blocksize=512K --iodepth=8  --time_based  --name=job1 --filename=/dev/nvme9n1 --name=job2 --filename=/dev/nvme10n1 --name=job3 --filename=/dev/nvme11n1 --name=job4 --filename=/dev/nvme12n1 --name=job5 --filename=/dev/nvme13n1 --name=job6 --filename=/dev/nvme14n1 --name=job7 --filename=/dev/nvme15n1 --name=job8 --filename=/dev/nvme16n1
    DEBUG: 2019/11/27 05:10:51 Calculating sha512sum of original volumes after writing on the linked clone volumes
    DEBUG: 2019/11/27 05:10:52 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 05:10:54 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 05:10:55 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 05:10:57 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 05:10:58 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 05:11:00 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 05:11:01 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 05:11:02 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 05:11:03 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 05:11:03 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 05:11:04 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 05:11:04 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 05:11:04 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 05:11:04 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 05:11:04 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 05:11:04 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 05:12:22 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 05:12:23 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 05:12:25 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 05:12:26 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 05:12:28 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 05:12:29 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 05:12:31 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 05:12:32 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 05:12:33 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 05:12:33 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 05:12:33 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 05:12:33 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 05:12:33 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 05:12:33 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 05:12:34 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 05:12:34 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 05:13:52 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 05:13:54 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 05:13:55 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 05:13:57 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/27 05:13:58 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 05:14:00 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/27 05:14:01 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/27 05:14:02 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/27 05:14:03 Calculating cksum for volume test-vol1 and plex p2
    DEBUG: 2019/11/27 05:14:03 Calculating cksum for volume test-vol2 and plex p2
    DEBUG: 2019/11/27 05:14:04 Calculating cksum for volume test-vol3 and plex p2
    DEBUG: 2019/11/27 05:14:04 Calculating cksum for volume test-vol4 and plex p2
    DEBUG: 2019/11/27 05:14:04 Calculating cksum for volume test-vol5 and plex p2
    DEBUG: 2019/11/27 05:14:04 Calculating cksum for volume test-vol6 and plex p2
    DEBUG: 2019/11/27 05:14:04 Calculating cksum for volume test-vol7 and plex p2
    DEBUG: 2019/11/27 05:14:04 Calculating cksum for volume test-vol8 and plex p2
    DEBUG: 2019/11/27 05:15:47 Calculating sha512sum of linked clone volumes after writing on the linked clone volumes
    DEBUG: 2019/11/27 05:15:48 Calculating cksum for volume lcv-test-vol1 and plex p0
    DEBUG: 2019/11/27 05:15:48 Calculating cksum for volume lcv-test-vol2 and plex p0
    DEBUG: 2019/11/27 05:15:48 Calculating cksum for volume lcv-test-vol3 and plex p0
    DEBUG: 2019/11/27 05:15:48 Calculating cksum for volume lcv-test-vol4 and plex p0
    DEBUG: 2019/11/27 05:15:48 Calculating cksum for volume lcv-test-vol5 and plex p0
    DEBUG: 2019/11/27 05:15:48 Calculating cksum for volume lcv-test-vol6 and plex p0
    DEBUG: 2019/11/27 05:15:49 Calculating cksum for volume lcv-test-vol7 and plex p0
    DEBUG: 2019/11/27 05:15:49 Calculating cksum for volume lcv-test-vol8 and plex p0
    DEBUG: 2019/11/27 05:17:01 Comparing checksum of original volumes before & after writing on linked clone volumes
    DEBUG: 2019/11/27 05:17:01 checksum of original volumes before and after writing on linked clone volumes matched
    DEBUG: 2019/11/27 05:17:01 checksum of linked clone volumes across plexes after writing on linked clone volumes matched
    DEBUG: 2019/11/27 05:17:01 Detaching all 8 linked clone volumes
    DEBUG: 2019/11/27 05:17:05 Deleting all 8 linked clone volumes
    DEBUG: 2019/11/27 05:17:56 Detaching few volumes before deleting the snapshots
    DEBUG: 2019/11/27 05:18:07 Deleting all 8 snapshots
    DEBUG: 2019/11/27 05:18:54 Detaching all remaining volumes
    DEBUG: 2019/11/27 05:19:05 Attaching all 8 volumes again
    DEBUG: 2019/11/27 05:19:49 Calculating sha512sum of volumes after deleting their snapshots
    DEBUG: 2019/11/27 05:19:51 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 05:19:53 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 05:19:54 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 05:19:56 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 05:19:57 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 05:19:58 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 05:20:00 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 05:20:01 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 05:20:02 Calculating cksum for volume test-vol1 and plex p0
    DEBUG: 2019/11/27 05:20:02 Calculating cksum for volume test-vol2 and plex p0
    DEBUG: 2019/11/27 05:20:02 Calculating cksum for volume test-vol3 and plex p0
    DEBUG: 2019/11/27 05:20:02 Calculating cksum for volume test-vol4 and plex p0
    DEBUG: 2019/11/27 05:20:03 Calculating cksum for volume test-vol5 and plex p0
    DEBUG: 2019/11/27 05:20:03 Calculating cksum for volume test-vol6 and plex p0
    DEBUG: 2019/11/27 05:20:03 Calculating cksum for volume test-vol7 and plex p0
    DEBUG: 2019/11/27 05:20:03 Calculating cksum for volume test-vol8 and plex p0
    DEBUG: 2019/11/27 05:21:21 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 05:21:23 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 05:21:24 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 05:21:25 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 05:21:27 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 05:21:28 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 05:21:30 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 05:21:31 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 05:21:32 Calculating cksum for volume test-vol1 and plex p1
    DEBUG: 2019/11/27 05:21:32 Calculating cksum for volume test-vol2 and plex p1
    DEBUG: 2019/11/27 05:21:32 Calculating cksum for volume test-vol3 and plex p1
    DEBUG: 2019/11/27 05:21:32 Calculating cksum for volume test-vol4 and plex p1
    DEBUG: 2019/11/27 05:21:32 Calculating cksum for volume test-vol5 and plex p1
    DEBUG: 2019/11/27 05:21:33 Calculating cksum for volume test-vol6 and plex p1
    DEBUG: 2019/11/27 05:21:33 Calculating cksum for volume test-vol7 and plex p1
    DEBUG: 2019/11/27 05:21:33 Calculating cksum for volume test-vol8 and plex p1
    DEBUG: 2019/11/27 05:22:49 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 05:22:51 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 05:22:52 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 05:22:54 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/27 05:22:55 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 05:22:56 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/27 05:22:58 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/27 05:22:59 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/27 05:23:00 Calculating cksum for volume test-vol1 and plex p2
    DEBUG: 2019/11/27 05:23:00 Calculating cksum for volume test-vol2 and plex p2
    DEBUG: 2019/11/27 05:23:00 Calculating cksum for volume test-vol3 and plex p2
    DEBUG: 2019/11/27 05:23:00 Calculating cksum for volume test-vol4 and plex p2
    DEBUG: 2019/11/27 05:23:01 Calculating cksum for volume test-vol5 and plex p2
    DEBUG: 2019/11/27 05:23:01 Calculating cksum for volume test-vol6 and plex p2
    DEBUG: 2019/11/27 05:23:01 Calculating cksum for volume test-vol7 and plex p2
    DEBUG: 2019/11/27 05:23:01 Calculating cksum for volume test-vol8 and plex p2
    DEBUG: 2019/11/27 05:24:43 Comparing checksum of original volumes before & after deleting snapshots
    DEBUG: 2019/11/27 05:24:43 checksum of original volumes before & after deleting snapshots matched
    DEBUG: 2019/11/27 05:24:43 Detaching all 8 volumes
    DEBUG: 2019/11/27 05:24:48 Delete all 8 volumes
    DEBUG: 2019/11/27 05:25:24 Removing label from the nodes where the plexes of mirrored volumes and snapshots were scheduled
    DEBUG: 2019/11/27 05:25:24 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 05:25:25 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 05:25:25 Removed label : mirror from node : appserv54
[AfterEach] Basic test for snapshot for verifying data integrity
  /gocode/main/test/e2e/tests/snapshot.go:134
    DEBUG: 2019/11/27 05:25:25 END_TEST Snapshot.Basic Time-taken : 2227.133526783
    DEBUG: 2019/11/27 05:25:25 Checking stale resources
    DEBUG: 2019/11/27 05:25:25 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 05:25:25 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 05:25:25 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:2227.382 seconds][0m
Snapshot.Basic Daily SS_Basic-1.0
[90m/gocode/main/test/e2e/tests/snapshot.go:102[0m
  Basic test for snapshot for verifying data integrity
  [90m/gocode/main/test/e2e/tests/snapshot.go:120[0m
    checking data integrity of volume and it's snapshot
    [90m/gocode/main/test/e2e/tests/snapshot.go:139[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14[0m [90mCheck if a pod's IP is pingable from other pod[0m 
  [1mCheck if a pod's IP is pingable from other pod using a network having invalid VLAN[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:899[0m
[BeforeEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:848
    DEBUG: 2019/11/27 05:25:25 START_TEST Network.PingBetweenTwoPods
    DEBUG: 2019/11/27 05:25:25 Login to cluster
    DEBUG: 2019/11/27 05:25:25 Checking basic Vnic usage
    DEBUG: 2019/11/27 05:25:26 Updating inventory struct
    DEBUG: 2019/11/27 05:25:27 Checking stale resources
    DEBUG: 2019/11/27 05:25:27 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 05:25:27 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 05:25:27 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 05:25:35 Creating storage classes
[It] Check if a pod's IP is pingable from other pod using a network having invalid VLAN
  /gocode/main/test/e2e/tests/network-pod.go:899
    DEBUG: 2019/11/27 05:25:45 Creating test network : testnetwork with invalid vlan
    DEBUG: 2019/11/27 05:25:45 Creating 2 pods of docker.io/redis:3.0.5 image with network : testnetwork and qos : high
    DEBUG: 2019/11/27 05:25:51 IP address ( 56.12.100.2 ) of test-pod-1 is between 56.12.100.2 and 56.12.100.254

    DEBUG: 2019/11/27 05:25:51 IP address ( 56.12.100.3 ) of test-pod-2 is between 56.12.100.2 and 56.12.100.254

    DEBUG: 2019/11/27 05:25:51 Trying to ping the test-pod-2's IP from pod test-pod-1
    DEBUG: 2019/11/27 05:25:55 Output : PING 56.12.100.3 (56.12.100.3): 48 data bytes
....60 bytes from test-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 7350   0 0040  40  01 20b2 56.12.100.2  56.12.100.3 
60 bytes from test-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 7550   0 0040  40  01 1eb2 56.12.100.2  56.12.100.3 
60 bytes from test-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 7850   0 0040  40  01 1bb2 56.12.100.2  56.12.100.3 
60 bytes from test-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 7c50   0 0040  40  01 17b2 56.12.100.2  56.12.100.3 
60 bytes from test-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 7f50   0 0040  40  01 14b2 56.12.100.2  56.12.100.3 
--- 56.12.100.3 ping statistics ---
5 packets transmitted, 0 packets received, 100% packet loss
, Error : failed to run commmand 'kubectl exec test-pod-1 -- ping -f -c 5 -W 5 56.12.100.3', output:PING 56.12.100.3 (56.12.100.3): 48 data bytes
....60 bytes from test-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 7350   0 0040  40  01 20b2 56.12.100.2  56.12.100.3 
60 bytes from test-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 7550   0 0040  40  01 1eb2 56.12.100.2  56.12.100.3 
60 bytes from test-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 7850   0 0040  40  01 1bb2 56.12.100.2  56.12.100.3 
60 bytes from test-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 7c50   0 0040  40  01 17b2 56.12.100.2  56.12.100.3 
60 bytes from test-pod-1 (56.12.100.2): Destination Host Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  00 4c00 7f50   0 0040  40  01 14b2 56.12.100.2  56.12.100.3 
--- 56.12.100.3 ping statistics ---
5 packets transmitted, 0 packets received, 100% packet loss
, error:command terminated with exit code 1

, command terminated with exit code 1

    DEBUG: 2019/11/27 05:25:55 Deleting pods : 
    DEBUG: 2019/11/27 05:26:08 Deleting test network : testnetwork
[AfterEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:857
    DEBUG: 2019/11/27 05:26:08 END_TEST Network.PingBetweenTwoPods Time-taken : 43.357871455
    DEBUG: 2019/11/27 05:26:08 Checking stale resources
    DEBUG: 2019/11/27 05:26:08 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 05:26:08 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 05:26:08 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:43.601 seconds][0m
Network.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14
[90m/gocode/main/test/e2e/tests/network-pod.go:842[0m
  Check if a pod's IP is pingable from other pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:843[0m
    Check if a pod's IP is pingable from other pod using a network having invalid VLAN
    [90m/gocode/main/test/e2e/tests/network-pod.go:899[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mLocalStorage.ConfigStress Daily S_Stress-1.1 S_Stress-1.2 S_Stress-1.3 S_Stress-1.4 S_Basic-1.3[0m [90mcreate maximum volumes and run attach detach delete in a loop[0m 
  [1mcreates maximum volumes and runs attach detach delete in a loop[0m
  [37m/gocode/main/test/e2e/tests/volume.go:2259[0m
[BeforeEach] create maximum volumes and run attach detach delete in a loop
  /gocode/main/test/e2e/tests/volume.go:2247
    DEBUG: 2019/11/27 05:26:09 START_TEST LocalStorage.ConfigStress
    DEBUG: 2019/11/27 05:26:09 Login to cluster
    DEBUG: 2019/11/27 05:26:09 Checking basic Vnic usage
    DEBUG: 2019/11/27 05:26:09 Updating inventory struct
    DEBUG: 2019/11/27 05:26:10 Checking stale resources
    DEBUG: 2019/11/27 05:26:10 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 05:26:10 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 05:26:10 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 05:26:18 Creating storage classes
[It] creates maximum volumes and runs attach detach delete in a loop
  /gocode/main/test/e2e/tests/volume.go:2259
    DEBUG: 2019/11/27 05:26:28 Running Create - Attach - Detach - Delete volumes in a loop
    DEBUG: 2019/11/27 05:26:28 Running iteration no : 1
    DEBUG: 2019/11/27 05:26:28 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 05:26:45 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 05:26:45 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 05:26:45 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 05:32:40 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 05:32:40 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 05:32:40 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 05:33:00 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 05:33:00 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 05:33:00 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 05:34:09 Checking Drive Usage on node : appserv54
    DEBUG: 2019/11/27 05:34:09 Checking Drive Usage on node : appserv55
    DEBUG: 2019/11/27 05:34:09 Checking Drive Usage on node : appserv53
    DEBUG: 2019/11/27 05:34:10 Running iteration no : 2
    DEBUG: 2019/11/27 05:34:10 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 05:34:26 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 05:34:26 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 05:34:26 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 05:40:20 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 05:40:20 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 05:40:20 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 05:40:58 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 05:40:58 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 05:40:58 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 05:42:08 Checking Drive Usage on node : appserv53
    DEBUG: 2019/11/27 05:42:08 Checking Drive Usage on node : appserv54
    DEBUG: 2019/11/27 05:42:08 Checking Drive Usage on node : appserv55
    DEBUG: 2019/11/27 05:42:08 Running iteration no : 3
    DEBUG: 2019/11/27 05:42:08 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 05:42:24 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 05:42:24 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 05:42:24 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 05:48:21 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 05:48:21 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 05:48:21 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 05:48:58 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 05:48:58 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 05:48:58 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 05:49:52 Checking Drive Usage on node : appserv53
    DEBUG: 2019/11/27 05:49:52 Checking Drive Usage on node : appserv54
    DEBUG: 2019/11/27 05:49:52 Checking Drive Usage on node : appserv55
    DEBUG: 2019/11/27 05:49:52 /**** Completed Operations: Create - Attach - Detach - Delete volumes in a loop. Total Iterations: 3 ****/
    DEBUG: 2019/11/27 05:49:52 Running Attach - Detach volumes in a loop
    DEBUG: 2019/11/27 05:49:52 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 05:50:09 Running iteration no : 1
    DEBUG: 2019/11/27 05:50:09 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 05:50:09 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 05:50:09 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 05:56:00 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 05:56:00 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 05:56:00 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 05:56:27 Running iteration no : 2
    DEBUG: 2019/11/27 05:56:27 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 05:56:27 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 05:56:27 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 06:02:19 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 06:02:19 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 06:02:19 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 06:02:39 Running iteration no : 3
    DEBUG: 2019/11/27 06:02:39 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 06:02:39 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 06:02:39 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 06:08:34 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 06:08:34 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 06:08:34 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 06:08:59 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 06:08:59 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 06:08:59 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 06:10:07 Checking Drive Usage on node : appserv53
    DEBUG: 2019/11/27 06:10:07 Checking Drive Usage on node : appserv54
    DEBUG: 2019/11/27 06:10:08 Checking Drive Usage on node : appserv55
    DEBUG: 2019/11/27 06:10:08 /**** Completed Operations: Attach - Detach volumes in a loop. Total Iterations: 3 ****/
    DEBUG: 2019/11/27 06:10:08 Running Create - Delete volumes in a loop
    DEBUG: 2019/11/27 06:10:08 Running iteration no : 1
    DEBUG: 2019/11/27 06:10:08 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 06:10:24 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 06:10:24 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 06:10:24 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 06:11:07 Running iteration no : 2
    DEBUG: 2019/11/27 06:11:07 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 06:11:24 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 06:11:24 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 06:11:24 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 06:12:07 Running iteration no : 3
    DEBUG: 2019/11/27 06:12:07 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 06:12:23 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 06:12:23 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 06:12:23 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 06:13:07 

/******************************************** Stress Test Summary ***************************************************************************/
    DEBUG: 2019/11/27 06:13:07 /**** Completed Operations: Attach - Detach volumes in a loop. Total Iterations: 3 ****/
    DEBUG: 2019/11/27 06:13:07 /**** Completed Operations: Create - Attach - Detach - Delete volumes in a loop. Total Iterations: 3 ****/
    DEBUG: 2019/11/27 06:13:07 /**** Completed Operation. Create - Delete volumes in a loop. Total Iterations: 3 ****/
    DEBUG: 2019/11/27 06:13:07 
/*********************************************************************************************************************************************/


[AfterEach] create maximum volumes and run attach detach delete in a loop
  /gocode/main/test/e2e/tests/volume.go:2254
    DEBUG: 2019/11/27 06:13:07 END_TEST LocalStorage.ConfigStress Time-taken : 2818.676080952
    DEBUG: 2019/11/27 06:13:07 Checking stale resources
    DEBUG: 2019/11/27 06:13:07 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 06:13:07 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 06:13:07 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:2818.925 seconds][0m
LocalStorage.ConfigStress Daily S_Stress-1.1 S_Stress-1.2 S_Stress-1.3 S_Stress-1.4 S_Basic-1.3
[90m/gocode/main/test/e2e/tests/volume.go:2237[0m
  create maximum volumes and run attach detach delete in a loop
  [90m/gocode/main/test/e2e/tests/volume.go:2242[0m
    creates maximum volumes and runs attach detach delete in a loop
    [90m/gocode/main/test/e2e/tests/volume.go:2259[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mVolumeRestore.Basic Daily S_Restore-1.0 S_Restore-5.0[0m [90mVolume restore basic test.[0m 
  [1mRestore volumes, Compare checksum taken before restore with checksum taken after restore, add plex to restored volumes,Compare checksum on added plex with volume's check-sums.[0m
  [37m/gocode/main/test/e2e/tests/volume-restore.go:77[0m
[BeforeEach] Volume restore basic test.
  /gocode/main/test/e2e/tests/volume-restore.go:64
    DEBUG: 2019/11/27 06:13:07 START_TEST VolumeRestore.Basic
    DEBUG: 2019/11/27 06:13:07 Login to cluster
    DEBUG: 2019/11/27 06:13:08 Checking basic Vnic usage
    DEBUG: 2019/11/27 06:13:08 Updating inventory struct
    DEBUG: 2019/11/27 06:13:09 Checking stale resources
    DEBUG: 2019/11/27 06:13:09 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 06:13:09 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 06:13:09 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 06:13:18 Creating storage classes
[It] Restore volumes, Compare checksum taken before restore with checksum taken after restore, add plex to restored volumes,Compare checksum on added plex with volume's check-sums.
  /gocode/main/test/e2e/tests/volume-restore.go:77
    DEBUG: 2019/11/27 06:13:28 Assigning label to nodes where the plexes of mirrored volumes should get scheduled
    DEBUG: 2019/11/27 06:13:28 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 06:13:28 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 06:13:28 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 06:13:28 Assigning label to nodes where local volume get scheduled
    DEBUG: 2019/11/27 06:13:29 Assigned label : local=true to node : appserv53
    DEBUG: 2019/11/27 06:13:29 Assigning label to nodes where remote volume get scheduled
    DEBUG: 2019/11/27 06:13:29 Assigned label : remote=true to node : appserv54
    DEBUG: 2019/11/27 06:13:29 Creating 4 local volumes of random sizes between 10737418240 and 21474836480
    DEBUG: 2019/11/27 06:13:29 Mirror Count: 1
    DEBUG: 2019/11/27 06:13:30 Creating 4 remote volumes of random sizes between 10737418240 and 21474836480
    DEBUG: 2019/11/27 06:13:30 Mirror Count: 1
    DEBUG: 2019/11/27 06:13:31 Creating 4 mirror volumes of random sizes between 10737418240 and 21474836480
    DEBUG: 2019/11/27 06:13:31 Mirror Count: 3
    DEBUG: 2019/11/27 06:13:32 Removing mirror label from the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 06:13:32 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 06:13:32 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 06:13:32 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 06:13:32 Removing label from the nodes : [appserv53]
    DEBUG: 2019/11/27 06:13:32 Removed label : local from node : appserv53
    DEBUG: 2019/11/27 06:13:32 Removing label from the nodes : [appserv54]
    DEBUG: 2019/11/27 06:13:32 Removed label : remote from node : appserv54
    DEBUG: 2019/11/27 06:13:32 Attaching all 12 volumes
    DEBUG: 2019/11/27 06:14:38 Writing "0xff%o\"efgh\"-12" pattern on volumes. Write size: 1G
    DEBUG: 2019/11/27 06:14:40 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"efgh\"-12 --verify_interval=4096 --size=1G --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1 --name=dev4 --filename=/dev/nvme4n1 --name=dev5 --filename=/dev/nvme5n1 --name=dev6 --filename=/dev/nvme6n1 --name=dev7 --filename=/dev/nvme7n1 --name=dev8 --filename=/dev/nvme8n1 --name=dev9 --filename=/dev/nvme9n1 --name=dev10 --filename=/dev/nvme10n1 --name=dev11 --filename=/dev/nvme11n1 --name=dev12 --filename=/dev/nvme12n1
    DEBUG: 2019/11/27 06:16:00 Calculating checksum of all volumes after first fio write
    DEBUG: 2019/11/27 06:16:00 Calculating checksum of all local and remote and mirror volumes
    DEBUG: 2019/11/27 06:19:49 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 0
    DEBUG: 2019/11/27 06:20:18 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 1
    DEBUG: 2019/11/27 06:20:46 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 2
    DEBUG: 2019/11/27 06:21:13 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 0
    DEBUG: 2019/11/27 06:21:41 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 1
    DEBUG: 2019/11/27 06:22:10 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 2
    DEBUG: 2019/11/27 06:22:38 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 0
    DEBUG: 2019/11/27 06:23:09 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 1
    DEBUG: 2019/11/27 06:23:39 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 2
    DEBUG: 2019/11/27 06:24:09 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 0
    DEBUG: 2019/11/27 06:24:42 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 1
    DEBUG: 2019/11/27 06:25:13 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 2
    DEBUG: 2019/11/27 06:25:45 Creating snapshots for the  volumes
    DEBUG: 2019/11/27 06:25:45 Mirror Count: 1
    DEBUG: 2019/11/27 06:26:02 Getting fbm and l1 count after writing first pattern.
    DEBUG: 2019/11/27 06:26:02 Calculating FBM and L1 count on appserv53
    DEBUG: 2019/11/27 06:26:04 FBM count is 16777216 and L1 count is 2624 on node appserv53.
    DEBUG: 2019/11/27 06:26:04 Calculating FBM and L1 count on appserv54
    DEBUG: 2019/11/27 06:26:06 FBM count is 16777216 and L1 count is 2624 on node appserv54.
    DEBUG: 2019/11/27 06:26:06 Calculating FBM and L1 count on appserv55
    DEBUG: 2019/11/27 06:26:08 FBM count is 8388608 and L1 count is 1312 on node appserv55.
    DEBUG: 2019/11/27 06:26:08 Writing "0xff%o\"akpz\"-12" pattern on volumes. Write size: 2G
    DEBUG: 2019/11/27 06:26:09 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"akpz\"-12 --verify_interval=4096 --size=2G --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1 --name=dev4 --filename=/dev/nvme4n1 --name=dev5 --filename=/dev/nvme5n1 --name=dev6 --filename=/dev/nvme6n1 --name=dev7 --filename=/dev/nvme7n1 --name=dev8 --filename=/dev/nvme8n1 --name=dev9 --filename=/dev/nvme9n1 --name=dev10 --filename=/dev/nvme10n1 --name=dev11 --filename=/dev/nvme11n1 --name=dev12 --filename=/dev/nvme12n1
    DEBUG: 2019/11/27 06:28:44 Calculating checksum of all volumes after second fio write
    DEBUG: 2019/11/27 06:28:44 Calculating checksum of all local and remote and mirror volumes
    DEBUG: 2019/11/27 06:32:33 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 0
    DEBUG: 2019/11/27 06:33:03 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 1
    DEBUG: 2019/11/27 06:33:32 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 2
    DEBUG: 2019/11/27 06:34:02 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 0
    DEBUG: 2019/11/27 06:34:32 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 1
    DEBUG: 2019/11/27 06:34:59 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 2
    DEBUG: 2019/11/27 06:35:28 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 0
    DEBUG: 2019/11/27 06:35:55 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 1
    DEBUG: 2019/11/27 06:36:26 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 2
    DEBUG: 2019/11/27 06:37:01 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 0
    DEBUG: 2019/11/27 06:37:33 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 1
    DEBUG: 2019/11/27 06:38:05 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 2
    DEBUG: 2019/11/27 06:38:36 Comparing checksum after first write and checksum after second write ,this should not be equal
    DEBUG: 2019/11/27 06:38:36 Getting fbm and l1 count after writing second pattern.
    DEBUG: 2019/11/27 06:38:36 Calculating FBM and L1 count on appserv53
    DEBUG: 2019/11/27 06:38:38 FBM count is 41943040 and L1 count is 6560 on node appserv53.
    DEBUG: 2019/11/27 06:38:38 Calculating FBM and L1 count on appserv54
    DEBUG: 2019/11/27 06:38:40 FBM count is 41943040 and L1 count is 6560 on node appserv54.
    DEBUG: 2019/11/27 06:38:40 Calculating FBM and L1 count on appserv55
    DEBUG: 2019/11/27 06:38:42 FBM count is 25165824 and L1 count is 3936 on node appserv55.
    DEBUG: 2019/11/27 06:38:42 Comparing Fbm count after writing first pattern and fbm after writing second pattern. FBM2 should be greater than FBM1
    DEBUG: 2019/11/27 06:38:42 Detaching the volumes
    DEBUG: 2019/11/27 06:38:47 Restoring the volumes
    DEBUG: 2019/11/27 06:38:52 Attaching all 12  volumes
    DEBUG: 2019/11/27 06:39:55 Waiting for plexs to resync.
    DEBUG: 2019/11/27 06:39:55 Number of volumes : 12
    DEBUG: 2019/11/27 06:39:55 Checking resync progress on volume : test-vol9
    DEBUG: 2019/11/27 06:39:55 Volume name & Plex : test-vol9.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:39:55 Volume name & Plex : test-vol9.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 06:39:57 Volume "test-vol9" has index "8" in embedded.
    DEBUG: 2019/11/27 06:39:57 Volume: test-vol9. Resync offset: 66

    DEBUG: 2019/11/27 06:39:58 Volume: test-vol9. Resync offset: 67

    DEBUG: 2019/11/27 06:39:58 Volume name & Plex : test-vol9.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 06:39:59 Volume "test-vol9" has index "8" in embedded.
    DEBUG: 2019/11/27 06:40:00 Volume: test-vol9. Resync offset: 72

    DEBUG: 2019/11/27 06:40:01 Volume: test-vol9. Resync offset: 74

    DEBUG: 2019/11/27 06:40:01 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol9.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol9.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol9.p2. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 All plexes of volume "test-vol9" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:31 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:31 Checking resync progress on volume : test-vol12
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol12.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol12.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol12.p2. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 All plexes of volume "test-vol12" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:31 Checking resync progress on volume : test-vol11
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol11.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol11.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol11.p2. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 All plexes of volume "test-vol11" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:31 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:31 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:31 Checking resync progress on volume : test-vol10
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol10.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol10.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 Volume name & Plex : test-vol10.p2. Plex State : InUse
    DEBUG: 2019/11/27 06:40:31 All plexes of volume "test-vol10" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:31 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/27 06:40:32 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:32 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:32 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/27 06:40:32 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:32 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:32 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/27 06:40:32 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:32 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:32 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/27 06:40:32 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:32 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:32 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/27 06:40:32 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:40:32 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/27 06:40:32 Calculating checksum after restoring the volumes
    DEBUG: 2019/11/27 06:40:32 Calculating checksum of all local and remote and mirror volumes
    DEBUG: 2019/11/27 06:44:20 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 0
    DEBUG: 2019/11/27 06:44:49 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 1
    DEBUG: 2019/11/27 06:45:18 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 2
    DEBUG: 2019/11/27 06:45:46 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 0
    DEBUG: 2019/11/27 06:46:15 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 1
    DEBUG: 2019/11/27 06:46:45 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 2
    DEBUG: 2019/11/27 06:47:15 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 0
    DEBUG: 2019/11/27 06:47:46 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 1
    DEBUG: 2019/11/27 06:48:16 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 2
    DEBUG: 2019/11/27 06:48:45 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 0
    DEBUG: 2019/11/27 06:49:18 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 1
    DEBUG: 2019/11/27 06:49:50 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 2
    DEBUG: 2019/11/27 06:50:21 Comparing checksum after  writing first pattern and checksum after volume restore,this should be equal
    DEBUG: 2019/11/27 06:50:21 Getting fbm and l1 count after restoring the volumes.
    DEBUG: 2019/11/27 06:50:21 Calculating FBM and L1 count on appserv53
    DEBUG: 2019/11/27 06:50:23 FBM count is 16777216 and L1 count is 3936 on node appserv53.
    DEBUG: 2019/11/27 06:50:23 Calculating FBM and L1 count on appserv54
    DEBUG: 2019/11/27 06:50:26 FBM count is 16777216 and L1 count is 3936 on node appserv54.
    DEBUG: 2019/11/27 06:50:26 Calculating FBM and L1 count on appserv55
    DEBUG: 2019/11/27 06:50:28 FBM count is 8388608 and L1 count is 1312 on node appserv55.
    DEBUG: 2019/11/27 06:50:28 Comparing fbm count after restoring the volume and fbm count after writing first pattern, this should be eqal
    DEBUG: 2019/11/27 06:50:28 Assigning label to nodes where plex get scheduled
    DEBUG: 2019/11/27 06:50:28 Assigned label : remote=true to node : appserv53
    DEBUG: 2019/11/27 06:50:28 Assigned label : local=true to node : appserv54
    DEBUG: 2019/11/27 06:50:28 Adding one plex to all mirror volumes.
    DEBUG: 2019/11/27 06:50:31 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:50:31 Volume name & Plex : test-vol1.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 06:50:32 Volume "test-vol1" has index "0" in embedded.
    DEBUG: 2019/11/27 06:50:33 Volume: test-vol1. Resync offset: 7

    DEBUG: 2019/11/27 06:50:34 Volume: test-vol1. Resync offset: 7

    DEBUG: 2019/11/27 06:50:39 Volume: test-vol1. Resync offset: 44

    DEBUG: 2019/11/27 06:50:39 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 06:51:09 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:51:09 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:51:09 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 06:51:10 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/27 06:51:10 Removing label from the nodes : [appserv53]
    DEBUG: 2019/11/27 06:51:10 Removed label : remote from node : appserv53
    DEBUG: 2019/11/27 06:51:10 Removing label from the nodes : [appserv54]
    DEBUG: 2019/11/27 06:51:11 Removed label : local from node : appserv54
    DEBUG: 2019/11/27 06:51:11 Calculating checksum after adding plex
    DEBUG: 2019/11/27 06:51:11 Calculating checksum of all local and remote and mirror volumes
    DEBUG: 2019/11/27 06:51:11 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 06:51:39 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 06:52:06 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 06:52:36 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 06:53:05 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 06:53:35 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 06:54:05 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 06:54:35 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 06:55:05 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 06:55:32 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 06:55:59 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 06:56:27 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 06:56:55 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 06:57:24 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 06:57:53 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 06:58:24 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 06:58:55 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 0
    DEBUG: 2019/11/27 06:59:22 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 1
    DEBUG: 2019/11/27 06:59:50 Changing preferred plex of volume: test-vol9. Volume load index: 8.New preferred plex: 2
    DEBUG: 2019/11/27 07:00:17 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 0
    DEBUG: 2019/11/27 07:00:45 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 1
    DEBUG: 2019/11/27 07:01:13 Changing preferred plex of volume: test-vol10. Volume load index: 9.New preferred plex: 2
    DEBUG: 2019/11/27 07:01:43 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 0
    DEBUG: 2019/11/27 07:02:11 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 1
    DEBUG: 2019/11/27 07:02:39 Changing preferred plex of volume: test-vol11. Volume load index: 10.New preferred plex: 2
    DEBUG: 2019/11/27 07:03:06 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 0
    DEBUG: 2019/11/27 07:03:34 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 1
    DEBUG: 2019/11/27 07:04:03 Changing preferred plex of volume: test-vol12. Volume load index: 11.New preferred plex: 2
    DEBUG: 2019/11/27 07:04:31 Comparing checksum after first write and checksum after adding plex ,this should be equal
    DEBUG: 2019/11/27 07:04:31 Detaching the volumes.
    DEBUG: 2019/11/27 07:04:53 Deleting snapshots: 
    DEBUG: 2019/11/27 07:05:38 Deleting the volumes
[AfterEach] Volume restore basic test.
  /gocode/main/test/e2e/tests/volume-restore.go:73
    DEBUG: 2019/11/27 07:06:39 END_TEST VolumeRestore.Basic Time-taken : 3211.235753456
    DEBUG: 2019/11/27 07:06:39 Checking stale resources
    DEBUG: 2019/11/27 07:06:39 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:06:39 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:06:39 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:3211.506 seconds][0m
VolumeRestore.Basic Daily S_Restore-1.0 S_Restore-5.0
[90m/gocode/main/test/e2e/tests/volume-restore.go:42[0m
  Volume restore basic test.
  [90m/gocode/main/test/e2e/tests/volume-restore.go:44[0m
    Restore volumes, Compare checksum taken before restore with checksum taken after restore, add plex to restored volumes,Compare checksum on added plex with volume's check-sums.
    [90m/gocode/main/test/e2e/tests/volume-restore.go:77[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mMirroring.MultiplePodsOneMirroredVolumeEach Daily SMP_Basic-1.0 Qos[0m [90mMultiple pods, one mirrored volume each.[0m 
  [1mMultiple pods, one mirrored volume each.[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:1083[0m
[BeforeEach] Multiple pods, one mirrored volume each.
  /gocode/main/test/e2e/tests/mirroring.go:1068
    DEBUG: 2019/11/27 07:06:39 START_TEST Mirroring.MultiplePodsOneMirroredVolumeEach
    DEBUG: 2019/11/27 07:06:39 Login to cluster
    DEBUG: 2019/11/27 07:06:40 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:06:40 Updating inventory struct
    DEBUG: 2019/11/27 07:06:41 Checking stale resources
    DEBUG: 2019/11/27 07:06:41 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:06:41 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:06:41 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:06:49 Creating storage classes
[It] Multiple pods, one mirrored volume each.
  /gocode/main/test/e2e/tests/mirroring.go:1083
    DEBUG: 2019/11/27 07:07:01 Assigning label to nodes where the plexes of mirrored volumes should get scheduled
    DEBUG: 2019/11/27 07:07:01 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 07:07:01 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 07:07:01 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 07:07:01 Creating 10 Dynamic Persistent Volume Claims (PVCs). Mirror count: 3. Selector: 
    DEBUG: 2019/11/27 07:07:02 Created PVC successfully.
    DEBUG: 2019/11/27 07:07:03 Created PVC successfully.
    DEBUG: 2019/11/27 07:07:03 Created PVC successfully.
    DEBUG: 2019/11/27 07:07:04 Created PVC successfully.
    DEBUG: 2019/11/27 07:07:04 Created PVC successfully.
    DEBUG: 2019/11/27 07:07:04 Created PVC successfully.
    DEBUG: 2019/11/27 07:07:05 Created PVC successfully.
    DEBUG: 2019/11/27 07:07:05 Created PVC successfully.
    DEBUG: 2019/11/27 07:07:05 Created PVC successfully.
    DEBUG: 2019/11/27 07:07:06 Created PVC successfully.
    DEBUG: 2019/11/27 07:07:08 Creating 10 fio pods: 
    DEBUG: 2019/11/27 07:07:11 Checking if given pods are in Running state
    DEBUG: 2019/11/27 07:08:04 Wait for volumes to move into attached state: 
    DEBUG: 2019/11/27 07:08:04 Waitting for volume to move to "Attached" state
    DEBUG: 2019/11/27 07:08:06 Sleeping for 180 seconds, so that prometheus will have some stats
    DEBUG: 2019/11/27 07:11:06 Validating qos associated with each volume: 
    DEBUG: 2019/11/27 07:11:08 Deleting pods : 
    DEBUG: 2019/11/27 07:11:22 Wait for volumes to come in Available state: 
    DEBUG: 2019/11/27 07:11:22 Delete PVCs: 
    DEBUG: 2019/11/27 07:11:24 Waiting for volumes to get deleted: 
    DEBUG: 2019/11/27 07:12:38 Removing label from the nodes where the plexes of mirrored volumes were scheduled
    DEBUG: 2019/11/27 07:12:38 Removed label : mirror from node : appserv54
    DEBUG: 2019/11/27 07:12:38 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 07:12:38 Removed label : mirror from node : appserv55
[AfterEach] Multiple pods, one mirrored volume each.
  /gocode/main/test/e2e/tests/mirroring.go:1078
    DEBUG: 2019/11/27 07:12:38 END_TEST Mirroring.MultiplePodsOneMirroredVolumeEach Time-taken : 359.157466262
    DEBUG: 2019/11/27 07:12:38 Checking stale resources
    DEBUG: 2019/11/27 07:12:38 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:12:38 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:12:38 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:359.422 seconds][0m
Mirroring.MultiplePodsOneMirroredVolumeEach Daily SMP_Basic-1.0 Qos
[90m/gocode/main/test/e2e/tests/mirroring.go:1061[0m
  Multiple pods, one mirrored volume each.
  [90m/gocode/main/test/e2e/tests/mirroring.go:1063[0m
    Multiple pods, one mirrored volume each.
    [90m/gocode/main/test/e2e/tests/mirroring.go:1083[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRemoteStorage.ConfigStress Daily RS_Stress-1.0[0m [90mCreate - Attach - Detach - Delete remote volumes in loop.[0m 
  [1mCreate - Attach - Detach - Delete remote volumes in loop.[0m
  [37m/gocode/main/test/e2e/tests/volume.go:4553[0m
[BeforeEach] Create - Attach - Detach - Delete remote volumes in loop.
  /gocode/main/test/e2e/tests/volume.go:4538
    DEBUG: 2019/11/27 07:12:38 START_TEST RemoteStorage.ConfigStress
    DEBUG: 2019/11/27 07:12:38 Login to cluster
    DEBUG: 2019/11/27 07:12:39 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:12:39 Updating inventory struct
    DEBUG: 2019/11/27 07:12:40 Checking stale resources
    DEBUG: 2019/11/27 07:12:40 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:12:40 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:12:40 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:12:49 Creating storage classes
[It] Create - Attach - Detach - Delete remote volumes in loop.
  /gocode/main/test/e2e/tests/volume.go:4553
    DEBUG: 2019/11/27 07:12:57 /****************************** Started Iteration: 1 ***********************/
    DEBUG: 2019/11/27 07:12:58 Create 16 single plex volumes per cluster node: 
    DEBUG: 2019/11/27 07:12:58 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 07:13:02 Attaching volumes of node appserv54 on node appserv53:
    DEBUG: 2019/11/27 07:14:27 Attaching volumes of node appserv55 on node appserv54:
    DEBUG: 2019/11/27 07:15:52 Attaching volumes of node appserv53 on node appserv55:
    DEBUG: 2019/11/27 07:17:17 Detach volumes:
    DEBUG: 2019/11/27 07:17:17 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 07:17:17 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 07:17:17 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 07:17:25 Delete volumes: 
    DEBUG: 2019/11/27 07:18:12 /****************************** Started Iteration: 2 ***********************/
    DEBUG: 2019/11/27 07:18:13 Create 16 single plex volumes per cluster node: 
    DEBUG: 2019/11/27 07:18:13 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 07:18:17 Attaching volumes of node appserv54 on node appserv53:
    DEBUG: 2019/11/27 07:19:42 Attaching volumes of node appserv55 on node appserv54:
    DEBUG: 2019/11/27 07:21:07 Attaching volumes of node appserv53 on node appserv55:
    DEBUG: 2019/11/27 07:22:31 Detach volumes:
    DEBUG: 2019/11/27 07:22:31 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 07:22:31 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 07:22:31 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 07:22:54 Delete volumes: 
    DEBUG: 2019/11/27 07:23:44 /****************************** Started Iteration: 3 ***********************/
    DEBUG: 2019/11/27 07:23:45 Create 16 single plex volumes per cluster node: 
    DEBUG: 2019/11/27 07:23:45 Creating volumes on the nodes : [appserv54 appserv55 appserv53]
    DEBUG: 2019/11/27 07:23:49 Attaching volumes of node appserv54 on node appserv53:
    DEBUG: 2019/11/27 07:25:14 Attaching volumes of node appserv55 on node appserv54:
    DEBUG: 2019/11/27 07:26:39 Attaching volumes of node appserv53 on node appserv55:
    DEBUG: 2019/11/27 07:28:09 Detach volumes:
    DEBUG: 2019/11/27 07:28:09 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 07:28:09 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 07:28:09 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 07:28:24 Delete volumes: 
[AfterEach] Create - Attach - Detach - Delete remote volumes in loop.
  /gocode/main/test/e2e/tests/volume.go:4548
    DEBUG: 2019/11/27 07:29:16 END_TEST RemoteStorage.ConfigStress Time-taken : 997.682102002
    DEBUG: 2019/11/27 07:29:16 Checking stale resources
    DEBUG: 2019/11/27 07:29:16 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:29:16 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:29:16 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:997.936 seconds][0m
RemoteStorage.ConfigStress Daily RS_Stress-1.0
[90m/gocode/main/test/e2e/tests/volume.go:4531[0m
  Create - Attach - Detach - Delete remote volumes in loop.
  [90m/gocode/main/test/e2e/tests/volume.go:4532[0m
    Create - Attach - Detach - Delete remote volumes in loop.
    [90m/gocode/main/test/e2e/tests/volume.go:4553[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mCreate network with same subnet of existing network but with different Vlan[0m
  [37m/gocode/main/test/e2e/tests/network.go:137[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:35
    DEBUG: 2019/11/27 07:29:16 START_TEST Network.NegativeTests
    DEBUG: 2019/11/27 07:29:16 Login to cluster
    DEBUG: 2019/11/27 07:29:17 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:29:17 Updating inventory struct
    DEBUG: 2019/11/27 07:29:18 Checking stale resources
    DEBUG: 2019/11/27 07:29:18 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:29:18 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:29:18 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:29:26 Creating storage classes
[It] Create network with same subnet of existing network but with different Vlan
  /gocode/main/test/e2e/tests/network.go:137
    DEBUG: 2019/11/27 07:29:36 Trying to create network with same subnet ( start addr : 172.16.180.4, end addr : 172.16.180.253) and different VLAN.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:48
    DEBUG: 2019/11/27 07:29:36 END_TEST Network.NegativeTests Time-taken : 19.967055011
    DEBUG: 2019/11/27 07:29:36 Checking stale resources
    DEBUG: 2019/11/27 07:29:36 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:29:36 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:29:36 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:20.222 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:26[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:27[0m
    Create network with same subnet of existing network but with different Vlan
    [90m/gocode/main/test/e2e/tests/network.go:137[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mEndpoint.PingBetweenStaticDynamicEndpointPods Daily N_Endpoint-1.12[0m [90mCreate pod with static and dynamic endpoint and try ping between two pods.[0m 
  [1mCreate pod with static and dynamic endpoint and try ping between two pods.[0m
  [37m/gocode/main/test/e2e/tests/endpoint.go:654[0m
[BeforeEach] Create pod with static and dynamic endpoint and try ping between two pods.
  /gocode/main/test/e2e/tests/endpoint.go:643
    DEBUG: 2019/11/27 07:29:37 Login to cluster
    DEBUG: 2019/11/27 07:29:37 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:29:37 Updating inventory struct
    DEBUG: 2019/11/27 07:29:38 Checking stale resources
    DEBUG: 2019/11/27 07:29:38 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:29:38 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:29:38 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:29:47 Creating storage classes
    DEBUG: 2019/11/27 07:29:55 START_TEST Endpoint.PingBetweenStaticDynamicEndpointPods
[It] Create pod with static and dynamic endpoint and try ping between two pods.
  /gocode/main/test/e2e/tests/endpoint.go:654
    DEBUG: 2019/11/27 07:29:55 Creating iperf server pod: iperf-server-1
    DEBUG: 2019/11/27 07:29:58 Create a endpoint: test-ep1
    DEBUG: 2019/11/27 07:29:58 Creating service with name: iperf-server-1
    DEBUG: 2019/11/27 07:29:58 Creating iperf Client pod: iperf-client-1
    DEBUG: 2019/11/27 07:30:01 Trying to ping the "iperf-server-1" from pod "iperf-client-1".
    DEBUG: 2019/11/27 07:30:01 172.16.179.6 is pingable from pod iperf-client-1 (172.16.179.7)
    DEBUG: 2019/11/27 07:30:01 Trying to ping the "iperf-client-1" from pod "iperf-server-1".
    DEBUG: 2019/11/27 07:30:02 172.16.179.7 is pingable from pod iperf-server-1 (172.16.179.6)
    DEBUG: 2019/11/27 07:30:02 Delete pods
    DEBUG: 2019/11/27 07:30:02 Deleting pods : 
    DEBUG: 2019/11/27 07:30:14 Delete Endpoint: test-ep1
[AfterEach] Create pod with static and dynamic endpoint and try ping between two pods.
  /gocode/main/test/e2e/tests/endpoint.go:649
    DEBUG: 2019/11/27 07:30:14 END_TEST Endpoint.PingBetweenStaticDynamicEndpointPods Time-taken : 18.711740075
    DEBUG: 2019/11/27 07:30:14 Checking stale resources
    DEBUG: 2019/11/27 07:30:14 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:30:14 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:30:14 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:37.767 seconds][0m
Endpoint.PingBetweenStaticDynamicEndpointPods Daily N_Endpoint-1.12
[90m/gocode/main/test/e2e/tests/endpoint.go:631[0m
  Create pod with static and dynamic endpoint and try ping between two pods.
  [90m/gocode/main/test/e2e/tests/endpoint.go:633[0m
    Create pod with static and dynamic endpoint and try ping between two pods.
    [90m/gocode/main/test/e2e/tests/endpoint.go:654[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4[0m [90mPing pod's IP from outside world[0m 
  [1mPing pod's IP from outside world using network having invalid VLAN[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:750[0m
[BeforeEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:700
    DEBUG: 2019/11/27 07:30:14 START_TEST Network.PingPodFromOutside
    DEBUG: 2019/11/27 07:30:14 Login to cluster
    DEBUG: 2019/11/27 07:30:15 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:30:15 Updating inventory struct
    DEBUG: 2019/11/27 07:30:16 Checking stale resources
    DEBUG: 2019/11/27 07:30:16 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:30:16 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:30:16 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:30:24 Creating storage classes
[It] Ping pod's IP from outside world using network having invalid VLAN
  /gocode/main/test/e2e/tests/network-pod.go:750
    DEBUG: 2019/11/27 07:30:33 Creating test network : testnetwork with invalid vlan
    DEBUG: 2019/11/27 07:30:33 Creating 1 pods of docker.io/redis:3.0.5 image with network : testnetwork and qos : high
    DEBUG: 2019/11/27 07:30:36 IP address ( 56.12.100.2 ) of e2etest-pod-1 is between 56.12.100.2 and 56.12.100.254

    DEBUG: 2019/11/27 07:30:36 Trying to ping the e2etest-pod-1
    DEBUG: 2019/11/27 07:30:36 Executing ping command: ping  -c 5 -W 5 56.12.100.2
    DEBUG: 2019/11/27 07:30:45 Deleting pods : 
    DEBUG: 2019/11/27 07:30:58 Deleting test network : testnetwork
[AfterEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:709
    DEBUG: 2019/11/27 07:30:58 END_TEST Network.PingPodFromOutside Time-taken : 43.887445354
    DEBUG: 2019/11/27 07:30:58 Checking stale resources
    DEBUG: 2019/11/27 07:30:58 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:30:58 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:30:58 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:44.121 seconds][0m
Network.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4
[90m/gocode/main/test/e2e/tests/network-pod.go:694[0m
  Ping pod's IP from outside world
  [90m/gocode/main/test/e2e/tests/network-pod.go:695[0m
    Ping pod's IP from outside world using network having invalid VLAN
    [90m/gocode/main/test/e2e/tests/network-pod.go:750[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mCreate the same network twice.[0m
  [37m/gocode/main/test/e2e/tests/network.go:89[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:35
    DEBUG: 2019/11/27 07:30:58 START_TEST Network.NegativeTests
    DEBUG: 2019/11/27 07:30:58 Login to cluster
    DEBUG: 2019/11/27 07:30:59 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:30:59 Updating inventory struct
    DEBUG: 2019/11/27 07:31:00 Checking stale resources
    DEBUG: 2019/11/27 07:31:00 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:31:00 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:31:00 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:31:09 Creating storage classes
[It] Create the same network twice.
  /gocode/main/test/e2e/tests/network.go:89
    DEBUG: 2019/11/27 07:31:17 Try to create a valid network.
    DEBUG: 2019/11/27 07:31:17 Try to create the same network again.
    DEBUG: 2019/11/27 07:31:18 Delete the newly added network.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:48
    DEBUG: 2019/11/27 07:31:18 END_TEST Network.NegativeTests Time-taken : 19.333778069
    DEBUG: 2019/11/27 07:31:18 Checking stale resources
    DEBUG: 2019/11/27 07:31:18 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:31:18 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:31:18 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.578 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:26[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:27[0m
    Create the same network twice.
    [90m/gocode/main/test/e2e/tests/network.go:89[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mLocalStorage.Basic Daily S_Basic-1.0 S_Basic-1.1 S_Basic-1.4 S_Verify-1.1 S_Verify-1.2 S_Limit-1.0 S_Limit-1.3[0m [90mStorage volume basic test[0m 
  [1mstorage volume basic operations[0m
  [37m/gocode/main/test/e2e/tests/volume.go:941[0m
[BeforeEach] Storage volume basic test
  /gocode/main/test/e2e/tests/volume.go:930
    DEBUG: 2019/11/27 07:31:18 START_TEST LocalStorage.Basic
    DEBUG: 2019/11/27 07:31:18 Login to cluster
    DEBUG: 2019/11/27 07:31:19 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:31:19 Updating inventory struct
    DEBUG: 2019/11/27 07:31:20 Checking stale resources
    DEBUG: 2019/11/27 07:31:20 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:31:20 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:31:20 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:31:28 Creating storage classes
[It] storage volume basic operations
  /gocode/main/test/e2e/tests/volume.go:941
    DEBUG: 2019/11/27 07:31:38 Creating volumes on the nodes : [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/27 07:31:54 Attaching volumes on the node : appserv53
    DEBUG: 2019/11/27 07:31:54 Attaching volumes on the node : appserv54
    DEBUG: 2019/11/27 07:31:54 Attaching volumes on the node : appserv55
    DEBUG: 2019/11/27 07:37:45 Total number of volumes: 192

    DEBUG: 2019/11/27 07:37:45 Running WRITE IOs with SHA512 checksum with IO SIZE (4K)
    DEBUG: 2019/11/27 07:37:53 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:37:53 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:37:53 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:38:36 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 07:38:36 Running VERIFY IOs with SHA512 checksum with IO SIZE (4K)
    DEBUG: 2019/11/27 07:38:45 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:38:45 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:38:45 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:39:20 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 07:39:20 Running WRITE IOs with SHA512 checksum with IO SIZE (64K)
    DEBUG: 2019/11/27 07:39:28 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:39:28 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:39:28 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:40:11 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 07:40:11 Running VERIFY IOs with SHA512 checksum with IO SIZE (64K)
    DEBUG: 2019/11/27 07:40:20 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:40:21 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:40:21 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=64K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:41:05 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 07:41:05 Running WRITE IOs with SHA512 checksum with BS Range (4K to 1M)
    DEBUG: 2019/11/27 07:41:13 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:41:14 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:41:14 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:41:57 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 07:41:57 Running VERIFY IOs with SHA512 checksum with BS Range (4K to 1M)
    DEBUG: 2019/11/27 07:42:06 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:42:06 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:42:06 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=4K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:42:49 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 07:42:49 Running WRITE IOs with SHA512 checksum with BS Range (32K to 1024K)
    DEBUG: 2019/11/27 07:42:57 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:42:57 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:42:57 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:43:39 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 07:43:39 Running VERIFY IOs with SHA512 checksum with BS Range (32K to 1024K)
    DEBUG: 2019/11/27 07:43:49 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:43:49 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:43:49 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize_range=32K-1024K --direct=1  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:44:32 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 07:44:32 Running WRITE IOs with SHA512 checksum with IO SIZE(4K), bs_unaligned
    DEBUG: 2019/11/27 07:44:41 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:44:41 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:44:41 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randwrite --runtime=30 --time_based --do_verify=0 --verify_state_save=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:45:35 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 07:45:35 Running VERIFY IOs with SHA512 checksum with IO SIZE(4K), bs_unaligned
    DEBUG: 2019/11/27 07:45:45 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:45:45 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:45:45 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --iodepth=16 --verify=sha512 --group_reporting --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --rw=randread --do_verify=1 --verify_state_load=1 --verify_fatal=1 --verify_dump=1  --blocksize=4K  --blocksize_unaligned=1 --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1 --name=job5 --filename=/dev/nvme5n1 --name=job6 --filename=/dev/nvme6n1 --name=job7 --filename=/dev/nvme7n1 --name=job8 --filename=/dev/nvme8n1 --name=job9 --filename=/dev/nvme9n1 --name=job10 --filename=/dev/nvme10n1 --name=job11 --filename=/dev/nvme11n1 --name=job12 --filename=/dev/nvme12n1 --name=job13 --filename=/dev/nvme13n1 --name=job14 --filename=/dev/nvme14n1 --name=job15 --filename=/dev/nvme15n1 --name=job16 --filename=/dev/nvme16n1 --name=job17 --filename=/dev/nvme17n1 --name=job18 --filename=/dev/nvme18n1 --name=job19 --filename=/dev/nvme19n1 --name=job20 --filename=/dev/nvme20n1 --name=job21 --filename=/dev/nvme21n1 --name=job22 --filename=/dev/nvme22n1 --name=job23 --filename=/dev/nvme23n1 --name=job24 --filename=/dev/nvme24n1 --name=job25 --filename=/dev/nvme25n1 --name=job26 --filename=/dev/nvme26n1 --name=job27 --filename=/dev/nvme27n1 --name=job28 --filename=/dev/nvme28n1 --name=job29 --filename=/dev/nvme29n1 --name=job30 --filename=/dev/nvme30n1 --name=job31 --filename=/dev/nvme31n1 --name=job32 --filename=/dev/nvme32n1 --name=job33 --filename=/dev/nvme33n1 --name=job34 --filename=/dev/nvme34n1 --name=job35 --filename=/dev/nvme35n1 --name=job36 --filename=/dev/nvme36n1 --name=job37 --filename=/dev/nvme37n1 --name=job38 --filename=/dev/nvme38n1 --name=job39 --filename=/dev/nvme39n1 --name=job40 --filename=/dev/nvme40n1 --name=job41 --filename=/dev/nvme41n1 --name=job42 --filename=/dev/nvme42n1 --name=job43 --filename=/dev/nvme43n1 --name=job44 --filename=/dev/nvme44n1 --name=job45 --filename=/dev/nvme45n1 --name=job46 --filename=/dev/nvme46n1 --name=job47 --filename=/dev/nvme47n1 --name=job48 --filename=/dev/nvme48n1 --name=job49 --filename=/dev/nvme49n1 --name=job50 --filename=/dev/nvme50n1 --name=job51 --filename=/dev/nvme51n1 --name=job52 --filename=/dev/nvme52n1 --name=job53 --filename=/dev/nvme53n1 --name=job54 --filename=/dev/nvme54n1 --name=job55 --filename=/dev/nvme55n1 --name=job56 --filename=/dev/nvme56n1 --name=job57 --filename=/dev/nvme57n1 --name=job58 --filename=/dev/nvme58n1 --name=job59 --filename=/dev/nvme59n1 --name=job60 --filename=/dev/nvme60n1 --name=job61 --filename=/dev/nvme61n1 --name=job62 --filename=/dev/nvme62n1 --name=job63 --filename=/dev/nvme63n1 --name=job64 --filename=/dev/nvme64n1
    DEBUG: 2019/11/27 07:47:10 Successfully completed fio jobs on cluster nodes.
    DEBUG: 2019/11/27 07:47:10 Creating file system & mounting volumes on the node : appserv53
    DEBUG: 2019/11/27 07:47:10 Creating file system & mounting volumes on the node : appserv54
    DEBUG: 2019/11/27 07:47:10 Creating file system & mounting volumes on the node : appserv55
    DEBUG: 2019/11/27 07:48:22 Umount volumes and delete mount directory on node : appserv54
    DEBUG: 2019/11/27 07:48:24 Umount volumes and delete mount directory on node : appserv55
    DEBUG: 2019/11/27 07:48:25 Umount volumes and delete mount directory on node : appserv53
    DEBUG: 2019/11/27 07:48:27 Detaching volumes on the node : appserv54
    DEBUG: 2019/11/27 07:48:27 Detaching volumes on the node : appserv55
    DEBUG: 2019/11/27 07:48:27 Detaching volumes on the node : appserv53
    DEBUG: 2019/11/27 07:48:47 Deleting volumes on the node : appserv53
    DEBUG: 2019/11/27 07:48:47 Deleting volumes on the node : appserv54
    DEBUG: 2019/11/27 07:48:47 Deleting volumes on the node : appserv55
    DEBUG: 2019/11/27 07:50:03 Rebooting all the nodes simultaneously and checking for FBM and L1 consistency
Powering OFF the node appserv55
Powering OFF the node appserv53
Powering OFF the node appserv54
    DEBUG: 2019/11/27 07:50:03 Node 172.16.6.153 took 0 seconds to power off
    DEBUG: 2019/11/27 07:50:04 Node 172.16.6.155 took 0 seconds to power off
    DEBUG: 2019/11/27 07:50:04 Node 172.16.6.154 took 0 seconds to power off
Powering ON the node appserv53
Powering ON the node appserv55
Powering ON the node appserv54
    DEBUG: 2019/11/27 07:50:35 Node 172.16.6.153 took 1 seconds to power on
    DEBUG: 2019/11/27 07:50:35 Node 172.16.6.155 took 1 seconds to power on
    DEBUG: 2019/11/27 07:50:35 Node 172.16.6.154 took 1 seconds to power on
    DEBUG: 2019/11/27 07:50:35 Waiting for nodes to come up, will wait upto 800 seconds
............    DEBUG: 2019/11/27 07:52:54 Nodes are up, waiting for armada to start
.......
    DEBUG: 2019/11/27 07:55:04 Found '3' nodes
    DEBUG: 2019/11/27 07:55:04 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 07:55:04 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 07:55:04 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 07:55:04 Recording timestamp of all services on all nodes
[AfterEach] Storage volume basic test
  /gocode/main/test/e2e/tests/volume.go:936
    DEBUG: 2019/11/27 07:55:12 END_TEST LocalStorage.Basic Time-taken : 1434.343950084
    DEBUG: 2019/11/27 07:55:12 Checking stale resources
    DEBUG: 2019/11/27 07:55:12 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:55:12 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:55:12 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:1434.595 seconds][0m
LocalStorage.Basic Daily S_Basic-1.0 S_Basic-1.1 S_Basic-1.4 S_Verify-1.1 S_Verify-1.2 S_Limit-1.0 S_Limit-1.3
[90m/gocode/main/test/e2e/tests/volume.go:922[0m
  Storage volume basic test
  [90m/gocode/main/test/e2e/tests/volume.go:925[0m
    storage volume basic operations
    [90m/gocode/main/test/e2e/tests/volume.go:941[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4[0m [90mPing pod's IP from outside world[0m 
  [1mPing pod's endpoint from outside world using public network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:733[0m
[BeforeEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:700
    DEBUG: 2019/11/27 07:55:13 START_TEST Network.PingPodFromOutside
    DEBUG: 2019/11/27 07:55:13 Login to cluster
    DEBUG: 2019/11/27 07:55:13 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:55:13 Updating inventory struct
    DEBUG: 2019/11/27 07:55:14 Checking stale resources
    DEBUG: 2019/11/27 07:55:14 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:55:14 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:55:14 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:55:23 Creating storage classes
[It] Ping pod's endpoint from outside world using public network
  /gocode/main/test/e2e/tests/network-pod.go:733
    DEBUG: 2019/11/27 07:55:32 Creating a endpoints : test-ep 
    DEBUG: 2019/11/27 07:55:33 Creating 1 pod(s) of docker.io/redis:3.0.5 image with provided endpoint(s)
    DEBUG: 2019/11/27 07:55:35 IP address ( 172.16.179.6 ) of eptest-pod-1 is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/27 07:55:35 Trying to ping the eptest-pod-1
    DEBUG: 2019/11/27 07:55:36 Executing ping command: ping  -c 5 -W 5 172.16.179.6
    DEBUG: 2019/11/27 07:55:40 172.16.179.6 is pingable from local machine
    DEBUG: 2019/11/27 07:55:40 Deleting pods : 
    DEBUG: 2019/11/27 07:55:55 Deleting endpoint : test-ep 
[AfterEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:709
    DEBUG: 2019/11/27 07:55:55 END_TEST Network.PingPodFromOutside Time-taken : 42.447944245
    DEBUG: 2019/11/27 07:55:55 Checking stale resources
    DEBUG: 2019/11/27 07:55:55 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:55:55 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:55:55 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:42.681 seconds][0m
Network.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4
[90m/gocode/main/test/e2e/tests/network-pod.go:694[0m
  Ping pod's IP from outside world
  [90m/gocode/main/test/e2e/tests/network-pod.go:695[0m
    Ping pod's endpoint from outside world using public network
    [90m/gocode/main/test/e2e/tests/network-pod.go:733[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries creating qos template without providing mandatory paramters min bw or min iops and by providing optional parameteres maxBW & maxIOPs.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:198[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/27 07:55:55 Login to cluster
    DEBUG: 2019/11/27 07:55:56 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:55:56 Updating inventory struct
    DEBUG: 2019/11/27 07:55:57 Checking stale resources
    DEBUG: 2019/11/27 07:55:57 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:55:57 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:55:57 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:56:05 Creating storage classes
    DEBUG: 2019/11/27 07:56:14 START_TEST PerfTier.NegativeTests
[It] tries creating qos template without providing mandatory paramters min bw or min iops and by providing optional parameteres maxBW & maxIOPs.
  /gocode/main/test/e2e/tests/perf-tier.go:198
    DEBUG: 2019/11/27 07:56:14 Try to create perf-tier without mandatory parameteres "minBw" & "minIops": 
    ERROR: 2019/11/27 07:56:14  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -B 1G -I 50k', output:, error:Error: Missing storage IOPs option.



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/27 07:56:14 END_TEST PerfTier.NegativeTests Time-taken: 0.067791823
    DEBUG: 2019/11/27 07:56:14 Checking stale resources
    DEBUG: 2019/11/27 07:56:14 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:56:14 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:56:14 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.229 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries creating qos template without providing mandatory paramters min bw or min iops and by providing optional parameteres maxBW & maxIOPs.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:198[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetworkRemoteStorage.NicVFsSchedulingTrafficFlowSameDirection Daily AT_Scheduling-1.0 Multizone[0m [90mNetwork plus remote storage nic & VFs scheduling, Traffic flows in same direction[0m 
  [1mNetwork plus remote storage nic & VFs scheduling, Traffic flows in same direction[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:1760[0m
[BeforeEach] Network plus remote storage nic & VFs scheduling, Traffic flows in same direction
  /gocode/main/test/e2e/tests/network-pod.go:1743
    DEBUG: 2019/11/27 07:56:14 START_TEST NetworkRemoteStorage.NicVFsSchedulingTrafficFlowSameDirection
    DEBUG: 2019/11/27 07:56:14 Login to cluster
    DEBUG: 2019/11/27 07:56:15 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:56:15 Updating inventory struct
    DEBUG: 2019/11/27 07:56:16 Checking stale resources
    DEBUG: 2019/11/27 07:56:16 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:56:16 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:56:16 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:56:25 Creating storage classes
[It] Network plus remote storage nic & VFs scheduling, Traffic flows in same direction
  /gocode/main/test/e2e/tests/network-pod.go:1760
    DEBUG: 2019/11/27 07:56:34 Creating 1 pair of iperf client-server pod
    DEBUG: 2019/11/27 07:56:34 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/27 07:56:36 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/27 07:57:06 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/27 07:57:09 Getting pods scheduled on network nicIds
    DEBUG: 2019/11/27 07:57:09 Checking distribution of network pods across nicId(s)
    DEBUG: 2019/11/27 07:57:09 Pod scheduled as expected
    DEBUG: 2019/11/27 07:57:09 Creating fio pod and remote volume with high qos: 
    DEBUG: 2019/11/27 07:57:09 Create 1 fio pod(s):
    DEBUG: 2019/11/27 07:57:09 Creating dynamic pvc : fio-pod-hightest-vol1
    DEBUG: 2019/11/27 07:57:09 Created PVC successfully.
    DEBUG: 2019/11/27 07:57:09 Creating fio pod: fio-pod-high-1
    DEBUG: 2019/11/27 07:57:10 Checking if given pods are in Running state
    DEBUG: 2019/11/27 07:57:19 Getting pods and volumes scheduled on storage nicIds
    DEBUG: 2019/11/27 07:57:19 Checking distribution of storage pods across nicId(s)
    DEBUG: 2019/11/27 07:57:19 Pod scheduled as expected
    DEBUG: 2019/11/27 07:57:19 Deleting all the pods: 
    DEBUG: 2019/11/27 07:57:45 Waitting for volume to move to "Available" state
    DEBUG: 2019/11/27 07:57:45 Delete PVCs: 
    DEBUG: 2019/11/27 07:57:45 Waiting for volumes to get deleted: 
[AfterEach] Network plus remote storage nic & VFs scheduling, Traffic flows in same direction
  /gocode/main/test/e2e/tests/network-pod.go:1754
    DEBUG: 2019/11/27 07:58:45 END_TEST NetworkRemoteStorage.NicVFsSchedulingTrafficFlowSameDirection Time-taken : 150.217518765
    DEBUG: 2019/11/27 07:58:45 Checking stale resources
    DEBUG: 2019/11/27 07:58:45 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:58:45 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:58:45 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:150.478 seconds][0m
NetworkRemoteStorage.NicVFsSchedulingTrafficFlowSameDirection Daily AT_Scheduling-1.0 Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:1737[0m
  Network plus remote storage nic & VFs scheduling, Traffic flows in same direction
  [90m/gocode/main/test/e2e/tests/network-pod.go:1738[0m
    Network plus remote storage nic & VFs scheduling, Traffic flows in same direction
    [90m/gocode/main/test/e2e/tests/network-pod.go:1760[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.Cluster Daily Rbac_Cluster-1.0 Rbac_Cluster-1.1[0m [90mrbac cluster test[0m 
  [1mcluster-view test[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:228[0m
[BeforeEach] rbac cluster test
  /gocode/main/test/e2e/tests/rbac.go:215
    DEBUG: 2019/11/27 07:58:45 START_TEST Rbac.Cluster
    DEBUG: 2019/11/27 07:58:45 Login to cluster
    DEBUG: 2019/11/27 07:58:46 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:58:46 Updating inventory struct
    DEBUG: 2019/11/27 07:58:47 Checking stale resources
    DEBUG: 2019/11/27 07:58:47 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:58:47 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:58:47 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:58:55 Creating storage classes
[It] cluster-view test
  /gocode/main/test/e2e/tests/rbac.go:228
    DEBUG: 2019/11/27 07:59:05 Create group
    DEBUG: 2019/11/27 07:59:05 Create user
    DEBUG: 2019/11/27 07:59:05 Login as user
    DEBUG: 2019/11/27 07:59:06 List Networks
    DEBUG: 2019/11/27 07:59:06 List users
    DEBUG: 2019/11/27 07:59:06 List groups
    DEBUG: 2019/11/27 07:59:06 List roles
    DEBUG: 2019/11/27 07:59:06 List auth-server
    DEBUG: 2019/11/27 07:59:06 Create user
    DEBUG: 2019/11/27 07:59:06 Create group
    DEBUG: 2019/11/27 07:59:07 Create role
    DEBUG: 2019/11/27 07:59:07 Create auth-server
    DEBUG: 2019/11/27 07:59:07 Create Network
[AfterEach] rbac cluster test
  /gocode/main/test/e2e/tests/rbac.go:222
    DEBUG: 2019/11/27 07:59:08 END_TEST Rbac.Cluster Time-taken : 23.091941835
    DEBUG: 2019/11/27 07:59:08 Checking stale resources
    DEBUG: 2019/11/27 07:59:08 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:59:08 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:59:08 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:23.339 seconds][0m
Rbac.Cluster Daily Rbac_Cluster-1.0 Rbac_Cluster-1.1
[90m/gocode/main/test/e2e/tests/rbac.go:207[0m
  rbac cluster test
  [90m/gocode/main/test/e2e/tests/rbac.go:210[0m
    cluster-view test
    [90m/gocode/main/test/e2e/tests/rbac.go:228[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mDelete the same network twice.[0m
  [37m/gocode/main/test/e2e/tests/network.go:123[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:35
    DEBUG: 2019/11/27 07:59:08 START_TEST Network.NegativeTests
    DEBUG: 2019/11/27 07:59:08 Login to cluster
    DEBUG: 2019/11/27 07:59:09 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:59:09 Updating inventory struct
    DEBUG: 2019/11/27 07:59:10 Checking stale resources
    DEBUG: 2019/11/27 07:59:10 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:59:10 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:59:10 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:59:18 Creating storage classes
[It] Delete the same network twice.
  /gocode/main/test/e2e/tests/network.go:123
    DEBUG: 2019/11/27 07:59:27 Create a valid network.
    DEBUG: 2019/11/27 07:59:28 Delete the newly added network.
    DEBUG: 2019/11/27 07:59:28 Try to delete the same network again.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:48
    DEBUG: 2019/11/27 07:59:28 END_TEST Network.NegativeTests Time-taken : 19.63880135
    DEBUG: 2019/11/27 07:59:28 Checking stale resources
    DEBUG: 2019/11/27 07:59:28 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:59:28 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:59:28 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.881 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:26[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:27[0m
    Delete the same network twice.
    [90m/gocode/main/test/e2e/tests/network.go:123[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.VFsSchedulingWithCustomQos Daily AT_Scheduling-3.1 Qos Multizone[0m [90mPod with custom qos(5G) should schedule on one nicID and pods with high qos should schedule on other nicID[0m 
  [1mCreate pods with custom, high qos and check scheduling on nicID(s)[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:1612[0m
[BeforeEach] Pod with custom qos(5G) should schedule on one nicID and pods with high qos should schedule on other nicID
  /gocode/main/test/e2e/tests/network-pod.go:1599
    DEBUG: 2019/11/27 07:59:28 START_TEST Network.VFsSchedulingWithCustomQos
    DEBUG: 2019/11/27 07:59:28 Login to cluster
    DEBUG: 2019/11/27 07:59:29 Checking basic Vnic usage
    DEBUG: 2019/11/27 07:59:29 Updating inventory struct
    DEBUG: 2019/11/27 07:59:30 Checking stale resources
    DEBUG: 2019/11/27 07:59:30 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 07:59:30 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 07:59:30 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 07:59:38 Creating storage classes
[It] Create pods with custom, high qos and check scheduling on nicID(s)
  /gocode/main/test/e2e/tests/network-pod.go:1612
    DEBUG: 2019/11/27 07:59:47 Create the perf-tier custom with 5G network bandwidth.
    DEBUG: 2019/11/27 07:59:52 Creating 1 pod with custom qos: 
    DEBUG: 2019/11/27 07:59:52 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/27 07:59:52 Getting node label of appserv53: 
    DEBUG: 2019/11/27 07:59:52 Creating iperf server pod: iperf-custom-1
    DEBUG: 2019/11/27 07:59:53 Checking if given pods are in Running state
    DEBUG: 2019/11/27 07:59:55 Creating 10 pods with high qos: 
    DEBUG: 2019/11/27 07:59:55 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/27 07:59:55 Getting node label of appserv53: 
    DEBUG: 2019/11/27 07:59:55 Creating iperf server pod: iperf-high-1
    DEBUG: 2019/11/27 07:59:56 Creating iperf server pod: iperf-high-2
    DEBUG: 2019/11/27 07:59:56 Creating iperf server pod: iperf-high-3
    DEBUG: 2019/11/27 07:59:56 Creating iperf server pod: iperf-high-4
    DEBUG: 2019/11/27 07:59:57 Creating iperf server pod: iperf-high-5
    DEBUG: 2019/11/27 07:59:57 Creating iperf server pod: iperf-high-6
    DEBUG: 2019/11/27 07:59:57 Creating iperf server pod: iperf-high-7
    DEBUG: 2019/11/27 07:59:58 Creating iperf server pod: iperf-high-8
    DEBUG: 2019/11/27 07:59:58 Creating iperf server pod: iperf-high-9
    DEBUG: 2019/11/27 07:59:58 Creating iperf server pod: iperf-high-10
    DEBUG: 2019/11/27 07:59:59 Checking if given pods are in Running state
    DEBUG: 2019/11/27 08:00:03 Pod scheduled as expected
    DEBUG: 2019/11/27 08:00:03 Deleting all the pods: 
    DEBUG: 2019/11/27 08:01:55 Delete the perf-tier custom
[AfterEach] Pod with custom qos(5G) should schedule on one nicID and pods with high qos should schedule on other nicID
  /gocode/main/test/e2e/tests/network-pod.go:1607
    DEBUG: 2019/11/27 08:01:55 END_TEST Network.VFsSchedulingWithCustomQos Time-taken : 146.81528217
    DEBUG: 2019/11/27 08:01:55 Checking stale resources
    DEBUG: 2019/11/27 08:01:55 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:01:55 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:01:55 Checking stale resources on the node: appserv53

[32mâ€¢ [SLOW TEST:147.061 seconds][0m
Network.VFsSchedulingWithCustomQos Daily AT_Scheduling-3.1 Qos Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:1593[0m
  Pod with custom qos(5G) should schedule on one nicID and pods with high qos should schedule on other nicID
  [90m/gocode/main/test/e2e/tests/network-pod.go:1594[0m
    Create pods with custom, high qos and check scheduling on nicID(s)
    [90m/gocode/main/test/e2e/tests/network-pod.go:1612[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mMirroring.OnlinePlexDeleteWithAddPlex Daily SM_PlexDelete-1.2[0m [90mCreate mirrored volumes and delete plexes from these volumes and add new plex[0m 
  [1mCreate mirrored volumes and delete plexes from these volumes and add new plex[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:2149[0m
[BeforeEach] Create mirrored volumes and delete plexes from these volumes and add new plex
  /gocode/main/test/e2e/tests/mirroring.go:2134
    DEBUG: 2019/11/27 08:01:55 START_TEST Mirroring.OnlinePlexDeleteWithAddPlex
    DEBUG: 2019/11/27 08:01:55 Login to cluster
    DEBUG: 2019/11/27 08:01:56 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:01:56 Updating inventory struct
    DEBUG: 2019/11/27 08:01:57 Checking stale resources
    DEBUG: 2019/11/27 08:01:57 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:01:57 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:01:57 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:02:05 Creating storage classes
[It] Create mirrored volumes and delete plexes from these volumes and add new plex
  /gocode/main/test/e2e/tests/mirroring.go:2149
    DEBUG: 2019/11/27 08:02:14 Creating 8 volumes. Mirror Count: 3:
    DEBUG: 2019/11/27 08:02:14 Mirror Count: 3
    DEBUG: 2019/11/27 08:02:16 Attaching volumes: 
    DEBUG: 2019/11/27 08:03:01 Running write fio job on all volumes: 
    DEBUG: 2019/11/27 08:03:02 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:03:02 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 08:03:02 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:05:03 Running read fio job on all volumes: 
    DEBUG: 2019/11/27 08:05:05 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 08:05:05 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 08:05:05 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 08:05:06 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 08:05:06 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 08:05:06 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 08:05:07 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 08:05:07 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 08:05:08 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 08:05:08 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:05:08 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:06:19 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 08:06:19 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 08:06:20 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 08:06:21 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 08:06:21 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 08:06:21 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 08:06:22 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 08:06:22 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 08:06:23 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:06:24 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 08:06:24 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:07:33 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 08:07:35 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/27 08:07:35 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 08:07:36 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 08:07:37 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 08:07:37 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/27 08:07:38 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 08:07:38 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/27 08:07:39 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:07:40 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/27 08:07:40 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:08:52 Removing a plex from each volume. Expected PlexCount: 2
    DEBUG: 2019/11/27 08:09:47 Removing a plex from each volume. Expected PlexCount: 1
    DEBUG: 2019/11/27 08:10:47 Adding new plex to each volume. Expected PlexCount 2: 
    DEBUG: 2019/11/27 08:10:50 Volume name & Plex : test-vol1.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 08:10:51 Volume "test-vol1" has index "0" in embedded.
    DEBUG: 2019/11/27 08:10:52 Volume: test-vol1. Resync offset: 82

    DEBUG: 2019/11/27 08:10:53 Volume: test-vol1. Resync offset: 91

    DEBUG: 2019/11/27 08:10:53 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:10:53 Volume name & Plex : test-vol2.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 08:10:54 Volume "test-vol2" has index "1" in embedded.
    DEBUG: 2019/11/27 08:10:55 Volume: test-vol2. Resync offset: 18

    DEBUG: 2019/11/27 08:10:56 Volume: test-vol2. Resync offset: 20

    DEBUG: 2019/11/27 08:10:56 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:10:56 Volume name & Plex : test-vol3.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 08:10:57 Volume "test-vol3" has index "2" in embedded.
    DEBUG: 2019/11/27 08:10:58 Volume: test-vol3. Resync offset: 17

    DEBUG: 2019/11/27 08:10:59 Volume: test-vol3. Resync offset: 18

    DEBUG: 2019/11/27 08:10:59 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:10:59 Volume name & Plex : test-vol4.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:00 Volume "test-vol4" has index "3" in embedded.
    DEBUG: 2019/11/27 08:11:01 Volume: test-vol4. Resync offset: 14

    DEBUG: 2019/11/27 08:11:01 Volume: test-vol4. Resync offset: 15

    DEBUG: 2019/11/27 08:11:01 Volume name & Plex : test-vol4.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:11:02 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:11:02 Volume name & Plex : test-vol5.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:03 Volume "test-vol5" has index "4" in embedded.
    DEBUG: 2019/11/27 08:11:04 Volume: test-vol5. Resync offset: 16

    DEBUG: 2019/11/27 08:11:04 Volume: test-vol5. Resync offset: 16

    DEBUG: 2019/11/27 08:11:10 Volume: test-vol5. Resync offset: 24

    DEBUG: 2019/11/27 08:11:10 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:11:10 Volume name & Plex : test-vol6.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:11 Volume "test-vol6" has index "5" in embedded.
    DEBUG: 2019/11/27 08:11:12 Volume: test-vol6. Resync offset: 20

    DEBUG: 2019/11/27 08:11:13 Volume: test-vol6. Resync offset: 21

    DEBUG: 2019/11/27 08:11:13 Volume name & Plex : test-vol7.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:14 Volume "test-vol7" has index "6" in embedded.
    DEBUG: 2019/11/27 08:11:15 Volume: test-vol7. Resync offset: 21

    DEBUG: 2019/11/27 08:11:15 Volume: test-vol7. Resync offset: 21

    DEBUG: 2019/11/27 08:11:21 Volume: test-vol7. Resync offset: 26

    DEBUG: 2019/11/27 08:11:21 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:11:21 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:11:21 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:23 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 08:11:23 Volume: test-vol8. Resync offset: 25

    DEBUG: 2019/11/27 08:11:24 Volume: test-vol8. Resync offset: 25

    DEBUG: 2019/11/27 08:11:30 Volume: test-vol8. Resync offset: 30

    DEBUG: 2019/11/27 08:11:30 Number of volumes : 8
    DEBUG: 2019/11/27 08:11:30 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/27 08:11:30 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:11:30 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:31 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 08:11:32 Volume: test-vol8. Resync offset: 31

    DEBUG: 2019/11/27 08:11:33 Volume: test-vol8. Resync offset: 32

    DEBUG: 2019/11/27 08:11:33 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/27 08:11:33 Volume name & Plex : test-vol4.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:34 Volume "test-vol4" has index "3" in embedded.
    DEBUG: 2019/11/27 08:11:35 Volume: test-vol4. Resync offset: 60

    DEBUG: 2019/11/27 08:11:35 Volume: test-vol4. Resync offset: 61

    DEBUG: 2019/11/27 08:11:35 Volume name & Plex : test-vol4.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:11:35 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 08:11:35 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:11:35 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:11:35 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 08:11:35 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 08:11:36 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:11:36 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:11:36 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 08:11:36 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 08:11:36 Volume name & Plex : test-vol3.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:37 Volume "test-vol3" has index "2" in embedded.
    DEBUG: 2019/11/27 08:11:38 Volume: test-vol3. Resync offset: 93

    DEBUG: 2019/11/27 08:11:38 Volume: test-vol3. Resync offset: 94

    DEBUG: 2019/11/27 08:11:38 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:11:38 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/27 08:11:39 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:11:39 Volume name & Plex : test-vol6.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:40 Volume "test-vol6" has index "5" in embedded.
    DEBUG: 2019/11/27 08:11:40 Volume: test-vol6. Resync offset: 46

    DEBUG: 2019/11/27 08:11:41 Volume: test-vol6. Resync offset: 47

    DEBUG: 2019/11/27 08:11:41 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/27 08:11:41 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:11:41 Volume name & Plex : test-vol5.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:43 Volume "test-vol5" has index "4" in embedded.
    DEBUG: 2019/11/27 08:11:43 Volume: test-vol5. Resync offset: 64

    DEBUG: 2019/11/27 08:11:44 Volume: test-vol5. Resync offset: 64

    DEBUG: 2019/11/27 08:11:50 Volume: test-vol5. Resync offset: 71

    DEBUG: 2019/11/27 08:11:50 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/27 08:11:50 Volume name & Plex : test-vol7.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:51 Volume "test-vol7" has index "6" in embedded.
    DEBUG: 2019/11/27 08:11:52 Volume: test-vol7. Resync offset: 53

    DEBUG: 2019/11/27 08:11:53 Volume: test-vol7. Resync offset: 54

    DEBUG: 2019/11/27 08:11:53 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:11:53 Adding new plex to each volume. Expected PlexCount 3: 
    DEBUG: 2019/11/27 08:11:56 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:11:56 Volume name & Plex : test-vol1.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:11:57 Volume "test-vol1" has index "0" in embedded.
    DEBUG: 2019/11/27 08:11:58 Volume: test-vol1. Resync offset: 
    DEBUG: 2019/11/27 08:11:59 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:11:59 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:11:59 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:11:59 Volume name & Plex : test-vol2.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:00 Volume "test-vol2" has index "1" in embedded.
    DEBUG: 2019/11/27 08:12:01 Volume: test-vol2. Resync offset: 23

    DEBUG: 2019/11/27 08:12:01 Volume: test-vol2. Resync offset: 25

    DEBUG: 2019/11/27 08:12:02 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:02 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:12:02 Volume name & Plex : test-vol3.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:03 Volume "test-vol3" has index "2" in embedded.
    DEBUG: 2019/11/27 08:12:04 Volume: test-vol3. Resync offset: 15

    DEBUG: 2019/11/27 08:12:04 Volume: test-vol3. Resync offset: 16

    DEBUG: 2019/11/27 08:12:04 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:04 Volume name & Plex : test-vol4.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:06 Volume "test-vol4" has index "3" in embedded.
    DEBUG: 2019/11/27 08:12:06 Volume: test-vol4. Resync offset: 4

    DEBUG: 2019/11/27 08:12:07 Volume: test-vol4. Resync offset: 4

    DEBUG: 2019/11/27 08:12:13 Volume: test-vol4. Resync offset: 13

    DEBUG: 2019/11/27 08:12:13 Volume name & Plex : test-vol4.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:12:13 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:13 Volume name & Plex : test-vol5.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:14 Volume "test-vol5" has index "4" in embedded.
    DEBUG: 2019/11/27 08:12:15 Volume: test-vol5. Resync offset: 98

    DEBUG: 2019/11/27 08:12:16 Volume: test-vol5. Resync offset: 99

    DEBUG: 2019/11/27 08:12:16 Volume name & Plex : test-vol5.p2. Plex State : 
    DEBUG: 2019/11/27 08:12:16 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:16 Volume name & Plex : test-vol6.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:17 Volume "test-vol6" has index "5" in embedded.
    DEBUG: 2019/11/27 08:12:18 Volume: test-vol6. Resync offset: 79

    DEBUG: 2019/11/27 08:12:18 Volume: test-vol6. Resync offset: 80

    DEBUG: 2019/11/27 08:12:18 Volume name & Plex : test-vol6.p2. Plex State : 
    DEBUG: 2019/11/27 08:12:19 Volume name & Plex : test-vol7.p0. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:20 Volume "test-vol7" has index "6" in embedded.
    DEBUG: 2019/11/27 08:12:21 Volume: test-vol7. Resync offset: 75

    DEBUG: 2019/11/27 08:12:21 Volume: test-vol7. Resync offset: 76

    DEBUG: 2019/11/27 08:12:21 Volume name & Plex : test-vol7.p1. Plex State : 
    DEBUG: 2019/11/27 08:12:21 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:12:21 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:21 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:23 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 08:12:23 Volume: test-vol8. Resync offset: 70

    DEBUG: 2019/11/27 08:12:24 Volume: test-vol8. Resync offset: 70

    DEBUG: 2019/11/27 08:12:30 Volume: test-vol8. Resync offset: 75

    DEBUG: 2019/11/27 08:12:30 Volume name & Plex : test-vol8.p2. Plex State : 
    DEBUG: 2019/11/27 08:12:30 Number of volumes : 8
    DEBUG: 2019/11/27 08:12:30 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/27 08:12:30 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:30 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:31 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 08:12:32 Volume: test-vol8. Resync offset: 76

    DEBUG: 2019/11/27 08:12:33 Volume: test-vol8. Resync offset: 77

    DEBUG: 2019/11/27 08:12:33 Volume name & Plex : test-vol8.p2. Plex State : 
    DEBUG: 2019/11/27 08:12:33 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/27 08:12:33 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:33 Volume name & Plex : test-vol4.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:34 Volume "test-vol4" has index "3" in embedded.
    DEBUG: 2019/11/27 08:12:35 Volume: test-vol4. Resync offset: 45

    DEBUG: 2019/11/27 08:12:35 Volume: test-vol4. Resync offset: 45

    DEBUG: 2019/11/27 08:12:41 Volume: test-vol4. Resync offset: 54

    DEBUG: 2019/11/27 08:12:41 Volume name & Plex : test-vol4.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:12:41 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 08:12:41 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:41 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:12:41 Volume name & Plex : test-vol3.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:43 Volume "test-vol3" has index "2" in embedded.
    DEBUG: 2019/11/27 08:12:43 Volume: test-vol3. Resync offset: 88

    DEBUG: 2019/11/27 08:12:44 Volume: test-vol3. Resync offset: 89

    DEBUG: 2019/11/27 08:12:44 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 08:12:44 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:44 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:12:44 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:12:44 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 08:12:44 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 08:12:44 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:44 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:12:44 Volume name & Plex : test-vol2.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:12:44 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 08:12:44 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/27 08:12:44 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:44 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:12:44 Volume name & Plex : test-vol6.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:46 Volume "test-vol6" has index "5" in embedded.
    DEBUG: 2019/11/27 08:12:46 Volume: test-vol6. Resync offset: 6

    DEBUG: 2019/11/27 08:12:47 Volume: test-vol6. Resync offset: 6

    DEBUG: 2019/11/27 08:12:53 Volume: test-vol6. Resync offset: 12

    DEBUG: 2019/11/27 08:12:53 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/27 08:12:53 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:12:53 Volume name & Plex : test-vol7.p1. Plex State : Resyncing
    DEBUG: 2019/11/27 08:12:54 Volume "test-vol7" has index "6" in embedded.
    DEBUG: 2019/11/27 08:12:55 Volume: test-vol7. Resync offset: 3

    DEBUG: 2019/11/27 08:12:56 Volume: test-vol7. Resync offset: 3

    DEBUG: 2019/11/27 08:13:01 Volume: test-vol7. Resync offset: 8

    DEBUG: 2019/11/27 08:13:01 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:13:01 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/27 08:13:01 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:13:01 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:13:01 Volume name & Plex : test-vol5.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 08:13:03 Volume "test-vol5" has index "4" in embedded.
    DEBUG: 2019/11/27 08:13:04 Volume: test-vol5. Resync offset: 50

    DEBUG: 2019/11/27 08:13:04 Volume: test-vol5. Resync offset: 51

    DEBUG: 2019/11/27 08:13:04 Wait till resync completion on all volumes
    DEBUG: 2019/11/27 08:13:04 Number of volumes : 8
    DEBUG: 2019/11/27 08:13:04 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/27 08:13:05 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:13:05 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:13:05 Volume name & Plex : test-vol8.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 08:13:06 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 08:13:06 Volume: test-vol8. Resync offset: 2

    DEBUG: 2019/11/27 08:13:07 Volume: test-vol8. Resync offset: 3

    DEBUG: 2019/11/27 08:13:07 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 08:13:37 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:13:37 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:13:37 Volume name & Plex : test-vol8.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 08:13:39 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 08:13:39 Volume: test-vol8. Resync offset: 28

    DEBUG: 2019/11/27 08:13:40 Volume: test-vol8. Resync offset: 29

    DEBUG: 2019/11/27 08:13:40 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 08:14:10 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:14:10 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:14:10 Volume name & Plex : test-vol8.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 08:14:11 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 08:14:12 Volume: test-vol8. Resync offset: 55

    DEBUG: 2019/11/27 08:14:13 Volume: test-vol8. Resync offset: 56

    DEBUG: 2019/11/27 08:14:13 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 08:14:43 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:14:43 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:14:43 Volume name & Plex : test-vol8.p2. Plex State : Resyncing
    DEBUG: 2019/11/27 08:14:44 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/27 08:14:45 Volume: test-vol8. Resync offset: 80

    DEBUG: 2019/11/27 08:14:46 Volume: test-vol8. Resync offset: 81

    DEBUG: 2019/11/27 08:14:46 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol8.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/27 08:15:16 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol4.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/27 08:15:16 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 08:15:16 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol2.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 08:15:16 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol3.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/27 08:15:16 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 Volume name & Plex : test-vol6.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:15:16 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/27 08:15:16 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/27 08:15:17 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:15:17 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:15:17 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:15:17 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/27 08:15:17 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/27 08:15:17 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/27 08:15:17 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/27 08:15:17 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/27 08:15:17 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/27 08:15:17 Running verify fio on all volumes across all plexes: 
    DEBUG: 2019/11/27 08:15:19 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 08:15:19 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 08:15:19 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 08:15:20 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/27 08:15:20 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 08:15:20 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 08:15:21 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 08:15:21 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/27 08:15:21 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/27 08:15:22 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:15:22 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:16:06 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 08:16:07 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 08:16:07 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 08:16:07 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 08:16:08 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 08:16:08 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/27 08:16:08 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 08:16:10 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/27 08:16:10 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/27 08:16:10 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:16:10 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:17:22 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 08:17:23 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/27 08:17:24 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/27 08:17:25 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 08:17:25 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 08:17:26 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 08:17:27 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/27 08:17:28 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/27 08:17:28 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/27 08:17:28 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:17:29 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/27 08:18:42 Detach & delete all volumes: 
[AfterEach] Create mirrored volumes and delete plexes from these volumes and add new plex
  /gocode/main/test/e2e/tests/mirroring.go:2144
    DEBUG: 2019/11/27 08:19:47 END_TEST Mirroring.OnlinePlexDeleteWithAddPlex Time-taken : 1071.802535222
    DEBUG: 2019/11/27 08:19:47 Checking stale resources
    DEBUG: 2019/11/27 08:19:47 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:19:47 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:19:47 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:1072.054 seconds][0m
Mirroring.OnlinePlexDeleteWithAddPlex Daily SM_PlexDelete-1.2
[90m/gocode/main/test/e2e/tests/mirroring.go:2128[0m
  Create mirrored volumes and delete plexes from these volumes and add new plex
  [90m/gocode/main/test/e2e/tests/mirroring.go:2129[0m
    Create mirrored volumes and delete plexes from these volumes and add new plex
    [90m/gocode/main/test/e2e/tests/mirroring.go:2149[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mCluster.AfterRebootCreateResourcesOnQuorumNode Management Daily M_Cluster-1.13[0m [90mReboot the quorum node and create volumes and pods on that node when the node joins back. Master doesn't change[0m 
  [1mReboot all quorum node(s) one by one and create volumes and pods on that node when the node joins back. Master doesn't change[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:722[0m
[BeforeEach] Reboot the quorum node and create volumes and pods on that node when the node joins back. Master doesn't change
  /gocode/main/test/e2e/tests/cluster.go:708
    DEBUG: 2019/11/27 08:19:47 START_TEST Cluster.AfterRebootCreateResourcesOnQuorumNode
    DEBUG: 2019/11/27 08:19:47 Login to cluster
    DEBUG: 2019/11/27 08:19:48 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:19:48 Updating inventory struct
    DEBUG: 2019/11/27 08:19:49 Checking stale resources
    DEBUG: 2019/11/27 08:19:49 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:19:49 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:19:49 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:19:57 Creating storage classes
[It] Reboot all quorum node(s) one by one and create volumes and pods on that node when the node joins back. Master doesn't change
  /gocode/main/test/e2e/tests/cluster.go:722
    DEBUG: 2019/11/27 08:20:07 Getting master
    DEBUG: 2019/11/27 08:20:07 Getting cluster quorum nodes
    DEBUG: 2019/11/27 08:20:07 Rebooting cluster quorum node "appserv53"
    DEBUG: 2019/11/27 08:20:07 Getting cluster quorum nodes
    DEBUG: 2019/11/27 08:20:07 Powering OFF the node appserv53
    DEBUG: 2019/11/27 08:20:07 Getting random fail node type
    DEBUG: 2019/11/27 08:20:08 Node 172.16.6.153 took 0 seconds to power off
    DEBUG: 2019/11/27 08:20:08 Ensuring that appserv53 node is unreachable: 
    DEBUG: 2019/11/27 08:20:08 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 08:20:17 Polling to check until node: appserv53 goes down
    DEBUG: 2019/11/27 08:21:39 Powering ON the node appserv53
    DEBUG: 2019/11/27 08:21:39 Node 172.16.6.153 took 0 seconds to power on
    DEBUG: 2019/11/27 08:21:39 Checking if node appserv53 is reachable or not: 
    DEBUG: 2019/11/27 08:21:39 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 08:21:58 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 08:22:15 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 08:22:32 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 08:22:49 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 08:23:06 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 08:23:23 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 08:23:40 Executing ping command: ping  -c 5 -W 5 appserv53
    DEBUG: 2019/11/27 08:23:44 appserv53 is pingable from local machine
    DEBUG: 2019/11/27 08:23:44 Checking ssh port is up or not on node: appserv53
    DEBUG: 2019/11/27 08:24:14 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/27 08:24:14 Found '3' nodes
    DEBUG: 2019/11/27 08:24:14 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 08:24:14 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 08:24:14 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 08:25:18 After power cycle/reboot, updating timestamp of node : appserv53
    DEBUG: 2019/11/27 08:25:20 Getting cluster quorum nodes
    DEBUG: 2019/11/27 08:26:20 Updating inventory struct
    DEBUG: 2019/11/27 08:26:21 Checking ssh port is up or not:
    DEBUG: 2019/11/27 08:26:21 Getting master
    DEBUG: 2019/11/27 08:26:21 Getting node label
    DEBUG: 2019/11/27 08:26:21 Create 5 fio pod(s):
    DEBUG: 2019/11/27 08:26:21 Creating dynamic pvc : fio-podtest-vol1
    DEBUG: 2019/11/27 08:26:22 Created PVC successfully.
    DEBUG: 2019/11/27 08:26:22 Creating fio pod: fio-pod-1
    DEBUG: 2019/11/27 08:26:22 Creating dynamic pvc : fio-podtest-vol2
    DEBUG: 2019/11/27 08:26:22 Created PVC successfully.
    DEBUG: 2019/11/27 08:26:22 Creating fio pod: fio-pod-2
    DEBUG: 2019/11/27 08:26:23 Creating dynamic pvc : fio-podtest-vol3
    DEBUG: 2019/11/27 08:26:23 Created PVC successfully.
    DEBUG: 2019/11/27 08:26:23 Creating fio pod: fio-pod-3
    DEBUG: 2019/11/27 08:26:23 Creating dynamic pvc : fio-podtest-vol4
    DEBUG: 2019/11/27 08:26:24 Created PVC successfully.
    DEBUG: 2019/11/27 08:26:24 Creating fio pod: fio-pod-4
    DEBUG: 2019/11/27 08:26:24 Creating dynamic pvc : fio-podtest-vol5
    DEBUG: 2019/11/27 08:26:24 Created PVC successfully.
    DEBUG: 2019/11/27 08:26:24 Creating fio pod: fio-pod-5
    DEBUG: 2019/11/27 08:26:25 Checking if given pods are in Running state
    DEBUG: 2019/11/27 08:26:37 Deleting pods : 
    DEBUG: 2019/11/27 08:26:55 Wait for volumes to come in Available state: 
    DEBUG: 2019/11/27 08:26:55 Delete PVCs: 
    DEBUG: 2019/11/27 08:26:55 Waiting for volumes to get deleted: 
    DEBUG: 2019/11/27 08:27:47 Rebooting cluster quorum node "appserv54"
    DEBUG: 2019/11/27 08:27:47 Getting cluster quorum nodes
    DEBUG: 2019/11/27 08:27:47 Powering OFF the node appserv54
    DEBUG: 2019/11/27 08:27:47 Getting random fail node type
    DEBUG: 2019/11/27 08:27:47 Doing sync on 172.16.6.154
    DEBUG: 2019/11/27 08:27:48 Ensuring that appserv54 node is unreachable: 
    DEBUG: 2019/11/27 08:27:48 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:27:52 appserv54 is pingable from local machine
    DEBUG: 2019/11/27 08:28:02 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:28:06 appserv54 is pingable from local machine
    DEBUG: 2019/11/27 08:28:16 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:28:25 Polling to check until node: appserv54 goes down
    DEBUG: 2019/11/27 08:29:39 Powering ON the node appserv54
    DEBUG: 2019/11/27 08:29:39 Node 172.16.6.154 took 0 seconds to power on
    DEBUG: 2019/11/27 08:29:39 Checking if node appserv54 is reachable or not: 
    DEBUG: 2019/11/27 08:29:39 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:29:56 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:30:13 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:30:30 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:30:47 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:31:04 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:31:21 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:31:38 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:31:55 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/27 08:31:59 appserv54 is pingable from local machine
    DEBUG: 2019/11/27 08:31:59 Checking ssh port is up or not on node: appserv54
    DEBUG: 2019/11/27 08:32:29 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/27 08:32:29 Found '3' nodes
    DEBUG: 2019/11/27 08:32:29 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 08:33:11 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 08:33:11 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 08:33:21 After power cycle/reboot, updating timestamp of node : appserv54
    DEBUG: 2019/11/27 08:33:24 Getting cluster quorum nodes
    DEBUG: 2019/11/27 08:34:24 Updating inventory struct
    DEBUG: 2019/11/27 08:34:25 Checking ssh port is up or not:
    DEBUG: 2019/11/27 08:34:25 Getting master
    DEBUG: 2019/11/27 08:34:25 Getting node label
    DEBUG: 2019/11/27 08:34:25 Create 5 fio pod(s):
    DEBUG: 2019/11/27 08:34:25 Creating dynamic pvc : fio-podtest-vol1
    DEBUG: 2019/11/27 08:34:26 Created PVC successfully.
    DEBUG: 2019/11/27 08:34:26 Creating fio pod: fio-pod-1
    DEBUG: 2019/11/27 08:34:27 Creating dynamic pvc : fio-podtest-vol2
    DEBUG: 2019/11/27 08:34:27 Created PVC successfully.
    DEBUG: 2019/11/27 08:34:27 Creating fio pod: fio-pod-2
    DEBUG: 2019/11/27 08:34:27 Creating dynamic pvc : fio-podtest-vol3
    DEBUG: 2019/11/27 08:34:28 Created PVC successfully.
    DEBUG: 2019/11/27 08:34:28 Creating fio pod: fio-pod-3
    DEBUG: 2019/11/27 08:34:28 Creating dynamic pvc : fio-podtest-vol4
    DEBUG: 2019/11/27 08:34:28 Created PVC successfully.
    DEBUG: 2019/11/27 08:34:28 Creating fio pod: fio-pod-4
    DEBUG: 2019/11/27 08:34:29 Creating dynamic pvc : fio-podtest-vol5
    DEBUG: 2019/11/27 08:34:29 Created PVC successfully.
    DEBUG: 2019/11/27 08:34:29 Creating fio pod: fio-pod-5
    DEBUG: 2019/11/27 08:34:29 Checking if given pods are in Running state
    DEBUG: 2019/11/27 08:34:42 Deleting pods : 
    DEBUG: 2019/11/27 08:35:02 Wait for volumes to come in Available state: 
    DEBUG: 2019/11/27 08:35:02 Delete PVCs: 
    DEBUG: 2019/11/27 08:35:02 Waiting for volumes to get deleted: 
[AfterEach] Reboot the quorum node and create volumes and pods on that node when the node joins back. Master doesn't change
  /gocode/main/test/e2e/tests/cluster.go:717
    DEBUG: 2019/11/27 08:35:47 END_TEST Cluster.AfterRebootCreateResourcesOnQuorumNode Time-taken : 959.969458758
    DEBUG: 2019/11/27 08:35:47 Checking stale resources
    DEBUG: 2019/11/27 08:35:47 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:35:47 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:35:47 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:960.209 seconds][0m
Cluster.AfterRebootCreateResourcesOnQuorumNode Management Daily M_Cluster-1.13
[90m/gocode/main/test/e2e/tests/cluster.go:699[0m
  Reboot the quorum node and create volumes and pods on that node when the node joins back. Master doesn't change
  [90m/gocode/main/test/e2e/tests/cluster.go:700[0m
    Reboot all quorum node(s) one by one and create volumes and pods on that node when the node joins back. Master doesn't change
    [90m/gocode/main/test/e2e/tests/cluster.go:722[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries to create perf tier with invalid Bandwidth.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:190[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/27 08:35:48 Login to cluster
    DEBUG: 2019/11/27 08:35:48 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:35:48 Updating inventory struct
    DEBUG: 2019/11/27 08:35:49 Checking stale resources
    DEBUG: 2019/11/27 08:35:49 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:35:49 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:35:49 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:35:58 Creating storage classes
    DEBUG: 2019/11/27 08:36:06 START_TEST PerfTier.NegativeTests
[It] tries to create perf tier with invalid Bandwidth.
  /gocode/main/test/e2e/tests/perf-tier.go:190
    DEBUG: 2019/11/27 08:36:06 Try to create perf-tier with invalid Bandwidth.
    ERROR: 2019/11/27 08:36:06  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b -500M -i 50k', output:, error:Error: Invalid --network-bw/-b specification. Input should be in K, M, G format



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/27 08:36:06 END_TEST PerfTier.NegativeTests Time-taken: 0.077558691
    DEBUG: 2019/11/27 08:36:06 Checking stale resources
    DEBUG: 2019/11/27 08:36:07 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:36:07 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:36:07 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.151 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries to create perf tier with invalid Bandwidth.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:190[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mPod.Schedule Management Daily M_Pod-1.12[0m [90mpod scheduling tests[0m 
  [1mPod scheduling[0m
  [37m/gocode/main/test/e2e/tests/pod.go:162[0m
[BeforeEach] pod scheduling tests
  /gocode/main/test/e2e/tests/pod.go:151
    DEBUG: 2019/11/27 08:36:07 START_TEST Pod.Schedule
    DEBUG: 2019/11/27 08:36:07 Login to cluster
    DEBUG: 2019/11/27 08:36:07 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:36:07 Updating inventory struct
    DEBUG: 2019/11/27 08:36:08 Checking stale resources
    DEBUG: 2019/11/27 08:36:08 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:36:08 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:36:08 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:36:17 Creating storage classes
[It] Pod scheduling
  /gocode/main/test/e2e/tests/pod.go:162
    DEBUG: 2019/11/27 08:36:25 Create Pod1
    DEBUG: 2019/11/27 08:36:28 Create Pod2
    DEBUG: 2019/11/27 08:36:31 Check for Pod spreading
    DEBUG: 2019/11/27 08:36:31 Create Pod to test node affinity
    DEBUG: 2019/11/27 08:36:34 Deleting pod: testpod101
    DEBUG: 2019/11/27 08:36:44 Deleting pod: testpod102
    DEBUG: 2019/11/27 08:36:48 Deleting pod: testpod103
    DEBUG: 2019/11/27 08:37:04 Make sure that blue network usage is zero
[AfterEach] pod scheduling tests
  /gocode/main/test/e2e/tests/pod.go:157
    DEBUG: 2019/11/27 08:37:04 END_TEST Pod.Schedule Time-taken : 57.109034778
    DEBUG: 2019/11/27 08:37:04 Checking stale resources
    DEBUG: 2019/11/27 08:37:04 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:37:04 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:37:04 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:57.344 seconds][0m
Pod.Schedule Management Daily M_Pod-1.12
[90m/gocode/main/test/e2e/tests/pod.go:144[0m
  pod scheduling tests
  [90m/gocode/main/test/e2e/tests/pod.go:146[0m
    Pod scheduling
    [90m/gocode/main/test/e2e/tests/pod.go:162[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries creating qos template with min iops > max iops.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:204[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/27 08:37:04 Login to cluster
    DEBUG: 2019/11/27 08:37:05 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:37:05 Updating inventory struct
    DEBUG: 2019/11/27 08:37:06 Checking stale resources
    DEBUG: 2019/11/27 08:37:06 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:37:06 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:37:06 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:37:14 Creating storage classes
    DEBUG: 2019/11/27 08:37:23 START_TEST PerfTier.NegativeTests
[It] tries creating qos template with min iops > max iops.
  /gocode/main/test/e2e/tests/perf-tier.go:204
    DEBUG: 2019/11/27 08:37:23 Try creating qos template with min iops > max iops.
    ERROR: 2019/11/27 08:37:23  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b 1G -i 50k -I 20k', status:&{{Status } {  0} Failure Performance tier validation failed, error: [spec.storage-iops: Invalid value: 20000: Maximum storage IOPS cannot be less than minimum storage IOPS] BadRequest <nil> 400}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "Performance tier validation failed, error: [spec.storage-iops: Invalid value: 20000: Maximum storage IOPS cannot be less than minimum storage IOPS]",
 "reason": "BadRequest",
 "code": 400
}



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/27 08:37:23 END_TEST PerfTier.NegativeTests Time-taken: 0.104451184
    DEBUG: 2019/11/27 08:37:23 Checking stale resources
    DEBUG: 2019/11/27 08:37:23 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:37:23 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:37:23 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.298 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries creating qos template with min iops > max iops.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:204[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries creating qos template with name more than 63 characters.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:235[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/27 08:37:23 Login to cluster
    DEBUG: 2019/11/27 08:37:24 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:37:24 Updating inventory struct
    DEBUG: 2019/11/27 08:37:25 Checking stale resources
    DEBUG: 2019/11/27 08:37:25 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:37:25 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:37:25 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:37:33 Creating storage classes
    DEBUG: 2019/11/27 08:37:42 START_TEST PerfTier.NegativeTests
[It] tries creating qos template with name more than 63 characters.
  /gocode/main/test/e2e/tests/perf-tier.go:235
    DEBUG: 2019/11/27 08:37:42 Try to create qos with perf-tier Name Length 64
    ERROR: 2019/11/27 08:37:42  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa -b 1G -i 50k', status:&{{Status } {  0} Failure Performance tier validation failed, error: [metadata.name: Invalid value: "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa": must be no more than 63 characters] BadRequest <nil> 400}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "Performance tier validation failed, error: [metadata.name: Invalid value: \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\": must be no more than 63 characters]",
 "reason": "BadRequest",
 "code": 400
}



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/27 08:37:42 END_TEST PerfTier.NegativeTests Time-taken: 0.091277376
    DEBUG: 2019/11/27 08:37:42 Checking stale resources
    DEBUG: 2019/11/27 08:37:42 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:37:42 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:37:42 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.223 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries creating qos template with name more than 63 characters.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:235[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.AllContainerEditViewLocal Daily Rbac_Local_Basic-2.2[0m [90mUser can edit/view in all namespaces with allcontainer edit/view role[0m 
  [1mUser can edit/view allcontainer(s) in all namespaces[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:1089[0m
[BeforeEach] User can edit/view in all namespaces with allcontainer edit/view role
  /gocode/main/test/e2e/tests/rbac.go:1077
    DEBUG: 2019/11/27 08:37:43 START_TEST Rbac.AllContainerEditViewLocal
    DEBUG: 2019/11/27 08:37:43 Login to cluster
    DEBUG: 2019/11/27 08:37:43 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:37:43 Updating inventory struct
    DEBUG: 2019/11/27 08:37:44 Checking stale resources
    DEBUG: 2019/11/27 08:37:44 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:37:44 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:37:44 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:37:52 Creating storage classes
[It] User can edit/view allcontainer(s) in all namespaces
  /gocode/main/test/e2e/tests/rbac.go:1089
    DEBUG: 2019/11/27 08:38:01 Creating group group0 with allcontainer-edit role(s)
    DEBUG: 2019/11/27 08:38:01 Creating group group1 with container-edit/nondefault role(s)
    DEBUG: 2019/11/27 08:38:01 Creating group group2 with volume-edit,volumeclaim-edit/nondefault role(s)
    DEBUG: 2019/11/27 08:38:01 Creating group group3 with volume-edit,volumeclaim-edit/default role(s)
    DEBUG: 2019/11/27 08:38:02 Creating local-user0 user
    DEBUG: 2019/11/27 08:38:02 Creating local-user1 user
    DEBUG: 2019/11/27 08:38:02 Creating local-user2 user
    DEBUG: 2019/11/27 08:38:02 Creating local-user3 user
    DEBUG: 2019/11/27 08:38:02 Login as local-user2 user in nondefault namespace
    DEBUG: 2019/11/27 08:38:18 Create volume
    DEBUG: 2019/11/27 08:38:18 Create pvc in nondefault namespace
    DEBUG: 2019/11/27 08:38:18 Created PVC successfully.
    DEBUG: 2019/11/27 08:38:18 Login as local-user3 user in default namespace
    DEBUG: 2019/11/27 08:38:34 Create volume
    DEBUG: 2019/11/27 08:38:34 Create pvc in default namespace
    DEBUG: 2019/11/27 08:38:35 Created PVC successfully.
    DEBUG: 2019/11/27 08:38:35 Login as local-user0 user
    DEBUG: 2019/11/27 08:38:35 Creating a pair of iperf client-server pod.
    DEBUG: 2019/11/27 08:38:35 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/27 08:38:36 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/27 08:39:06 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/27 08:39:06 Checking if given pods are in Running state
    DEBUG: 2019/11/27 08:39:06 Checking if given pods are in Running state
    DEBUG: 2019/11/27 08:39:08 Creating fio pod fio-pod in default namespace
    DEBUG: 2019/11/27 08:39:13 Creating a pair of iperf client-server pod.
    DEBUG: 2019/11/27 08:39:13 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/27 08:39:14 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/27 08:39:44 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/27 08:39:44 Checking if given pods are in Running state
    DEBUG: 2019/11/27 08:39:44 Checking if given pods are in Running state
    DEBUG: 2019/11/27 08:39:54 Creating fio pod fio-pod in nondefault namespace
    DEBUG: 2019/11/27 08:40:14 Editing group0 group with allcontainer-view role(s)
    DEBUG: 2019/11/27 08:40:14 Login as local-user0 user
    DEBUG: 2019/11/27 08:40:15 List pods from default namepsace: 
    DEBUG: 2019/11/27 08:40:15 List pods from nondefault namepsace: 
    DEBUG: 2019/11/27 08:40:15 List PVCs in default namespace
    DEBUG: 2019/11/27 08:40:15 List Endpoints in default namespace
    DEBUG: 2019/11/27 08:40:15 List Services in default namespace
    DEBUG: 2019/11/27 08:40:15 Try to create the pod in default namespace
    DEBUG: 2019/11/27 08:40:16 Try to create service in default namespace
    DEBUG: 2019/11/27 08:40:16 Try to delete pods in default namespace
    DEBUG: 2019/11/27 08:40:16 Try to delete services in default namespace
    DEBUG: 2019/11/27 08:40:16 Try to delete endpoints in default namespace
    DEBUG: 2019/11/27 08:40:16 List PVCs in nondefault namespace
    DEBUG: 2019/11/27 08:40:16 List Endpoints in nondefault namespace
    DEBUG: 2019/11/27 08:40:16 List Services in nondefault namespace
    DEBUG: 2019/11/27 08:40:17 Try to create the pod in default namespace
    DEBUG: 2019/11/27 08:40:17 Try to create service in default namespace
    DEBUG: 2019/11/27 08:40:17 Try to delete pods in nondefault namespace
    DEBUG: 2019/11/27 08:40:17 Try to delete services in nondefault namespace
    DEBUG: 2019/11/27 08:40:17 Try to delete endpoints in nondefault namespace
    DEBUG: 2019/11/27 08:40:18 Editing group0 group with allcontainer-edit role(s)
    DEBUG: 2019/11/27 08:40:18 Login as local-user0 user
    DEBUG: 2019/11/27 08:40:19 List pods from default namepsace: 
    DEBUG: 2019/11/27 08:40:19 Delete pods in default namespace
    DEBUG: 2019/11/27 08:40:34 List PVCs in default namespace
    DEBUG: 2019/11/27 08:40:34 List Services in default namespace
    DEBUG: 2019/11/27 08:40:34 Delete services in default namespace
    DEBUG: 2019/11/27 08:40:34 List Endpoints in default namespace
    DEBUG: 2019/11/27 08:40:35 List pods from nondefault namepsace: 
    DEBUG: 2019/11/27 08:40:35 Delete pods in nondefault namespace
    DEBUG: 2019/11/27 08:40:46 List PVCs in nondefault namespace
    DEBUG: 2019/11/27 08:40:46 List Services in nondefault namespace
    DEBUG: 2019/11/27 08:40:46 Delete services in nondefault namespace
    DEBUG: 2019/11/27 08:40:47 List Endpoints in nondefault namespace
    DEBUG: 2019/11/27 08:40:47 Login as local-user2 user in nondefault namespace
    DEBUG: 2019/11/27 08:41:02 Delete PVC in nondefault namespace
    DEBUG: 2019/11/27 08:41:03 Login as local-user3 user in default namespace
    DEBUG: 2019/11/27 08:41:18 Delete PVC in default namespace
    DEBUG: 2019/11/27 08:41:18 Wait for volumes to come in Available state
    DEBUG: 2019/11/27 08:41:19 Delete Volume
    DEBUG: 2019/11/27 08:43:18 Delete users
    DEBUG: 2019/11/27 08:43:18 Delete groups
[AfterEach] User can edit/view in all namespaces with allcontainer edit/view role
  /gocode/main/test/e2e/tests/rbac.go:1084
    DEBUG: 2019/11/27 08:43:18 END_TEST Rbac.AllContainerEditViewLocal Time-taken : 335.73773799
    DEBUG: 2019/11/27 08:43:18 Checking stale resources
    DEBUG: 2019/11/27 08:43:18 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:43:18 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:43:18 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:335.984 seconds][0m
Rbac.AllContainerEditViewLocal Daily Rbac_Local_Basic-2.2
[90m/gocode/main/test/e2e/tests/rbac.go:1065[0m
  User can edit/view in all namespaces with allcontainer edit/view role
  [90m/gocode/main/test/e2e/tests/rbac.go:1067[0m
    User can edit/view allcontainer(s) in all namespaces
    [90m/gocode/main/test/e2e/tests/rbac.go:1089[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mBenchmarking.NetworkUniDirectionalTwoPorts Daily AT_Benchmark-1.1 Qos Multizone[0m [90mNetwork uni-directional benchmarking with two network ports[0m 
  [1mNetwork uni-directional benchmarking with two network ports, should get 18G bandwidth[0m
  [37m/gocode/main/test/e2e/tests/benchmarking.go:93[0m
[BeforeEach] Network uni-directional benchmarking with two network ports
  /gocode/main/test/e2e/tests/benchmarking.go:79
    DEBUG: 2019/11/27 08:43:19 START_TEST Benchmarking.NetworkUniDirectionalTwoPorts
    DEBUG: 2019/11/27 08:43:19 Login to cluster
    DEBUG: 2019/11/27 08:43:19 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:43:19 Updating inventory struct
    DEBUG: 2019/11/27 08:43:20 Checking stale resources
    DEBUG: 2019/11/27 08:43:20 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:43:20 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:43:20 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:43:29 Creating storage classes
[It] Network uni-directional benchmarking with two network ports, should get 18G bandwidth
  /gocode/main/test/e2e/tests/benchmarking.go:93
    DEBUG: 2019/11/27 08:43:38 Creating four pairs of iperf client-server pods
    DEBUG: 2019/11/27 08:43:38 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/27 08:43:40 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/27 08:43:40 Creating iperf server pod: iperf-server-high2
    DEBUG: 2019/11/27 08:43:43 Creating service with name: iperf-server-high2
    DEBUG: 2019/11/27 08:43:43 Creating iperf server pod: iperf-server-high3
    DEBUG: 2019/11/27 08:43:46 Creating service with name: iperf-server-high3
    DEBUG: 2019/11/27 08:43:46 Creating iperf server pod: iperf-server-high4
    DEBUG: 2019/11/27 08:43:49 Creating service with name: iperf-server-high4
    DEBUG: 2019/11/27 08:44:19 Creating iperf Client pod: iperf-client-high1
    DEBUG: 2019/11/27 08:44:24 Creating iperf Client pod: iperf-client-high2
    DEBUG: 2019/11/27 08:44:28 Creating iperf Client pod: iperf-client-high3
    DEBUG: 2019/11/27 08:44:33 Creating iperf Client pod: iperf-client-high4
    DEBUG: 2019/11/27 08:44:36 Sleeping for 180 seconds, so that prometheus database will have some stats.
    DEBUG: 2019/11/27 08:47:36 Validating if bandwidth is honored or not for server pods:
    DEBUG: 2019/11/27 08:47:37 QoS honored for pod: iperf-server-high1
    DEBUG: 2019/11/27 08:47:37 QoS honored for pod: iperf-server-high2
    DEBUG: 2019/11/27 08:47:38 QoS honored for pod: iperf-server-high3
    DEBUG: 2019/11/27 08:47:38 QoS honored for pod: iperf-server-high4
    DEBUG: 2019/11/27 08:47:38 Validating if bandwidth is honored or not for client pods:
    DEBUG: 2019/11/27 08:47:39 QoS honored for pod: iperf-client-high1
    DEBUG: 2019/11/27 08:47:39 QoS honored for pod: iperf-client-high2
    DEBUG: 2019/11/27 08:47:40 QoS honored for pod: iperf-client-high3
    DEBUG: 2019/11/27 08:47:40 QoS honored for pod: iperf-client-high4
    DEBUG: 2019/11/27 08:47:40 Measuring throughput. num of links used: 2
    DEBUG: 2019/11/27 08:47:41 Node: appserv53. Expected Throughput: 16200000000. RX Throughput: 19167171831. TX Throughput: 0
    DEBUG: 2019/11/27 08:47:41 Node: appserv54. Expected Throughput: 16200000000. RX Throughput: 0. TX Throughput: 19162089631
    DEBUG: 2019/11/27 08:47:41 Deleting pods:
    DEBUG: 2019/11/27 08:47:41 Deleting pods : 
    DEBUG: 2019/11/27 08:48:04 Delete services: 
    DEBUG: 2019/11/27 08:48:04 Deleting service(s)
[AfterEach] Network uni-directional benchmarking with two network ports
  /gocode/main/test/e2e/tests/benchmarking.go:88
    DEBUG: 2019/11/27 08:48:05 END_TEST Benchmarking.NetworkUniDirectionalTwoPorts Time-taken : 286.36176660900003
    DEBUG: 2019/11/27 08:48:05 Checking stale resources
    DEBUG: 2019/11/27 08:48:05 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:48:05 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:48:05 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:286.600 seconds][0m
Benchmarking.NetworkUniDirectionalTwoPorts Daily AT_Benchmark-1.1 Qos Multizone
[90m/gocode/main/test/e2e/tests/benchmarking.go:70[0m
  Network uni-directional benchmarking with two network ports
  [90m/gocode/main/test/e2e/tests/benchmarking.go:71[0m
    Network uni-directional benchmarking with two network ports, should get 18G bandwidth
    [90m/gocode/main/test/e2e/tests/benchmarking.go:93[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mVolume.NameLengthCheck Management Daily M_Volume-1.1[0m [90mwhen volume name is[0m 
  [1mequal to defined character limit[0m
  [37m/gocode/main/test/e2e/tests/volume.go:233[0m
[BeforeEach] when volume name is
  /gocode/main/test/e2e/tests/volume.go:220
    DEBUG: 2019/11/27 08:48:05 START_TEST Volume.NameLengthCheck
    DEBUG: 2019/11/27 08:48:05 Login to cluster
    DEBUG: 2019/11/27 08:48:06 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:48:06 Updating inventory struct
    DEBUG: 2019/11/27 08:48:07 Checking stale resources
    DEBUG: 2019/11/27 08:48:07 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:48:07 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:48:07 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:48:15 Creating storage classes
[It] equal to defined character limit
  /gocode/main/test/e2e/tests/volume.go:233
    DEBUG: 2019/11/27 08:48:24 Creating the volume
    DEBUG: 2019/11/27 08:48:24 Getting Volume
    DEBUG: 2019/11/27 08:48:24 Verify that volume status is Available
    DEBUG: 2019/11/27 08:48:24 Deleting the volume
[AfterEach] when volume name is
  /gocode/main/test/e2e/tests/volume.go:228
    DEBUG: 2019/11/27 08:48:48 END_TEST Volume.NameLengthCheck Time-taken : 42.695175487
    DEBUG: 2019/11/27 08:48:48 Checking stale resources
    DEBUG: 2019/11/27 08:48:48 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:48:48 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:48:48 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:42.925 seconds][0m
Volume.NameLengthCheck Management Daily M_Volume-1.1
[90m/gocode/main/test/e2e/tests/volume.go:210[0m
  when volume name is
  [90m/gocode/main/test/e2e/tests/volume.go:213[0m
    equal to defined character limit
    [90m/gocode/main/test/e2e/tests/volume.go:233[0m
[90m------------------------------[0m
[0mEndpoint.Basic Daily N_Endpoint-1.4 N_Endpoint-1.5[0m [90mEndpoint Basic testcases[0m 
  [1mCreate the endpoint with same name again[0m
  [37m/gocode/main/test/e2e/tests/endpoint.go:57[0m
[BeforeEach] Endpoint Basic testcases
  /gocode/main/test/e2e/tests/endpoint.go:31
    DEBUG: 2019/11/27 08:48:48 Login to cluster
    DEBUG: 2019/11/27 08:48:49 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:48:49 Updating inventory struct
    DEBUG: 2019/11/27 08:48:50 Checking stale resources
    DEBUG: 2019/11/27 08:48:50 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:48:50 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:48:50 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:48:58 Creating storage classes
    DEBUG: 2019/11/27 08:49:07 START_TEST Endpoint.Basic
[It] Create the endpoint with same name again
  /gocode/main/test/e2e/tests/endpoint.go:57
    DEBUG: 2019/11/27 08:49:07 Create a endpoint.
    DEBUG: 2019/11/27 08:49:07 Try to create endpoint with the same name again.
    DEBUG: 2019/11/27 08:49:07 Endpoint create command failed: failed to run commmand 'dctl  -o json endpoint create ep5 -ns default -n default -l custom-endpoint=true', status:&{{Status } {  0} Failure Endpoint "default/ep5" already exists AlreadyExists 0xc00019a140 409}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "Endpoint \"default/ep5\" already exists",
 "reason": "AlreadyExists",
 "details": {
  "name": "default/ep5",
  "kind": "Endpoint"
 },
 "code": 409
}



    DEBUG: 2019/11/27 08:49:07 Delete the endpoint.
[AfterEach] Endpoint Basic testcases
  /gocode/main/test/e2e/tests/endpoint.go:41
    DEBUG: 2019/11/27 08:49:07 END_TEST Endpoint.Basic Time-taken : 0.405056849
    DEBUG: 2019/11/27 08:49:07 Checking stale resources
    DEBUG: 2019/11/27 08:49:07 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:49:07 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:49:07 Checking stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:19.559 seconds][0m
Endpoint.Basic Daily N_Endpoint-1.4 N_Endpoint-1.5
[90m/gocode/main/test/e2e/tests/endpoint.go:23[0m
  Endpoint Basic testcases
  [90m/gocode/main/test/e2e/tests/endpoint.go:30[0m
    Create the endpoint with same name again
    [90m/gocode/main/test/e2e/tests/endpoint.go:57[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.UniDirectionalQosValidationAcrossBothNics Daily AT_Qos-1.5 AT_Qos-1.6 Qos Multizone[0m [90mValidate Qos on two node with iperfpod[0m 
  [1mNetwork uni-directional port 0 with medium QoS and port 2 with high QoS, QoS should be honoured[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:3508[0m
[BeforeEach] Validate Qos on two node with iperfpod
  /gocode/main/test/e2e/tests/network-pod.go:3488
    DEBUG: 2019/11/27 08:49:08 START_TEST Network.UniDirectionalQosValidationAcrossBothNics
    DEBUG: 2019/11/27 08:49:08 Login to cluster
    DEBUG: 2019/11/27 08:49:08 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:49:08 Updating inventory struct
    DEBUG: 2019/11/27 08:49:09 Checking stale resources
    DEBUG: 2019/11/27 08:49:09 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:49:09 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:49:09 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:49:18 Creating storage classes
[It] Network uni-directional port 0 with medium QoS and port 2 with high QoS, QoS should be honoured
  /gocode/main/test/e2e/tests/network-pod.go:3508
    DEBUG: 2019/11/27 08:49:27 Creating perf-tier : template
    DEBUG: 2019/11/27 08:49:27 Creating 2 pairs of iperf client-server pods with template QoS
    DEBUG: 2019/11/27 08:49:27 Creating iperf server pod: iperf-server-template1
    DEBUG: 2019/11/27 08:49:30 Creating service with name: iperf-server-template1
    DEBUG: 2019/11/27 08:49:30 Creating iperf server pod: iperf-server-template2
    DEBUG: 2019/11/27 08:49:32 Creating service with name: iperf-server-template2
    DEBUG: 2019/11/27 08:50:03 Creating iperf Client pod: iperf-client-template1
    DEBUG: 2019/11/27 08:50:05 Creating iperf Client pod: iperf-client-template2
    DEBUG: 2019/11/27 08:50:12 Deleting pods scheduled on nic 2 with template QoS
    DEBUG: 2019/11/27 08:50:12 Deleting pods : 
    DEBUG: 2019/11/27 08:50:24 Deleting pods : 
    DEBUG: 2019/11/27 08:50:31 Creating 4 pairs of iperf client-server pods with high QoS on nic 2
    DEBUG: 2019/11/27 08:50:31 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/27 08:50:34 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/27 08:50:34 Creating iperf server pod: iperf-server-high2
    DEBUG: 2019/11/27 08:50:37 Creating service with name: iperf-server-high2
    DEBUG: 2019/11/27 08:50:37 Creating iperf server pod: iperf-server-high3
    DEBUG: 2019/11/27 08:50:39 Creating service with name: iperf-server-high3
    DEBUG: 2019/11/27 08:50:39 Creating iperf server pod: iperf-server-high4
    DEBUG: 2019/11/27 08:50:42 Creating service with name: iperf-server-high4
    DEBUG: 2019/11/27 08:51:12 Creating iperf Client pod: iperf-client-high1
    DEBUG: 2019/11/27 08:51:15 Creating iperf Client pod: iperf-client-high2
    DEBUG: 2019/11/27 08:51:18 Creating iperf Client pod: iperf-client-high3
    DEBUG: 2019/11/27 08:51:20 Creating iperf Client pod: iperf-client-high4
    DEBUG: 2019/11/27 08:51:23 Getting all pods which are scheduled on nicId 2 
    DEBUG: 2019/11/27 08:51:23 Getting all pods which are scheduled on nicId 2 
    DEBUG: 2019/11/27 08:51:23 Deleting pods scheduled on nic 0 with template QoS
    DEBUG: 2019/11/27 08:51:23 Deleting pods : 
    DEBUG: 2019/11/27 08:51:34 Deleting pods : 
    DEBUG: 2019/11/27 08:51:41 Creating 4 pairs of iperf client-server pods with medium QoS on nic 0
    DEBUG: 2019/11/27 08:51:41 Creating iperf server pod: iperf-server-medium1
    DEBUG: 2019/11/27 08:51:44 Creating service with name: iperf-server-medium1
    DEBUG: 2019/11/27 08:51:44 Creating iperf server pod: iperf-server-medium2
    DEBUG: 2019/11/27 08:51:47 Creating service with name: iperf-server-medium2
    DEBUG: 2019/11/27 08:51:47 Creating iperf server pod: iperf-server-medium3
    DEBUG: 2019/11/27 08:51:49 Creating service with name: iperf-server-medium3
    DEBUG: 2019/11/27 08:51:49 Creating iperf server pod: iperf-server-medium4
    DEBUG: 2019/11/27 08:51:52 Creating service with name: iperf-server-medium4
    DEBUG: 2019/11/27 08:52:22 Creating iperf Client pod: iperf-client-medium1
    DEBUG: 2019/11/27 08:52:25 Creating iperf Client pod: iperf-client-medium2
    DEBUG: 2019/11/27 08:52:28 Creating iperf Client pod: iperf-client-medium3
    DEBUG: 2019/11/27 08:52:30 Creating iperf Client pod: iperf-client-medium4
    DEBUG: 2019/11/27 08:52:33 Getting all pods which are scheduled on nicId 0 
    DEBUG: 2019/11/27 08:52:33 Getting all pods which are scheduled on nicId 0 
    DEBUG: 2019/11/27 08:52:33 Scheduling of all pods on both nics is validated
    DEBUG: 2019/11/27 08:52:33 Sleeping for 180 seconds, so that prometheus database will have some stats.
    DEBUG: 2019/11/27 08:55:33 Validating if bandwidth is honored or not for server pods:
    DEBUG: 2019/11/27 08:55:34 QoS honored for pod: iperf-server-high1
    DEBUG: 2019/11/27 08:55:35 QoS honored for pod: iperf-server-high2
    DEBUG: 2019/11/27 08:55:36 QoS honored for pod: iperf-server-high3
    DEBUG: 2019/11/27 08:55:36 QoS honored for pod: iperf-server-high4
    DEBUG: 2019/11/27 08:55:37 QoS honored for pod: iperf-server-medium1
    DEBUG: 2019/11/27 08:55:37 QoS honored for pod: iperf-server-medium2
    DEBUG: 2019/11/27 08:55:38 QoS honored for pod: iperf-server-medium3
    DEBUG: 2019/11/27 08:55:38 QoS honored for pod: iperf-server-medium4
    DEBUG: 2019/11/27 08:55:38 Validating if bandwidth is honored or not for client pods:
    DEBUG: 2019/11/27 08:55:39 QoS honored for pod: iperf-client-high1
    DEBUG: 2019/11/27 08:55:39 QoS honored for pod: iperf-client-high2
    DEBUG: 2019/11/27 08:55:40 QoS honored for pod: iperf-client-high3
    DEBUG: 2019/11/27 08:55:40 QoS honored for pod: iperf-client-high4
    DEBUG: 2019/11/27 08:55:41 QoS honored for pod: iperf-client-medium1
    DEBUG: 2019/11/27 08:55:42 QoS honored for pod: iperf-client-medium2
    DEBUG: 2019/11/27 08:55:42 QoS honored for pod: iperf-client-medium3
    DEBUG: 2019/11/27 08:55:43 QoS honored for pod: iperf-client-medium4
    DEBUG: 2019/11/27 08:55:43 Measuring throughput. num of links used: 2
    DEBUG: 2019/11/27 08:55:43 Node: appserv53. Expected Throughput: 16200000000. RX Throughput: 18474617225. TX Throughput: 0
    DEBUG: 2019/11/27 08:55:43 Node: appserv54. Expected Throughput: 16200000000. RX Throughput: 0. TX Throughput: 18452875402
    DEBUG: 2019/11/27 08:55:43 Deleting pods :
    DEBUG: 2019/11/27 08:55:43 Deleting pods : 
    DEBUG: 2019/11/27 08:56:07 Deleting pods : 
    DEBUG: 2019/11/27 08:56:34 Deleting perf-tier : template
    DEBUG: 2019/11/27 08:56:35 Deleting services : 
    DEBUG: 2019/11/27 08:56:35 Deleting service(s)
[AfterEach] Validate Qos on two node with iperfpod
  /gocode/main/test/e2e/tests/network-pod.go:3497
    DEBUG: 2019/11/27 08:56:36 END_TEST UniDirectionalQosValidationAcrossBothNics Time-taken : 448.227018534
    DEBUG: 2019/11/27 08:56:36 Checking stale resources
    DEBUG: 2019/11/27 08:56:36 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:56:36 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:56:36 Checking stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:448.460 seconds][0m
Network.UniDirectionalQosValidationAcrossBothNics Daily AT_Qos-1.5 AT_Qos-1.6 Qos Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:3480[0m
  Validate Qos on two node with iperfpod
  [90m/gocode/main/test/e2e/tests/network-pod.go:3481[0m
    Network uni-directional port 0 with medium QoS and port 2 with high QoS, QoS should be honoured
    [90m/gocode/main/test/e2e/tests/network-pod.go:3508[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mInvalid IP address range test[0m
  [37m/gocode/main/test/e2e/tests/network.go:81[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:35
    DEBUG: 2019/11/27 08:56:36 START_TEST Network.NegativeTests
    DEBUG: 2019/11/27 08:56:36 Login to cluster
    DEBUG: 2019/11/27 08:56:37 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:56:37 Updating inventory struct
    DEBUG: 2019/11/27 08:56:38 Checking stale resources
    DEBUG: 2019/11/27 08:56:38 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:56:38 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:56:38 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:56:46 Creating storage classes
[It] Invalid IP address range test
  /gocode/main/test/e2e/tests/network.go:81
    DEBUG: 2019/11/27 08:56:55 Try to create network with invalid IP address range.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:48
    DEBUG: 2019/11/27 08:56:55 END_TEST Network.NegativeTests Time-taken : 18.886829597
    DEBUG: 2019/11/27 08:56:55 Checking stale resources
    DEBUG: 2019/11/27 08:56:55 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:56:55 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:56:55 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.107 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:26[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:27[0m
    Invalid IP address range test
    [90m/gocode/main/test/e2e/tests/network.go:81[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14[0m [90mCheck if a pod's IP is pingable from other pod[0m 
  [1mCheck if a pod's IP is pingable from other pod using public network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:862[0m
[BeforeEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:848
    DEBUG: 2019/11/27 08:56:55 START_TEST Network.PingBetweenTwoPods
    DEBUG: 2019/11/27 08:56:55 Login to cluster
    DEBUG: 2019/11/27 08:56:56 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:56:56 Updating inventory struct
    DEBUG: 2019/11/27 08:56:57 Checking stale resources
    DEBUG: 2019/11/27 08:56:57 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:56:57 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:56:57 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:57:05 Creating storage classes
[It] Check if a pod's IP is pingable from other pod using public network
  /gocode/main/test/e2e/tests/network-pod.go:862
    DEBUG: 2019/11/27 08:57:14 Creating 2 pods of docker.io/redis:3.0.5 image with network : default and qos : high
    DEBUG: 2019/11/27 08:57:20 IP address ( 172.16.179.6 ) of e2etest-pod-1 is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/27 08:57:20 IP address ( 172.16.179.7 ) of e2etest-pod-2 is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/27 08:57:20 Trying to ping the e2etest-pod-2's IP from pod e2etest-pod-1
    DEBUG: 2019/11/27 08:57:20 172.16.179.7 is pingable from pod e2etest-pod-1 (172.16.179.6)
    DEBUG: 2019/11/27 08:57:20 Deleting pods : 
[AfterEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:857
    DEBUG: 2019/11/27 08:57:34 END_TEST Network.PingBetweenTwoPods Time-taken : 38.789419128
    DEBUG: 2019/11/27 08:57:34 Checking stale resources
    DEBUG: 2019/11/27 08:57:34 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:57:34 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:57:34 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:39.013 seconds][0m
Network.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14
[90m/gocode/main/test/e2e/tests/network-pod.go:842[0m
  Check if a pod's IP is pingable from other pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:843[0m
    Check if a pod's IP is pingable from other pod using public network
    [90m/gocode/main/test/e2e/tests/network-pod.go:862[0m
[90m------------------------------[0m
[0mRbac.EditView Daily Rbac_Local_Basic-2.0[0m [90mUser can edit/view in it's namespace[0m 
  [1mUser can edit/view node(s)[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:700[0m
[BeforeEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:528
    DEBUG: 2019/11/27 08:57:34 START_TEST Rbac.EditView
    DEBUG: 2019/11/27 08:57:34 Login to cluster
    DEBUG: 2019/11/27 08:57:35 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:57:35 Updating inventory struct
    DEBUG: 2019/11/27 08:57:36 Checking stale resources
    DEBUG: 2019/11/27 08:57:36 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:57:36 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:57:36 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:57:44 Creating storage classes
    DEBUG: 2019/11/27 08:57:53 User Logout
[It] User can edit/view node(s)
  /gocode/main/test/e2e/tests/rbac.go:700
    DEBUG: 2019/11/27 08:57:54 Creating group, user with role(s)
    DEBUG: 2019/11/27 08:57:55 Creating group jacksgroup with node-edit role(s)
    DEBUG: 2019/11/27 08:57:55 Creating jack user in jacksgroup group
    DEBUG: 2019/11/27 08:57:55 Login as jack user
    DEBUG: 2019/11/27 08:57:56 Listing nodes
    DEBUG: 2019/11/27 08:57:56 Performing operations on all nodes
    DEBUG: 2019/11/27 08:57:56 Node : appserv54
    DEBUG: 2019/11/27 08:57:56 Getting node info
    DEBUG: 2019/11/27 08:57:56 Getting network status of node
    DEBUG: 2019/11/27 08:57:56 Getting health status of node
    DEBUG: 2019/11/27 08:57:56 Rediscovering a node
    DEBUG: 2019/11/27 08:57:56 Getting label of a node
    DEBUG: 2019/11/27 08:57:56 Setting label to a node
    DEBUG: 2019/11/27 08:57:57 Node : appserv55
    DEBUG: 2019/11/27 08:57:57 Getting node info
    DEBUG: 2019/11/27 08:57:57 Getting network status of node
    DEBUG: 2019/11/27 08:57:57 Getting health status of node
    DEBUG: 2019/11/27 08:57:57 Rediscovering a node
    DEBUG: 2019/11/27 08:57:57 Getting label of a node
    DEBUG: 2019/11/27 08:57:57 Setting label to a node
    DEBUG: 2019/11/27 08:57:57 Node : appserv53
    DEBUG: 2019/11/27 08:57:57 Getting node info
    DEBUG: 2019/11/27 08:57:57 Getting network status of node
    DEBUG: 2019/11/27 08:57:57 Getting health status of node
    DEBUG: 2019/11/27 08:57:58 Rediscovering a node
    DEBUG: 2019/11/27 08:57:58 Getting label of a node
    DEBUG: 2019/11/27 08:57:58 Setting label to a node
    DEBUG: 2019/11/27 08:57:58 Getting cluster status
    DEBUG: 2019/11/27 08:57:58 Getting cluster non master node
[AfterEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:539
    DEBUG: 2019/11/27 08:57:58 User Logout
    DEBUG: 2019/11/27 08:57:59 END_TEST Rbac.EditView Time-taken : 25.1933865
    DEBUG: 2019/11/27 08:57:59 Checking stale resources
    DEBUG: 2019/11/27 08:57:59 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:57:59 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:57:59 Checking stale resources on the node: appserv55

[36m[1mS [SKIPPING] [25.434 seconds][0m
Rbac.EditView Daily Rbac_Local_Basic-2.0
[90m/gocode/main/test/e2e/tests/rbac.go:518[0m
  User can edit/view in it's namespace
  [90m/gocode/main/test/e2e/tests/rbac.go:520[0m
    [36m[1mUser can edit/view node(s) [It][0m
    [90m/gocode/main/test/e2e/tests/rbac.go:700[0m

    [36m TODO : write function for GetClusterNonQuorumNode because we can't remove Quorum node[0m

    /gocode/main/test/e2e/tests/rbac.go:752
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.EditView Daily Rbac_Local_Basic-2.0[0m [90mUser can edit/view in it's namespace[0m 
  [1mUser can edit/view user(s)[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:597[0m
[BeforeEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:528
    DEBUG: 2019/11/27 08:58:00 START_TEST Rbac.EditView
    DEBUG: 2019/11/27 08:58:00 Login to cluster
    DEBUG: 2019/11/27 08:58:00 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:58:00 Updating inventory struct
    DEBUG: 2019/11/27 08:58:01 Checking stale resources
    DEBUG: 2019/11/27 08:58:01 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:58:01 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:58:01 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:58:10 Creating storage classes
    DEBUG: 2019/11/27 08:58:18 User Logout
[It] User can edit/view user(s)
  /gocode/main/test/e2e/tests/rbac.go:597
    DEBUG: 2019/11/27 08:58:20 Creating group, user with role(s)
    DEBUG: 2019/11/27 08:58:20 Creating group jacksgroup with user-edit role(s)
    DEBUG: 2019/11/27 08:58:20 Creating jack user in jacksgroup group
    DEBUG: 2019/11/27 08:58:20 Login as jack user
    DEBUG: 2019/11/27 08:58:21 Creating group and user
    DEBUG: 2019/11/27 08:58:21 Creating group jillsgroup with user-view role(s)
    DEBUG: 2019/11/27 08:58:21 Creating jill user in jillsgroup's group
    DEBUG: 2019/11/27 08:58:21 Group list
    DEBUG: 2019/11/27 08:58:21 List users
    DEBUG: 2019/11/27 08:58:21 Editing jacksgroup group with user-view role(s)
    DEBUG: 2019/11/27 08:58:22 Group list
    DEBUG: 2019/11/27 08:58:22 List users
    DEBUG: 2019/11/27 08:58:22 Creating user with user-view role.
    DEBUG: 2019/11/27 08:58:22 Creating alan user in jacksgroup's group
    ERROR: 2019/11/27 08:58:22  util.go:325: TestFailed Expected error msg : User jack cannot perform POST on users. Actual  error msg : <nil>.

    DEBUG: 2019/11/27 08:58:22 Editing group role(s)
    DEBUG: 2019/11/27 08:58:22 Editing jacksgroup group with user-edit role(s)
    DEBUG: 2019/11/27 08:58:22 Login as jack user
    DEBUG: 2019/11/27 08:58:23 Try to delete perf-user jill
    DEBUG: 2019/11/27 08:58:23 Try to delete group jillsgroup
[AfterEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:539
    DEBUG: 2019/11/27 08:58:23 User Logout
    DEBUG: 2019/11/27 08:58:24 END_TEST Rbac.EditView Time-taken : 24.762647528
    DEBUG: 2019/11/27 08:58:24 Checking stale resources
    DEBUG: 2019/11/27 08:58:24 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:58:24 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:58:24 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:25.000 seconds][0m
Rbac.EditView Daily Rbac_Local_Basic-2.0
[90m/gocode/main/test/e2e/tests/rbac.go:518[0m
  User can edit/view in it's namespace
  [90m/gocode/main/test/e2e/tests/rbac.go:520[0m
    User can edit/view user(s)
    [90m/gocode/main/test/e2e/tests/rbac.go:597[0m
[90m------------------------------[0m
[0mNetworkRemoteStorage.NicVFsScheduling Daily AT_Scheduling-1.2 Qos Multizone[0m [90mNetwork plus remote storage nic & VFs scheduling[0m 
  [1mNetwork plus remote storage nic & VFs scheduling[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:1993[0m
[BeforeEach] Network plus remote storage nic & VFs scheduling
  /gocode/main/test/e2e/tests/network-pod.go:1977
    DEBUG: 2019/11/27 08:58:25 START_TEST NetworkRemoteStorage.NicVFsScheduling
    DEBUG: 2019/11/27 08:58:25 Login to cluster
    DEBUG: 2019/11/27 08:58:25 Checking basic Vnic usage
    DEBUG: 2019/11/27 08:58:25 Updating inventory struct
    DEBUG: 2019/11/27 08:58:26 Checking stale resources
    DEBUG: 2019/11/27 08:58:26 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 08:58:26 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 08:58:26 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 08:58:35 Creating storage classes
[It] Network plus remote storage nic & VFs scheduling
  /gocode/main/test/e2e/tests/network-pod.go:1993
    DEBUG: 2019/11/27 08:58:44 Creating 2 pair of iperf client-server pod
    DEBUG: 2019/11/27 08:58:44 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/27 08:58:46 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/27 08:58:46 Creating iperf server pod: iperf-serverhigh2
    DEBUG: 2019/11/27 08:58:49 Creating service with name: iperf-serverhigh2
    DEBUG: 2019/11/27 08:59:19 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/27 08:59:22 Creating iperf Client pod: iperf-clienthigh2
    DEBUG: 2019/11/27 08:59:26 Getting pods scheduled on network nicIds
    DEBUG: 2019/11/27 08:59:26 Checking distribution of network pods across nicId(s)
    DEBUG: 2019/11/27 08:59:26 Pod scheduled as expected
    DEBUG: 2019/11/27 08:59:26 Creating fio pod and remote volume with high qos: 
    DEBUG: 2019/11/27 08:59:26 Create 2 fio pod(s):
    DEBUG: 2019/11/27 08:59:26 Creating dynamic pvc : fio-pod-hightest-vol1
    DEBUG: 2019/11/27 08:59:26 Created PVC successfully.
    DEBUG: 2019/11/27 08:59:26 Creating fio pod: fio-pod-high-1
    DEBUG: 2019/11/27 08:59:26 Creating dynamic pvc : fio-pod-hightest-vol2
    DEBUG: 2019/11/27 08:59:27 Created PVC successfully.
    DEBUG: 2019/11/27 08:59:27 Creating fio pod: fio-pod-high-2
    DEBUG: 2019/11/27 08:59:27 Checking if given pods are in Running state
    DEBUG: 2019/11/27 08:59:36 Getting pods and volumes scheduled on storage nicIds
    DEBUG: 2019/11/27 08:59:36 Checking distribution of storage pods across nicId(s)
    DEBUG: 2019/11/27 08:59:36 Pod scheduled as expected
    DEBUG: 2019/11/27 08:59:37 Deleting all the pods: 
    DEBUG: 2019/11/27 09:00:38 Waitting for volume to move to "Available" state
    DEBUG: 2019/11/27 09:00:39 Delete PVCs: 
    DEBUG: 2019/11/27 09:00:39 Waiting for volumes to get deleted: 
[AfterEach] Network plus remote storage nic & VFs scheduling
  /gocode/main/test/e2e/tests/network-pod.go:1988
    DEBUG: 2019/11/27 09:01:17 END_TEST NetworkRemoteStorage.NicVFsScheduling Time-taken : 172.604955443
    DEBUG: 2019/11/27 09:01:17 Checking stale resources
    DEBUG: 2019/11/27 09:01:17 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 09:01:17 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 09:01:17 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:172.839 seconds][0m
NetworkRemoteStorage.NicVFsScheduling Daily AT_Scheduling-1.2 Qos Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:1971[0m
  Network plus remote storage nic & VFs scheduling
  [90m/gocode/main/test/e2e/tests/network-pod.go:1972[0m
    Network plus remote storage nic & VFs scheduling
    [90m/gocode/main/test/e2e/tests/network-pod.go:1993[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mMirroring.MirrorResyncSingleTargetShutDown Daily SM_Reboot-2.6[0m [90mShutdown one of target node, then do IOs, bring up node, validate resync.[0m 
  [1mShutdown one of target node, then do IOs, bring up node, validate resync.[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:2470[0m
[BeforeEach] Shutdown one of target node, then do IOs, bring up node, validate resync.
  /gocode/main/test/e2e/tests/mirroring.go:2455
    DEBUG: 2019/11/27 09:01:17 START_TEST Mirroring.MirrorResyncSingleTargetShutDown
    DEBUG: 2019/11/27 09:01:17 Login to cluster
    DEBUG: 2019/11/27 09:01:18 Checking basic Vnic usage
    DEBUG: 2019/11/27 09:01:18 Updating inventory struct
    DEBUG: 2019/11/27 09:01:19 Checking stale resources
    DEBUG: 2019/11/27 09:01:19 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 09:01:19 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 09:01:19 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 09:01:27 Creating storage classes
[It] Shutdown one of target node, then do IOs, bring up node, validate resync.
  /gocode/main/test/e2e/tests/mirroring.go:2470
    DEBUG: 2019/11/27 09:01:36 Assigning label to nodes where the plexes of mirrored volumes should get scheduled
    DEBUG: 2019/11/27 09:01:36 Assigned label : mirror=true to node : appserv55
    DEBUG: 2019/11/27 09:01:37 Assigned label : mirror=true to node : appserv53
    DEBUG: 2019/11/27 09:01:37 Assigned label : mirror=true to node : appserv54
    DEBUG: 2019/11/27 09:01:37 Creating 5 volumes. Mirror Count: 3:
    DEBUG: 2019/11/27 09:01:37 Mirror Count: 3
    DEBUG: 2019/11/27 09:01:38 Attaching volumes: 
    DEBUG: 2019/11/27 09:02:04 Getting cluster quorum nodes
    DEBUG: 2019/11/27 09:02:04 Powering OFF the node appserv55
    DEBUG: 2019/11/27 09:02:04 Getting random fail node type
    DEBUG: 2019/11/27 09:02:05 Node 172.16.6.155 took 0 seconds to power off
    DEBUG: 2019/11/27 09:02:05 Ensuring that appserv55 node is unreachable: 
    DEBUG: 2019/11/27 09:02:05 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 09:02:14 Polling to check until node: appserv55 goes down
    DEBUG: 2019/11/27 09:02:44 Error: . Retrying once again...
    DEBUG: 2019/11/27 09:03:33 Writing pattern on volumes: 
    DEBUG: 2019/11/27 09:03:34 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1 --name=dev4 --filename=/dev/nvme4n1 --name=dev5 --filename=/dev/nvme5n1
    DEBUG: 2019/11/27 09:05:35 Powering ON the node appserv55
    DEBUG: 2019/11/27 09:05:36 Node 172.16.6.155 took 1 seconds to power on
    DEBUG: 2019/11/27 09:05:36 Checking if node appserv55 is reachable or not: 
    DEBUG: 2019/11/27 09:05:36 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 09:05:53 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 09:06:10 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 09:06:27 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 09:06:44 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 09:07:01 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 09:07:18 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 09:07:35 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 09:07:52 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/27 09:07:56 appserv55 is pingable from local machine
    DEBUG: 2019/11/27 09:07:56 Checking ssh port is up or not on node: appserv55
    DEBUG: 2019/11/27 09:08:26 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/27 09:08:26 Found '3' nodes
    DEBUG: 2019/11/27 09:08:26 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/27 09:09:11 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/27 09:09:11 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/27 09:09:21 After power cycle/reboot, updating timestamp of node : appserv55
    DEBUG: 2019/11/27 09:09:24 Getting cluster quorum nodes
    DEBUG: 2019/11/27 09:10:24 Updating inventory struct
    DEBUG: 2019/11/27 09:10:25 Waiting for volumes to come into Attached state after rebootin target node(s): 
    DEBUG: 2019/11/27 09:10:25 Wait till resync completion: 
    DEBUG: 2019/11/27 09:10:26 Number of volumes : 5
    DEBUG: 2019/11/27 09:10:26 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/27 09:10:26 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol2.p2. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/27 09:10:26 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/27 09:10:26 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol3.p2. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/27 09:10:26 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 Volume name & Plex : test-vol4.p2. Plex State : InUse
    DEBUG: 2019/11/27 09:10:26 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/27 09:10:26 Validate pattern written on volumes: 
    DEBUG: 2019/11/27 09:10:28 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/27 09:10:29 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/27 09:10:30 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/27 09:10:32 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/27 09:10:33 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/27 09:10:34 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1 --name=dev4 --filename=/dev/nvme4n1 --name=dev5 --filename=/dev/nvme5n1
    DEBUG: 2019/11/27 09:11:51 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/27 09:11:52 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/27 09:11:54 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/27 09:11:55 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/27 09:11:57 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/27 09:11:57 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1 --name=dev4 --filename=/dev/nvme4n1 --name=dev5 --filename=/dev/nvme5n1
    DEBUG: 2019/11/27 09:13:14 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/27 09:13:15 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/27 09:13:17 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/27 09:13:18 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/27 09:13:20 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/27 09:13:20 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1 --name=dev4 --filename=/dev/nvme4n1 --name=dev5 --filename=/dev/nvme5n1
    DEBUG: 2019/11/27 09:14:37 Detach & Delete volumes: 
    DEBUG: 2019/11/27 09:15:14 Removing label from the nodes where the plexes of mirrored volumes were scheduled
    DEBUG: 2019/11/27 09:15:14 Removed label : mirror from node : appserv55
    DEBUG: 2019/11/27 09:15:14 Removed label : mirror from node : appserv53
    DEBUG: 2019/11/27 09:15:15 Removed label : mirror from node : appserv54
[AfterEach] Shutdown one of target node, then do IOs, bring up node, validate resync.
  /gocode/main/test/e2e/tests/mirroring.go:2465
    DEBUG: 2019/11/27 09:15:15 END_TEST Mirroring.MirrorResyncSingleTargetShutDown Time-taken : 837.124891118
    DEBUG: 2019/11/27 09:15:15 Checking stale resources
    DEBUG: 2019/11/27 09:15:15 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 09:15:15 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 09:15:15 Checking stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:837.350 seconds][0m
Mirroring.MirrorResyncSingleTargetShutDown Daily SM_Reboot-2.6
[90m/gocode/main/test/e2e/tests/mirroring.go:2448[0m
  Shutdown one of target node, then do IOs, bring up node, validate resync.
  [90m/gocode/main/test/e2e/tests/mirroring.go:2449[0m
    Shutdown one of target node, then do IOs, bring up node, validate resync.
    [90m/gocode/main/test/e2e/tests/mirroring.go:2470[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m    DEBUG: 2019/11/27 09:15:15 Running After suite ...
    DEBUG: 2019/11/27 09:15:15 Checking existance of loopback device(s) on all cluster nodes
    DEBUG: 2019/11/27 09:15:26 Checking stale resources
    DEBUG: 2019/11/27 09:15:26 Checking stale resources on the node: appserv53
    DEBUG: 2019/11/27 09:15:26 Checking stale resources on the node: appserv54
    DEBUG: 2019/11/27 09:15:26 Checking stale resources on the node: appserv55
    DEBUG: 2019/11/27 09:15:26 Delete storage classes
    DEBUG: 2019/11/27 09:15:29 Login to cluster
    DEBUG: 2019/11/27 09:15:30 Destroying the cluster: 6136201a-1109-11ea-9c78-a4bf01194d67, Master node is appserv54
    DEBUG: 2019/11/27 09:15:30 Checking in a loop for cluster status
    DEBUG: 2019/11/27 09:16:09 Doing sync all nodes.
    DEBUG: 2019/11/27 09:16:09 Doing sync all nodes.
    DEBUG: 2019/11/27 09:16:10 Doing sync all nodes.
    DEBUG: 2019/11/27 09:16:12 Checking for cluster-info.json on node :172.16.6.153
    DEBUG: 2019/11/27 09:16:13 Checking for cluster-info.json on node :172.16.6.154
    DEBUG: 2019/11/27 09:16:13 Checking for cluster-info.json on node :172.16.6.155
    DEBUG: 2019/11/27 09:16:14 Rebooting all nodes.
    DEBUG: 2019/11/27 09:16:14 Doing sync on 172.16.6.153
    DEBUG: 2019/11/27 09:16:15 Doing sync on 172.16.6.154
    DEBUG: 2019/11/27 09:16:16 Doing sync on 172.16.6.155
    DEBUG: 2019/11/27 09:16:17 Waiting for nodes to come up, will wait upto 800 seconds
...............    DEBUG: 2019/11/27 09:19:00 Nodes are up, waiting for armada to start
......
    DEBUG: 2019/11/27 09:20:00 Bug converted to warning list is:

[1m[32mRan 118 of 418 Specs in 43945.133 seconds[0m
[1m[32mSUCCESS![0m -- [32m[1m118 Passed[0m | [91m[1m0 Failed[0m | [33m[1m0 Pending[0m | [36m[1m300 Skipped[0m
ERROR: logging before flag.Parse: I1127 09:20:00.942955   10810 driver.go:205] All tests pass
+ val=0
+ '[' 0 == 0 ']'
++ grep 'SUCCESS\!' console_ouput.txt
++ awk '{print $6}'
+ pass='[91m[1m0'
+ '[' '[91m[1m0' ']'
+ val=0
++ grep 'FAIL\!' console_ouput.txt
++ awk '{print $6}'
+ failcnt=
+ '[' ']'
+ echo 'Collecting techsupport from all nodes.'
Collecting techsupport from all nodes.
+ ../scripts/utils/collect_techsupport.sh -n appserv53 appserv54 appserv55


Collecting techsupport from all the nodes in parallel. Please wait ...


Cluster not found. Collecting techsupport without dctl login.


Copying tech-support dump from appserv55

Error from server: Cluster not found
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?

Cluster not found. Collecting techsupport without dctl login.


Copying tech-support dump from appserv54

Error from server: Cluster not found
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?

Cluster not found. Collecting techsupport without dctl login.


Copying tech-support dump from appserv53

Error from server: Cluster not found
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?


***** Tech support logs are stored in /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/bin/naveen/2019-11-27T09-11-01 directory *****

DCTL_CONFIG  was exported.
Taking .dctl.d from /dwshome/naveen/sanity/auto_tb7 
Creating file :  config-files-2019.11.27-09.22.46.tar.gz


***** Config files of dctl and kubectl are stored in /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/bin/naveen/2019-11-27T09-11-01 directory *****

Collecting pod description and logs for all the pods ...Unable to connect to the server: dial tcp 172.16.19.55:6443: connect: no route to host
Unable to connect to the server: dial tcp 172.16.19.55:6443: connect: no route to host


***** Copied pod description and logs from all pods in pod_description_and_logs.tar.gz file *****



Please copy e2e logs to /dwshome/naveen/jenkins/workspace/GA-2.3.0-NYNJ-Daily/GA-2.3.0-NYNJ-Daily/diamanti-test-pkg/bin/naveen/2019-11-27T09-11-01 directory 

+ echo 'Updating google sheets with the results'
Updating google sheets with the results
+ cd /var/lib/jenkins/workspace/build_sanity/Update_Spreadsheets_Project/python_spreadsheets/app/
+ ./get_latest_log_file.sh GA-2.3.0-NYNJ-Daily
Warning: Permanently added 'builds,172.16.4.5' (ECDSA) to the list of known hosts.
Permission denied, please try again.
Permission denied, please try again.
Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).
+ ./non_jenkins_user_update_sheets.sh -f log -d GA-2.3.0 -s NYNJ
INFO services.parser_service : Drive Directory : GA-2.3.0
INFO services.parser_service : Build Number : 

INFO services.parser_service : PASS IDs :
[]

INFO services.parser_service : FAIL IDs :
[]

INFO services.parser_service : SKIPPED IDs :
[]

ERROR __main__ : 
rpm NOT FOUND in log file

INFO __main__ : Completed updating individual sheets on the drive.
INFO __main__ : Total time taken :	0.00145292282104s
INFO __main__ : Log available at :	/var/lib/jenkins/workspace/build_sanity/Update_Spreadsheets_Project/python_spreadsheets/app/utils/../../log/Nov_27_2019_09:22:52_log
+ ./non_jenkins_user_update_sheets.sh -f log -d GA-2.3.0 -s Common
INFO services.parser_service : Drive Directory : GA-2.3.0
INFO services.parser_service : Build Number : 

INFO services.parser_service : PASS IDs :
[]

INFO services.parser_service : FAIL IDs :
[]

INFO services.parser_service : SKIPPED IDs :
[]

ERROR __main__ : 
rpm NOT FOUND in log file

INFO __main__ : Completed updating individual sheets on the drive.
INFO __main__ : Total time taken :	0.00127196311951s
INFO __main__ : Log available at :	/var/lib/jenkins/workspace/build_sanity/Update_Spreadsheets_Project/python_spreadsheets/app/utils/../../log/Nov_27_2019_09:22:53_log
+ ./non_jenkins_user_update_sheets.sh -f log show
INFO services.parser_service : Drive Directory : 
INFO services.parser_service : Build Number : 

INFO services.parser_service : Passed TCs :
	

INFO services.parser_service : Failed TCs :
	

INFO services.parser_service : Skipped TCs :
	

+ exit 0
Set build name.
New build name is 'Daily-55-0
'
Sending e-mails to: automation@diamanti.com
[lockable-resources] released lock on [testbed7]
Finished: SUCCESS
